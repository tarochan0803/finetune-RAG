# immediate_rag_system.py - å³åº§ã«å‹•ä½œã™ã‚‹RAGã‚·ã‚¹ãƒ†ãƒ 
# ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ä¸è¦ã€æ—¢å­˜ãƒ¢ãƒ‡ãƒ«+é«˜å“è³ªãƒ‡ãƒ¼ã‚¿ã§å³åº§ã«å‹•ä½œ

import os
import sys
import logging
import streamlit as st
import pandas as pd
import torch
from typing import List, Dict, Any, Optional
import time

# å¿…è¦ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    from langchain_chroma import Chroma
    from langchain_huggingface.embeddings import HuggingFaceEmbeddings
    from langchain_huggingface import HuggingFacePipeline
    from langchain.prompts import PromptTemplate
    from langchain_core.documents import Document
    from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
except ImportError as e:
    print(f"Import Error: {e}")
    print("å¿…è¦ãªãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ã„ã¾ã™...")
    import subprocess
    subprocess.check_call([sys.executable, "-m", "pip", "install", "langchain", "langchain-chroma", "langchain-huggingface"])

class ImmediateRAGSystem:
    """å³åº§ã«å‹•ä½œã™ã‚‹RAGã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        self.logger = self.setup_logging()
        self.embedding_model = None
        self.vectordb = None
        self.llm_pipeline = None
        self.initialized = False
        
    def setup_logging(self):
        """ãƒ­ã‚®ãƒ³ã‚°è¨­å®š"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s'
        )
        return logging.getLogger(__name__)
    
    def initialize_components(self):
        """ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆåˆæœŸåŒ–"""
        self.logger.info("ğŸš€ å³åº§RAGã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–é–‹å§‹")
        
        try:
            # 1. åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ï¼ˆè»½é‡ï¼‰
            self.logger.info("ğŸ“Š åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–...")
            self.embedding_model = HuggingFaceEmbeddings(
                model_name="intfloat/multilingual-e5-base",
                model_kwargs={'device': 'cpu'}  # CPUä½¿ç”¨ã§å®‰å®šæ€§ç¢ºä¿
            )
            
            # 2. VectorDBï¼ˆæ—¢å­˜ã®chroma_dbã‚’ä½¿ç”¨ï¼‰
            self.logger.info("ğŸ—„ï¸ VectorDBæ¥ç¶š...")
            db_path = "./chroma_db"
            if os.path.exists(db_path):
                self.vectordb = Chroma(
                    collection_name="my_collection",
                    persist_directory=db_path,
                    embedding_function=self.embedding_model
                )
                doc_count = self.vectordb._collection.count()
                self.logger.info(f"âœ… VectorDBæ¥ç¶šå®Œäº† ({doc_count}ä»¶)")
            else:
                self.logger.warning("âŒ VectorDBãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                return False
            
            # 3. LLMãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ï¼ˆè»½é‡ãƒ¢ãƒ‡ãƒ«ä½¿ç”¨ï¼‰
            self.logger.info("ğŸ¤– LLMãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åˆæœŸåŒ–...")
            
            # GPUä½¿ç”¨å¯èƒ½ã‹ãƒã‚§ãƒƒã‚¯
            device = "cuda" if torch.cuda.is_available() else "cpu"
            self.logger.info(f"ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {device}")
            
            if device == "cpu":
                # CPUç”¨è»½é‡è¨­å®š
                model_name = "rinna/japanese-gpt2-medium"  # ã‚ˆã‚Šè»½é‡ãªãƒ¢ãƒ‡ãƒ«
                tokenizer = AutoTokenizer.from_pretrained(model_name)
                model = AutoModelForCausalLM.from_pretrained(
                    model_name,
                    torch_dtype=torch.float32,
                    low_cpu_mem_usage=True
                )
            else:
                # GPUç”¨è¨­å®šï¼ˆCUDAäº’æ›æ€§å•é¡Œå¯¾å¿œï¼‰
                model_name = "rinna/japanese-gpt2-medium"
                tokenizer = AutoTokenizer.from_pretrained(model_name)
                model = AutoModelForCausalLM.from_pretrained(
                    model_name,
                    torch_dtype=torch.float16,
                    device_map="auto"
                )
            
            if tokenizer.pad_token is None:
                tokenizer.pad_token = tokenizer.eos_token
            
            self.llm_pipeline = pipeline(
                "text-generation",
                model=model,
                tokenizer=tokenizer,
                max_new_tokens=256,
                temperature=0.7,
                do_sample=True,
                device=0 if device == "cuda" else -1
            )
            
            self.logger.info("âœ… LLMãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åˆæœŸåŒ–å®Œäº†")
            self.initialized = True
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def search_documents(self, query: str, k: int = 5) -> List[Document]:
        """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ¤œç´¢"""
        if not self.vectordb:
            return []
        
        try:
            docs = self.vectordb.similarity_search(query, k=k)
            self.logger.info(f"ğŸ” æ¤œç´¢å®Œäº†: {len(docs)}ä»¶å–å¾—")
            return docs
        except Exception as e:
            self.logger.error(f"æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def generate_answer(self, query: str, context_docs: List[Document]) -> str:
        """å›ç­”ç”Ÿæˆ"""
        if not self.llm_pipeline:
            return "LLMãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“"
        
        try:
            # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆä½œæˆ
            context = "\n".join([doc.page_content[:200] for doc in context_docs[:3]])
            
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆ
            prompt = f"""ä»¥ä¸‹ã®æƒ…å ±ã‚’å‚è€ƒã«ã€è³ªå•ã«ç­”ãˆã¦ãã ã•ã„ã€‚

å‚è€ƒæƒ…å ±:
{context}

è³ªå•: {query}

å›ç­”:"""
            
            # å›ç­”ç”Ÿæˆ
            response = self.llm_pipeline(
                prompt,
                max_new_tokens=256,
                temperature=0.7,
                do_sample=True,
                pad_token_id=self.llm_pipeline.tokenizer.eos_token_id
            )
            
            # å›ç­”æŠ½å‡º
            generated_text = response[0]['generated_text']
            answer = generated_text[len(prompt):].strip()
            
            return answer if answer else "ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ãŒã€é©åˆ‡ãªå›ç­”ã‚’ç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸã€‚"
            
        except Exception as e:
            self.logger.error(f"å›ç­”ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            return f"å›ç­”ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}"
    
    def query_rag(self, query: str) -> Dict[str, Any]:
        """RAGã‚¯ã‚¨ãƒªå®Ÿè¡Œ"""
        if not self.initialized:
            return {"answer": "ã‚·ã‚¹ãƒ†ãƒ ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“", "sources": []}
        
        start_time = time.time()
        
        # 1. ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ¤œç´¢
        docs = self.search_documents(query)
        
        # 2. å›ç­”ç”Ÿæˆ
        answer = self.generate_answer(query, docs)
        
        end_time = time.time()
        
        return {
            "answer": answer,
            "sources": docs,
            "processing_time": end_time - start_time,
            "source_count": len(docs)
        }

def create_streamlit_app():
    """Streamlitã‚¢ãƒ—ãƒªä½œæˆ"""
    st.set_page_config(page_title="å³åº§RAG", layout="centered")
    st.title("âš¡ å³åº§RAGã‚·ã‚¹ãƒ†ãƒ ")
    st.caption("ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ä¸è¦ - å³åº§ã«å‹•ä½œã™ã‚‹é«˜å“è³ªRAG")
    
    # ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
    @st.cache_resource
    def load_rag_system():
        system = ImmediateRAGSystem()
        if system.initialize_components():
            return system
        else:
            return None
    
    rag_system = load_rag_system()
    
    if not rag_system:
        st.error("ğŸš« RAGã‚·ã‚¹ãƒ†ãƒ ã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ")
        st.info("ä»¥ä¸‹ã‚’ç¢ºèªã—ã¦ãã ã•ã„:")
        st.info("1. chroma_dbãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã™ã‚‹ã‹")
        st.info("2. å¿…è¦ãªãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹ã‹")
        st.stop()
    
    st.success("âœ… RAGã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
    
    # ã‚µã‚¤ãƒ‰ãƒãƒ¼
    with st.sidebar:
        st.header("âš™ï¸ ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±")
        st.info("ğŸ’¡ è»½é‡ãƒ¢ãƒ‡ãƒ«ä½¿ç”¨")
        st.info("ğŸ” æ—¢å­˜VectorDBæ´»ç”¨")
        st.info("âš¡ å³åº§ã«å‹•ä½œ")
        
        if torch.cuda.is_available():
            try:
                gpu_name = torch.cuda.get_device_name(0)
                st.success(f"ğŸ® GPU: {gpu_name}")
            except:
                st.info("ğŸ’» CPUä½¿ç”¨")
        else:
            st.info("ğŸ’» CPUä½¿ç”¨")
    
    # ãƒ¡ã‚¤ãƒ³ãƒãƒ£ãƒƒãƒˆ
    if "messages" not in st.session_state:
        st.session_state.messages = []
    
    # ä¼šè©±å±¥æ­´è¡¨ç¤º
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])
    
    # è³ªå•å…¥åŠ›
    if user_input := st.chat_input("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„..."):
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸è¡¨ç¤º
        with st.chat_message("user"):
            st.markdown(user_input)
        st.session_state.messages.append({"role": "user", "content": user_input})
        
        # AIå¿œç­”
        with st.chat_message("assistant"):
            with st.spinner("ğŸ¤” è€ƒãˆä¸­..."):
                result = rag_system.query_rag(user_input)
            
            # å›ç­”è¡¨ç¤º
            st.markdown(result["answer"])
            
            # è©³ç´°æƒ…å ±
            with st.expander("ğŸ“Š è©³ç´°æƒ…å ±"):
                st.write(f"â±ï¸ å‡¦ç†æ™‚é–“: {result['processing_time']:.2f}ç§’")
                st.write(f"ğŸ“„ å‚è€ƒæ–‡æ›¸æ•°: {result['source_count']}ä»¶")
                
                if result["sources"]:
                    st.write("ğŸ“– å‚è€ƒæ–‡æ›¸:")
                    for i, doc in enumerate(result["sources"][:3]):
                        st.text_area(
                            f"æ–‡æ›¸ {i+1}",
                            doc.page_content[:300] + "..." if len(doc.page_content) > 300 else doc.page_content,
                            height=100,
                            disabled=True
                        )
        
        st.session_state.messages.append({"role": "assistant", "content": result["answer"]})

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    if len(sys.argv) > 1 and sys.argv[1] == "streamlit":
        create_streamlit_app()
    else:
        # CLIç‰ˆãƒ†ã‚¹ãƒˆ
        print("ğŸš€ å³åº§RAGã‚·ã‚¹ãƒ†ãƒ  - CLIç‰ˆãƒ†ã‚¹ãƒˆ")
        
        system = ImmediateRAGSystem()
        if not system.initialize_components():
            print("âŒ åˆæœŸåŒ–å¤±æ•—")
            return
        
        print("âœ… åˆæœŸåŒ–å®Œäº†")
        
        # ãƒ†ã‚¹ãƒˆã‚¯ã‚¨ãƒª
        test_queries = [
            "æ ªå¼ä¼šç¤¾ä¸‰å»ºã®å£ä»•æ§˜ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„",
            "ä»®ç­‹äº¤ã®æ¨™æº–ä»•æ§˜ã¯ä½•ã§ã™ã‹",
            "é‹¼è£½æŸã®ä½¿ç”¨åŸºæº–ã«ã¤ã„ã¦"
        ]
        
        for query in test_queries:
            print(f"\nâ“ è³ªå•: {query}")
            result = system.query_rag(query)
            print(f"ğŸ’¬ å›ç­”: {result['answer']}")
            print(f"â±ï¸ å‡¦ç†æ™‚é–“: {result['processing_time']:.2f}ç§’")
            print("-" * 50)

if __name__ == "__main__":
    main()