# run_finetune.py - å®Ÿè¡Œç”¨ãƒ¡ã‚¤ãƒ³ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
# GPUç’°å¢ƒãƒã‚§ãƒƒã‚¯ï¼†æœ€é©åŒ–ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å®Ÿè¡Œ

import torch
import subprocess
import sys
import os

def check_environment():
    """å®Ÿè¡Œç’°å¢ƒã‚’ãƒã‚§ãƒƒã‚¯"""
    print("=== å®Ÿè¡Œç’°å¢ƒãƒã‚§ãƒƒã‚¯ ===")
    
    # GPUç¢ºèª
    if torch.cuda.is_available():
        gpu_count = torch.cuda.device_count()
        current_gpu = torch.cuda.current_device()
        gpu_name = torch.cuda.get_device_name(current_gpu)
        memory_total = torch.cuda.get_device_properties(current_gpu).total_memory / 1e9
        
        print(f"âœ… CUDAåˆ©ç”¨å¯èƒ½")
        print(f"   GPUæ•°: {gpu_count}")
        print(f"   ç¾åœ¨ã®GPU: {gpu_name}")
        print(f"   VRAMå®¹é‡: {memory_total:.1f}GB")
        
        # VRAMä½¿ç”¨é‡ç¢ºèª
        memory_allocated = torch.cuda.memory_allocated(current_gpu) / 1e9
        memory_reserved = torch.cuda.memory_reserved(current_gpu) / 1e9
        print(f"   ä½¿ç”¨ä¸­VRAM: {memory_allocated:.1f}GB")
        print(f"   äºˆç´„æ¸ˆã¿VRAM: {memory_reserved:.1f}GB")
        
        return True, memory_total
    else:
        print("âŒ CUDAãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
        return False, 0

def install_requirements():
    """å¿…è¦ãªãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«"""
    print("\n=== ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ç¢ºèªãƒ»ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« ===")
    
    packages = [
        "torch",
        "transformers>=4.36.0", 
        "datasets",
        "peft",
        "accelerate",
        "bitsandbytes",
        "tensorboard",
        "scipy",
    ]
    
    for package in packages:
        try:
            __import__(package.split(">=")[0].split("==")[0])
            print(f"âœ… {package}")
        except ImportError:
            print(f"â¬‡ï¸ {package} ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ä¸­...")
            subprocess.check_call([sys.executable, "-m", "pip", "install", package])

def optimize_settings_for_gpu(vram_gb):
    """GPU VRAMã«å¿œã˜ãŸæœ€é©è¨­å®šã‚’ç”Ÿæˆ"""
    print(f"\n=== VRAM {vram_gb:.1f}GBç”¨è¨­å®šæœ€é©åŒ– ===")
    
    if vram_gb >= 24:  # RTX 4090, A100ç­‰
        settings = {
            "batch_size": 2,
            "grad_accumulation": 8,
            "max_length": 1200,
            "lora_r": 32,
            "learning_rate": 1e-4,
        }
        print("ğŸš€ é«˜æ€§èƒ½GPUè¨­å®š")
    elif vram_gb >= 16:  # RTX 4080, 3090ç­‰
        settings = {
            "batch_size": 1,
            "grad_accumulation": 12,
            "max_length": 1000,
            "lora_r": 16,
            "learning_rate": 5e-5,
        }
        print("âš¡ ä¸­æ€§èƒ½GPUè¨­å®š")
    elif vram_gb >= 12:  # RTX 4070Ti, 3080ç­‰
        settings = {
            "batch_size": 1,
            "grad_accumulation": 8,
            "max_length": 800,
            "lora_r": 8,
            "learning_rate": 2e-5,
        }
        print("âš™ï¸ æ¨™æº–GPUè¨­å®š")
    else:  # RTX 4060, 3070ç­‰
        settings = {
            "batch_size": 1,
            "grad_accumulation": 4,
            "max_length": 600,
            "lora_r": 4,
            "learning_rate": 1e-5,
        }
        print("ğŸ’§ è»½é‡GPUè¨­å®š")
    
    for key, value in settings.items():
        print(f"   {key}: {value}")
    
    return settings

def create_optimized_script(settings):
    """æœ€é©åŒ–ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ç”Ÿæˆ"""
    script_content = f'''# auto_optimized_finetune.py - è‡ªå‹•æœ€é©åŒ–ç‰ˆ
import torch
import os
from datasets import load_dataset, load_from_disk
from transformers import (
    AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments,
    DataCollatorForSeq2Seq, EarlyStoppingCallback
)
from peft import LoraConfig, get_peft_model, TaskType
from transformers import BitsAndBytesConfig

# è‡ªå‹•æœ€é©åŒ–è¨­å®š
MODEL_NAME = "elyza/ELYZA-japanese-Llama-2-7b-instruct"
JSONL_PATH = "/home/ncnadmin/my_rag_project/enhanced_training_dataset.jsonl"
OUTPUT_DIR = "./optimized_rag_model"
CACHE_DIR = "cache/optimized"

# GPUæœ€é©åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
BATCH_SIZE = {settings["batch_size"]}
GRAD_ACCUMULATION = {settings["grad_accumulation"]}
MAX_LENGTH = {settings["max_length"]}
LORA_R = {settings["lora_r"]}
LEARNING_RATE = {settings["learning_rate"]}

print("ğŸš€ æœ€é©åŒ–ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°é–‹å§‹")
print(f"   ãƒãƒƒãƒã‚µã‚¤ã‚º: {{BATCH_SIZE}}")
print(f"   å‹¾é…è“„ç©: {{GRAD_ACCUMULATION}}")
print(f"   æœ€å¤§é•·: {{MAX_LENGTH}}")
print(f"   LoRA r: {{LORA_R}}")
print(f"   å­¦ç¿’ç‡: {{LEARNING_RATE}}")

# ãƒ‡ãƒ¼ã‚¿æº–å‚™
def preprocess_function(examples):
    instruction = examples.get("instruction", "").strip()
    input_text = examples.get("input", "").strip()
    output_text = examples.get("output", "").strip()
    
    if instruction:
        prompt = f"### æŒ‡ç¤º:\\n{{instruction}}\\n\\n### å…¥åŠ›:\\n{{input_text}}\\n\\n### å¿œç­”:\\n"
    else:
        prompt = f"### å…¥åŠ›:\\n{{input_text}}\\n\\n### å¿œç­”:\\n"
    
    return {{"prompt": prompt, "completion": output_text}}

# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
raw_dataset = load_dataset("json", data_files={{"train": JSONL_PATH}}, split="train")
dataset = raw_dataset.map(preprocess_function, remove_columns=raw_dataset.column_names)
dataset = dataset.train_test_split(test_size=0.1, seed=42)

# ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

def tokenize_function(examples):
    prompts = examples["prompt"]
    completions = examples["completion"]
    
    full_texts = [p + c + tokenizer.eos_token for p, c in zip(prompts, completions)]
    
    model_inputs = tokenizer(
        full_texts,
        max_length=MAX_LENGTH,
        truncation=True,
        padding=False,
    )
    
    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆé•·ã‚’è¨ˆç®—ã—ã¦ãƒ©ãƒ™ãƒ«ãƒã‚¹ã‚¯
    prompt_inputs = tokenizer(prompts, truncation=True, padding=False)
    labels = []
    
    for i, input_ids in enumerate(model_inputs["input_ids"]):
        prompt_len = len(prompt_inputs["input_ids"][i])
        label = input_ids.copy()
        label[:prompt_len] = [-100] * prompt_len
        labels.append(label)
    
    model_inputs["labels"] = labels
    return model_inputs

# ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚ºï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥åˆ©ç”¨ï¼‰
try:
    tokenized_dataset = load_from_disk(CACHE_DIR)
    print("âœ… ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿")
except:
    print("â³ ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚ºå®Ÿè¡Œä¸­...")
    tokenized_dataset = dataset.map(
        tokenize_function,
        batched=True,
        remove_columns=["prompt", "completion"],
        num_proc=4
    )
    tokenized_dataset.save_to_disk(CACHE_DIR)
    print("âœ… ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚ºå®Œäº†")

# ãƒ¢ãƒ‡ãƒ«è¨­å®š
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
)

model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    quantization_config=bnb_config,
    device_map="auto",
    trust_remote_code=True,
    torch_dtype=torch.bfloat16,
)

model.gradient_checkpointing_enable()

# LoRAè¨­å®š
lora_config = LoraConfig(
    task_type=TaskType.CAUSAL_LM,
    r=LORA_R,
    lora_alpha=LORA_R * 2,
    lora_dropout=0.1,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
)

model = get_peft_model(model, lora_config)
print(f"âœ… LoRAé©ç”¨å®Œäº†ï¼ˆå­¦ç¿’å¯èƒ½ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {{model.num_parameters(only_trainable=True):,}}ï¼‰")

# ãƒ‡ãƒ¼ã‚¿ã‚³ãƒ¬ã‚¯ã‚¿ãƒ¼
data_collator = DataCollatorForSeq2Seq(
    tokenizer=tokenizer,
    padding="longest",
    label_pad_token_id=-100,
)

# å­¦ç¿’è¨­å®š
training_args = TrainingArguments(
    output_dir=OUTPUT_DIR,
    num_train_epochs=3,
    per_device_train_batch_size=BATCH_SIZE,
    per_device_eval_batch_size=BATCH_SIZE,
    gradient_accumulation_steps=GRAD_ACCUMULATION,
    learning_rate=LEARNING_RATE,
    lr_scheduler_type="cosine",
    warmup_steps=100,
    bf16=True,
    optim="paged_adamw_8bit",
    dataloader_num_workers=2,
    
    evaluation_strategy="steps",
    eval_steps=100,
    save_strategy="steps",
    save_steps=100,
    save_total_limit=2,
    load_best_model_at_end=True,
    metric_for_best_model="eval_loss",
    
    logging_steps=10,
    report_to=["tensorboard"],
    
    remove_unused_columns=False,
    group_by_length=True,
)

# ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ä½œæˆ
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset["train"],
    eval_dataset=tokenized_dataset["test"],
    data_collator=data_collator,
    tokenizer=tokenizer,
    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]
)

print("ğŸ¯ å­¦ç¿’é–‹å§‹...")
trainer.train()

print("ğŸ’¾ ãƒ¢ãƒ‡ãƒ«ä¿å­˜ä¸­...")
trainer.save_model()
tokenizer.save_pretrained(OUTPUT_DIR)

print("ğŸ‰ å­¦ç¿’å®Œäº†ï¼")
print(f"ğŸ“ ä¿å­˜å…ˆ: {{OUTPUT_DIR}}")
print("\\næ¬¡ã®æ‰‹é †:")
print("1. config.py ã® lora_adapter_path ã‚’ä»¥ä¸‹ã«å¤‰æ›´:")
print(f"   self.lora_adapter_path = '{{OUTPUT_DIR}}'")
print("2. RAGã‚¢ãƒ—ãƒªã‚’èµ·å‹•ã—ã¦æ€§èƒ½ç¢ºèª")
'''
    
    with open("/home/ncnadmin/my_rag_project/auto_optimized_finetune.py", "w", encoding="utf-8") as f:
        f.write(script_content)
    
    print(f"âœ… æœ€é©åŒ–ã‚¹ã‚¯ãƒªãƒ—ãƒˆç”Ÿæˆå®Œäº†: auto_optimized_finetune.py")

def main():
    print("ğŸ¤– ãƒ­ãƒ¼ã‚«ãƒ«AIãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°è‡ªå‹•ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—")
    print("=" * 50)
    
    # ç’°å¢ƒãƒã‚§ãƒƒã‚¯
    cuda_available, vram_gb = check_environment()
    if not cuda_available:
        print("âŒ GPUç’°å¢ƒãŒå¿…è¦ã§ã™")
        return
    
    # ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
    install_requirements()
    
    # è¨­å®šæœ€é©åŒ–
    settings = optimize_settings_for_gpu(vram_gb)
    
    # ã‚¹ã‚¯ãƒªãƒ—ãƒˆç”Ÿæˆ
    create_optimized_script(settings)
    
    print("\\nğŸš€ å®Ÿè¡Œæº–å‚™å®Œäº†ï¼")
    print("æ¬¡ã®ã‚³ãƒãƒ³ãƒ‰ã§ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’é–‹å§‹:")
    print("python3 auto_optimized_finetune.py")
    
    # å®Ÿè¡Œç¢ºèª
    response = input("\\nä»Šã™ããƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’é–‹å§‹ã—ã¾ã™ã‹ï¼Ÿ (y/N): ")
    if response.lower() in ['y', 'yes']:
        print("\\nğŸ¯ ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°é–‹å§‹...")
        os.system("python3 auto_optimized_finetune.py")
    else:
        print("\\nâœ‹ å¾Œã§æ‰‹å‹•å®Ÿè¡Œã—ã¦ãã ã•ã„")

if __name__ == "__main__":
    main()