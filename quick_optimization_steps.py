# quick_optimization_steps.py - æ‰‹å‹•å®Ÿè¡Œç‰ˆæœ€é©åŒ–
# ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ã«å®Ÿè¡Œå¯èƒ½

import os
import time

def step1_data_quality():
    """ã‚¹ãƒ†ãƒƒãƒ—1: ãƒ‡ãƒ¼ã‚¿å“è³ªå‘ä¸Šï¼ˆå®Œäº†æ¸ˆã¿ï¼‰"""
    print("âœ… ã‚¹ãƒ†ãƒƒãƒ—1: ãƒ‡ãƒ¼ã‚¿å“è³ªå‘ä¸Š - å®Œäº†æ¸ˆã¿")
    print("   60,403ä»¶ã®é«˜å“è³ªå­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ")
    return True

def step2_lightweight_finetune():
    """ã‚¹ãƒ†ãƒƒãƒ—2: è»½é‡ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆRTX 5070å¯¾å¿œï¼‰"""
    print("ğŸš€ ã‚¹ãƒ†ãƒƒãƒ—2: è»½é‡ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å®Ÿè¡Œ")
    
    # è»½é‡ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚³ãƒ¼ãƒ‰
    finetune_code = '''
import torch
import os
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer
from peft import LoraConfig, get_peft_model, TaskType
from transformers import BitsAndBytesConfig, DataCollatorForLanguageModeling

print("ğŸ”¥ RTX 5070å‘ã‘è»½é‡ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°")

# è¨­å®š
MODEL_NAME = "elyza/ELYZA-japanese-Llama-2-7b-instruct"
OUTPUT_DIR = "./rag_optimized_model"
JSONL_PATH = "/home/ncnadmin/my_rag_project/premium_training_dataset.jsonl"

# ãƒ‡ãƒ¼ã‚¿æº–å‚™ï¼ˆã‚µãƒ³ãƒ—ãƒ«æ•°åˆ¶é™ï¼‰
dataset = load_dataset("json", data_files={"train": JSONL_PATH}, split="train")
dataset = dataset.select(range(2000))  # 2000ã‚µãƒ³ãƒ—ãƒ«ã§é«˜é€Ÿå­¦ç¿’

def format_prompt(example):
    instruction = example.get("instruction", "").strip()
    input_text = example.get("input", "").strip() 
    output_text = example.get("output", "").strip()
    
    if instruction:
        text = f"### æŒ‡ç¤º:\\n{instruction}\\n\\n### å…¥åŠ›:\\n{input_text}\\n\\n### å¿œç­”:\\n{output_text}<|endoftext|>"
    else:
        text = f"### å…¥åŠ›:\\n{input_text}\\n\\n### å¿œç­”:\\n{output_text}<|endoftext|>"
    return {"text": text}

dataset = dataset.map(format_prompt)
dataset = dataset.train_test_split(test_size=0.1)

# ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

def tokenize(examples):
    return tokenizer(examples["text"], truncation=True, padding=False, max_length=512)

tokenized = dataset.map(tokenize, batched=True, remove_columns=["instruction", "input", "output", "text"])

# 4bité‡å­åŒ–ãƒ¢ãƒ‡ãƒ«
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_quant_type="nf4",
)

model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    quantization_config=bnb_config,
    device_map="auto",
    trust_remote_code=True,
)

# è»½é‡LoRA
lora_config = LoraConfig(
    r=8,
    lora_alpha=16,
    target_modules=["q_proj", "v_proj"],
    lora_dropout=0.1,
    task_type=TaskType.CAUSAL_LM,
)

model = get_peft_model(model, lora_config)
print(f"å­¦ç¿’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {model.num_parameters(only_trainable=True):,}")

# è»½é‡å­¦ç¿’è¨­å®š
training_args = TrainingArguments(
    output_dir=OUTPUT_DIR,
    num_train_epochs=2,
    per_device_train_batch_size=1,
    gradient_accumulation_steps=4,
    learning_rate=1e-4,
    fp16=True,
    logging_steps=50,
    save_steps=500,
    eval_steps=500,
    evaluation_strategy="steps",
    save_total_limit=1,
    remove_unused_columns=False,
    dataloader_pin_memory=False,
)

# ãƒ‡ãƒ¼ã‚¿ã‚³ãƒ¬ã‚¯ã‚¿ãƒ¼
data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm=False,
)

# ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized["train"],
    eval_dataset=tokenized["test"],
    data_collator=data_collator,
)

print("ğŸš€ å­¦ç¿’é–‹å§‹...")
trainer.train()

print("ğŸ’¾ ä¿å­˜ä¸­...")
model.save_pretrained(OUTPUT_DIR)
tokenizer.save_pretrained(OUTPUT_DIR)

print(f"ğŸ‰ ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å®Œäº†ï¼")
print(f"ä¿å­˜å…ˆ: {OUTPUT_DIR}")
'''
    
    with open("/home/ncnadmin/my_rag_project/quick_finetune_exec.py", "w", encoding="utf-8") as f:
        f.write(finetune_code)
    
    print("âœ… è»½é‡ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¹ã‚¯ãƒªãƒ—ãƒˆç”Ÿæˆå®Œäº†")
    print("   å®Ÿè¡Œ: python3 quick_finetune_exec.py")
    return True

def step3_create_optimized_config():
    """ã‚¹ãƒ†ãƒƒãƒ—3: æœ€é©åŒ–è¨­å®šä½œæˆ"""
    print("âš™ï¸ ã‚¹ãƒ†ãƒƒãƒ—3: æœ€é©åŒ–è¨­å®šä½œæˆ")
    
    config_content = '''# config_optimized.py - æœ€é©åŒ–å®Œäº†ç‰ˆ
import os
import logging
import torch

class Config:
    def __init__(self):
        # --- æœ€é©åŒ–æ¸ˆã¿ãƒ¢ãƒ‡ãƒ« ---
        self.base_model_name = "elyza/ELYZA-japanese-Llama-2-7b-instruct"
        self.lora_adapter_path = "./rag_optimized_model"  # ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿
        
        # --- é«˜æ€§èƒ½è¨­å®š ---
        self.use_4bit_quant = True
        self.quant_compute_dtype = torch.bfloat16
        self.model_load_dtype = torch.bfloat16
        self.use_flash_attention_2 = True
        
        # --- RAGè¨­å®š ---
        self.embeddings_model = "intfloat/multilingual-e5-base"
        self.chunk_size = 800
        self.chunk_overlap = 100
        self.rag_variant_k = [5, 8, 10]
        
        # --- æ¨è«–æœ€é©åŒ– ---
        self.max_new_tokens = 256
        self.temperature = 0.1
        self.top_p = 0.9
        self.repetition_penalty = 1.15
        self.do_sample = False
        self.num_beams = 1
        
        # --- ãƒ‡ãƒ¼ã‚¿è¨­å®š ---
        self.persist_directory = "./chroma_db"
        self.collection_name = "my_collection"
        self.csv_file = "/home/ncnadmin/rag_data3.csv"
        self.required_columns = [
            "company", "conditions", "type", "category",
            "major_item", "middle_item", "small_item", "text"
        ]
        
        # --- ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæœ€é©åŒ– ---
        self.intermediate_prompt_template = """### æŒ‡ç¤º:
ä»¥ä¸‹ã®å‚è€ƒæƒ…å ±ã‚’åŸºã«ã€æ­£ç¢ºã§å°‚é–€çš„ãªå›ç­”ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚

### å‚è€ƒæƒ…å ±:
{context}

### è³ªå•:
{question}

### å¿œç­”:
"""
        
        self.synthesis_prompt_template = """### æŒ‡ç¤º:
è¤‡æ•°ã®å›ç­”æ¡ˆã‚’çµ±åˆã—ã€æœ€ã‚‚æ­£ç¢ºãªæœ€çµ‚å›ç­”ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚

### è³ªå•:
{original_question}

### å›ç­”æ¡ˆ1:
{answer_1}

### å›ç­”æ¡ˆ2:
{answer_2}

### å›ç­”æ¡ˆ3:
{answer_3}

### æœ€çµ‚å›ç­”:
"""
        
        # --- ãã®ä»–è¨­å®š ---
        self.max_parallel_variants = 3
        self.pipeline_batch_size = 4
        self.metadata_display_columns = ["source", "type", "major_item", "middle_item", "small_item"]
        self.log_level = logging.INFO
        self.hf_token = os.getenv("HF_API_TOKEN")

def setup_logging(config: Config, log_filename: str = "rag_optimized.log") -> logging.Logger:
    logger = logging.getLogger("RAGOptimized")
    
    if logger.hasHandlers():
        logger.handlers.clear()
    
    logger.setLevel(config.log_level)
    formatter = logging.Formatter("%(asctime)s - %(levelname)s - %(message)s")
    
    try:
        file_handler = logging.FileHandler(log_filename, encoding='utf-8')
        file_handler.setLevel(config.log_level)
        file_handler.setFormatter(formatter)
        logger.addHandler(file_handler)
    except Exception as e:
        print(f"ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆå¤±æ•—: {e}")
    
    stream_handler = logging.StreamHandler()
    stream_handler.setLevel(config.log_level)
    stream_handler.setFormatter(formatter)
    logger.addHandler(stream_handler)
    
    logger.propagate = False
    return logger
'''
    
    with open("/home/ncnadmin/my_rag_project/config_optimized.py", "w", encoding="utf-8") as f:
        f.write(config_content)
    
    print("âœ… æœ€é©åŒ–è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆå®Œäº†: config_optimized.py")
    return True

def create_optimized_rag_app():
    """æœ€é©åŒ–ç‰ˆRAGã‚¢ãƒ—ãƒªä½œæˆ"""
    print("ğŸ¯ æœ€é©åŒ–ç‰ˆRAGã‚¢ãƒ—ãƒªä½œæˆ")
    
    app_content = '''# RAGapp_optimized.py - æœ€é©åŒ–å®Œäº†ç‰ˆRAGã‚¢ãƒ—ãƒª
import streamlit as st
import pandas as pd
import os
import datetime
import sys
import logging
import json
import torch
from typing import Optional, Tuple, List, Dict, Any

# æœ€é©åŒ–ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    from config_optimized import Config, setup_logging
    from rag_query_utils import initialize_pipeline, ask_question_ensemble_stream
    from utils import format_document_snippet, normalize_str, preprocess_query
except ImportError as e:
    st.error(f"Import Error: {e}")
    st.stop()

# ã‚°ãƒ­ãƒ¼ãƒãƒ«è¨­å®š
config = Config()
logger = setup_logging(config, log_filename="rag_optimized_app.log")

@st.cache_resource
def load_optimized_pipeline():
    """æœ€é©åŒ–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åˆæœŸåŒ–"""
    logger.info("æœ€é©åŒ–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åˆæœŸåŒ–ä¸­...")
    try:
        pipeline_components = initialize_pipeline(config, logger)
        if not all(comp is not None for comp in pipeline_components):
            logger.error("ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åˆæœŸåŒ–å¤±æ•—")
            return (None,) * 4
        logger.info("æœ€é©åŒ–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åˆæœŸåŒ–å®Œäº†")
        return pipeline_components
    except Exception as e:
        logger.critical(f"ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
        return (None,) * 4

def main():
    st.set_page_config(page_title="æœ€é©åŒ–RAG", layout="centered")
    st.title("ğŸš€ æœ€é©åŒ–RAGã‚·ã‚¹ãƒ†ãƒ ")
    st.caption("é«˜å“è³ªãƒ‡ãƒ¼ã‚¿ + ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚° + æ¨è«–æœ€é©åŒ–")
    
    # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åˆæœŸåŒ–
    with st.spinner("æœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–ä¸­..."):
        vectordb, intermediate_llm, tokenizer, embedding_function = load_optimized_pipeline()
    
    if vectordb is None:
        st.error("ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ")
        st.stop()
    
    # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹åˆæœŸåŒ–
    if "query_history" not in st.session_state:
        st.session_state.query_history = []
    if "current_answer_stream" not in st.session_state:
        st.session_state.current_answer_stream = None
    
    # ã‚µã‚¤ãƒ‰ãƒãƒ¼
    with st.sidebar:
        st.header("âš™ï¸ æœ€é©åŒ–è¨­å®š")
        st.success("âœ… é«˜å“è³ªãƒ‡ãƒ¼ã‚¿ï¼ˆ60,403ä»¶ï¼‰")
        st.success("âœ… ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿")
        st.success("âœ… æ¨è«–æœ€é©åŒ–")
        
        if torch.cuda.is_available():
            gpu_name = torch.cuda.get_device_name(0)
            st.info(f"GPU: {gpu_name}")
    
    # ä¼šè©±å±¥æ­´è¡¨ç¤º
    for message in st.session_state.query_history:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])
    
    # è³ªå•å…¥åŠ›
    if user_input := st.chat_input("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„..."):
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸è¡¨ç¤º
        with st.chat_message("user"):
            st.markdown(user_input)
        st.session_state.query_history.append({"role": "user", "content": user_input})
        
        # AIå¿œç­”ç”Ÿæˆ
        with st.chat_message("assistant"):
            response_placeholder = st.empty()
            
            try:
                # æœ€é©åŒ–ã‚¯ã‚¨ãƒªå‡¦ç†
                response = ask_question_ensemble_stream(
                    vectordb=vectordb,
                    intermediate_llm=intermediate_llm,
                    tokenizer=tokenizer,
                    embedding_function=embedding_function,
                    config=config,
                    query=preprocess_query(user_input),
                    logger=logger
                )
                
                # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°è¡¨ç¤º
                full_response = ""
                for chunk in response.get("result_stream", []):
                    full_response += chunk
                    response_placeholder.markdown(full_response + "â–Œ")
                
                response_placeholder.markdown(full_response)
                st.session_state.query_history.append({"role": "assistant", "content": full_response})
                
            except Exception as e:
                error_msg = f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}"
                response_placeholder.error(error_msg)
                st.session_state.query_history.append({"role": "assistant", "content": error_msg})

if __name__ == "__main__":
    main()
'''
    
    with open("/home/ncnadmin/my_rag_project/RAGapp_optimized.py", "w", encoding="utf-8") as f:
        f.write(app_content)
    
    print("âœ… æœ€é©åŒ–RAGã‚¢ãƒ—ãƒªä½œæˆå®Œäº†: RAGapp_optimized.py")
    return True

def main():
    print("ğŸ”¥ RAGã‚·ã‚¹ãƒ†ãƒ æ‰‹å‹•æœ€é©åŒ–")
    print("=" * 50)
    
    # ã‚¹ãƒ†ãƒƒãƒ—å®Ÿè¡Œ
    step1_data_quality()
    step2_lightweight_finetune()
    step3_create_optimized_config()
    create_optimized_rag_app()
    
    print("\nğŸ‰ æœ€é©åŒ–æº–å‚™å®Œäº†ï¼")
    print("\nğŸ“‹ å®Ÿè¡Œæ‰‹é †:")
    print("1. python3 quick_finetune_exec.py  # ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å®Ÿè¡Œ")
    print("2. streamlit run RAGapp_optimized.py  # æœ€é©åŒ–ã‚¢ãƒ—ãƒªèµ·å‹•")
    print("\nğŸ’¡ æœŸå¾…ã•ã‚Œã‚‹åŠ¹æœ:")
    print("- APIæ–™é‡‘: ã‚¼ãƒ­ï¼ˆå®Œå…¨ãƒ­ãƒ¼ã‚«ãƒ«ï¼‰")
    print("- æ¨è«–é€Ÿåº¦: å¤§å¹…å‘ä¸Š")
    print("- å›ç­”å“è³ª: å°‚é–€ãƒ‡ãƒ¼ã‚¿ã§æœ€é©åŒ–")
    print("- ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼: 100%ä¿è­·")

if __name__ == "__main__":
    main()