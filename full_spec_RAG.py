# full_spec_RAG.py - ãƒ•ãƒ«ã‚¹ãƒšãƒƒã‚¯ç‰ˆRAGã‚·ã‚¹ãƒ†ãƒ 
# ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ« + æœ€é«˜æ€§èƒ½RAG = å®Œå…¨ãªã‚¨ãƒ³ã‚¿ãƒ¼ãƒ—ãƒ©ã‚¤ã‚ºRAGã‚·ã‚¹ãƒ†ãƒ 

import os
import sys
import warnings
warnings.filterwarnings("ignore")

# PATHè¨­å®š
sys.path.append('/home/ncnadmin/.local/bin')
os.environ['PATH'] = '/home/ncnadmin/.local/bin:' + os.environ.get('PATH', '')

import streamlit as st
import pandas as pd
import time
import json
import torch
import gc
import logging
from typing import List, Dict, Any, Optional
from datetime import datetime
from pathlib import Path

# Transformersé–¢é€£
from transformers import (
    AutoModelForCausalLM, AutoTokenizer, 
    BitsAndBytesConfig, GenerationConfig
)
from peft import PeftModel

# RAGé–¢é€£
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import Chroma
from langchain.schema import Document
import chromadb

# è¨­å®šèª­ã¿è¾¼ã¿
from config import Config, setup_logging

class FullSpecRAGSystem:
    """ãƒ•ãƒ«ã‚¹ãƒšãƒƒã‚¯ç‰ˆRAGã‚·ã‚¹ãƒ†ãƒ  - ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«çµ±åˆ"""
    
    def __init__(self, config: Config):
        self.config = config
        self.logger = setup_logging(config, "full_spec_rag.log")
        
        # ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹
        self.model = None
        self.tokenizer = None
        self.embeddings = None
        self.vectorstore = None
        self.text_splitter = None
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¿½è·¡
        self.performance_stats = {
            'total_queries': 0,
            'avg_response_time': 0.0,
            'cache_hits': 0,
            'model_load_time': 0.0
        }
        
        # åˆæœŸåŒ–
        self.logger.info("ğŸš€ ãƒ•ãƒ«ã‚¹ãƒšãƒƒã‚¯ç‰ˆRAGã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–é–‹å§‹")
        self._initialize_system()
    
    def _initialize_system(self):
        """ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–"""
        try:
            # GPUæƒ…å ±ç¢ºèª
            self._check_gpu_status()
            
            # ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆåˆæœŸåŒ–
            self._setup_text_splitter()
            self._setup_embeddings()
            self._setup_vectorstore()
            self._load_finetuned_model()
            
            self.logger.info("âœ… ãƒ•ãƒ«ã‚¹ãƒšãƒƒã‚¯ç‰ˆRAGã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
            
        except Exception as e:
            self.logger.error(f"âŒ ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
            raise
    
    def _check_gpu_status(self):
        """GPUçŠ¶æ…‹ç¢ºèª"""
        if torch.cuda.is_available():
            gpu_count = torch.cuda.device_count()
            current_gpu = torch.cuda.current_device()
            gpu_name = torch.cuda.get_device_name(current_gpu)
            total_memory = torch.cuda.get_device_properties(current_gpu).total_memory / 1e9
            
            self.logger.info(f"ğŸ® GPUåˆ©ç”¨å¯èƒ½: {gpu_name}")
            self.logger.info(f"   GPUæ•°: {gpu_count}")
            self.logger.info(f"   VRAM: {total_memory:.1f}GB")
            
            # ãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢
            torch.cuda.empty_cache()
            gc.collect()
            
        else:
            self.logger.warning("âš ï¸ GPUåˆ©ç”¨ä¸å¯ - CPUå‹•ä½œ")
    
    def _setup_text_splitter(self):
        """ãƒ†ã‚­ã‚¹ãƒˆåˆ†å‰²å™¨è¨­å®š"""
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=self.config.chunk_size,
            chunk_overlap=self.config.chunk_overlap,
            length_function=len,
            separators=["\n\n", "\n", "ã€‚", ".", " ", ""]
        )
        self.logger.info(f"ğŸ“„ ãƒ†ã‚­ã‚¹ãƒˆåˆ†å‰²å™¨è¨­å®šå®Œäº† (chunk_size: {self.config.chunk_size})")
    
    def _setup_embeddings(self):
        """åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«è¨­å®š"""
        try:
            # åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã¯CPUä½¿ç”¨ï¼ˆå®‰å®šæ€§å„ªå…ˆï¼‰
            model_kwargs = {'device': 'cpu'}
            encode_kwargs = {'normalize_embeddings': True}
            
            self.embeddings = HuggingFaceEmbeddings(
                model_name=self.config.embeddings_model,
                model_kwargs=model_kwargs,
                encode_kwargs=encode_kwargs
            )
            
            self.logger.info(f"ğŸ”¤ åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†: {self.config.embeddings_model}")
            
        except Exception as e:
            self.logger.error(f"âŒ åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            raise
    
    def _setup_vectorstore(self):
        """ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢è¨­å®š"""
        try:
            # æ—¢å­˜ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ç¢ºèª
            if os.path.exists(self.config.persist_directory):
                self.logger.info(f"ğŸ“ æ—¢å­˜ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ç™ºè¦‹: {self.config.persist_directory}")
                
                # Chromaãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢èª­ã¿è¾¼ã¿
                self.vectorstore = Chroma(
                    persist_directory=self.config.persist_directory,
                    collection_name=self.config.collection_name,
                    embedding_function=self.embeddings
                )
                
                # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°ç¢ºèª
                try:
                    collection = self.vectorstore._collection
                    document_count = collection.count()
                    self.logger.info(f"âœ… ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢èª­ã¿è¾¼ã¿å®Œäº†: {document_count}ä»¶ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ")
                except:
                    self.logger.info("âœ… ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢èª­ã¿è¾¼ã¿å®Œäº†")
                    
            else:
                self.logger.warning("âš ï¸ ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                self.logger.info("ğŸ’¡ æ–°ã—ã„ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã‚’ä½œæˆã—ã¾ã™")
                # æ–°ã—ã„ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã‚’ä½œæˆ
                self.vectorstore = Chroma.from_documents(
                    documents=[], # åˆæœŸãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¯ç©º
                    embedding=self.embeddings,
                    collection_name=self.config.collection_name,
                    persist_directory=self.config.persist_directory
                )
                self.vectorstore.persist()
                self.logger.info("âœ… æ–°ã—ã„ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ãŒä½œæˆã•ã‚Œã¾ã—ãŸ")
            
        except Exception as e:
            self.logger.error(f"âŒ ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢è¨­å®šã‚¨ãƒ©ãƒ¼: {e}")
            self.vectorstore = None
    
    def retrieve_documents(self, query: str, k: int = 5) -> List[Document]:
        """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ¤œç´¢"""
        try:
            if not self.vectorstore:
                self.logger.warning("âš ï¸ ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
                return []
            
            # ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢
            docs = self.vectorstore.similarity_search(query, k=k)
            
            self.logger.debug(f"ğŸ” æ¤œç´¢å®Ÿè¡Œ: ã‚¯ã‚¨ãƒª='{query}', çµæœ={len(docs)}ä»¶")
            
            return docs
            
        except Exception as e:
            self.logger.error(f"âŒ ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def _load_finetuned_model(self):
        """ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿"""
        try:
            start_time = time.time()
            self.logger.info(f"ğŸ¤– ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿é–‹å§‹")
            self.logger.info(f"   ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«: {self.config.base_model_name}")
            self.logger.info(f"   LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼: {self.config.lora_adapter_path}")
            
            # ãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢
            torch.cuda.empty_cache()
            gc.collect()
            
            # é‡å­åŒ–è¨­å®š
            quantization_config = None
            if self.config.use_4bit_quant:
                quantization_config = BitsAndBytesConfig(
                    load_in_4bit=True,
                    bnb_4bit_quant_type="nf4",
                    bnb_4bit_compute_dtype=self.config.quant_compute_dtype,
                    bnb_4bit_use_double_quant=True,
                )
                self.logger.info("âš™ï¸ 4bité‡å­åŒ–ã‚’æœ‰åŠ¹åŒ–")
            
            # Flash Attention 2 è¨­å®š
            attn_implementation = None
            if self.config.use_flash_attention_2:
                attn_implementation = "flash_attention_2"
                self.logger.info("âš¡ Flash Attention 2 ã‚’æœ‰åŠ¹åŒ–")
            
            device_map = "auto"
            self.logger.info("ğŸ® GPUä½¿ç”¨ãƒ¢ãƒ¼ãƒ‰")
            
            # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼èª­ã¿è¾¼ã¿
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.config.base_model_name,
                trust_remote_code=True,
                padding_side="left"
            )
            
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            # ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ï¼ˆã‚·ãƒ³ãƒ—ãƒ«è¨­å®šï¼‰
            base_model = AutoModelForCausalLM.from_pretrained(
                self.config.base_model_name,
                torch_dtype=self.config.model_load_dtype,
                trust_remote_code=True,
                device_map=device_map,
                quantization_config=quantization_config,
                attn_implementation=attn_implementation
            )
            
            # LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼é©ç”¨
            if os.path.exists(self.config.lora_adapter_path):
                try:
                    self.model = PeftModel.from_pretrained(
                        base_model,
                        self.config.lora_adapter_path,
                        torch_dtype=self.config.model_load_dtype
                    )
                    self.logger.info("âœ… LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼é©ç”¨å®Œäº†")
                except RuntimeError as e:
                    self.logger.error(f"âŒ LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼é©ç”¨ã‚¨ãƒ©ãƒ¼: {e}. ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®ã¿ä½¿ç”¨ã—ã¾ã™ã€‚")
                    self.model = base_model
            else:
                self.logger.warning(f"âš ï¸ LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {self.config.lora_adapter_path}")
                self.logger.info("ğŸ“ ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®ã¿ä½¿ç”¨")
                self.model = base_model
            
            # æ¨è«–ãƒ¢ãƒ¼ãƒ‰è¨­å®š
            self.model.eval()
            
            load_time = time.time() - start_time
            self.performance_stats['model_load_time'] = load_time
            
            self.logger.info(f"ğŸ‰ ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº† ({load_time:.2f}ç§’)")
            
            # ãƒ¢ãƒ‡ãƒ«æƒ…å ±è¡¨ç¤º
            if hasattr(self.model, 'num_parameters'):
                try:
                    total_params = self.model.num_parameters()
                    trainable_params = self.model.num_parameters(only_trainable=True) if hasattr(self.model, 'num_parameters') else 0
                    self.logger.info(f"   ç·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {total_params:,}")
                    self.logger.info(f"   å­¦ç¿’å¯èƒ½ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {trainable_params:,}")
                except:
                    pass
            
        except Exception as e:
            self.logger.error(f"âŒ ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            raise
    
    
    def generate_response(self, query: str, context_docs: List[Document]) -> str:
        """ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã§å›ç­”ç”Ÿæˆ"""
        try:
            if not context_docs:
                # ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ãŒãªã„å ´åˆã¯ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®ã¿ã§å›ç­”
                context = "é–¢é€£è³‡æ–™ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
            else:
                # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ§‹ç¯‰
                context_parts = []
                for i, doc in enumerate(context_docs[:3], 1):  # ä¸Šä½3ä»¶
                    content = doc.page_content.strip()
                    source_info = ""
                    if doc.metadata:
                        company = doc.metadata.get('company', '')
                        category = doc.metadata.get('category', '')
                        if company and category:
                            source_info = f"[{company} - {category}]"
                    
                    context_parts.append(f"å‚è€ƒæƒ…å ±{i}: {source_info}\n{content}")
                
                context = "\n\n".join(context_parts)
            
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰
            prompt = self.config.intermediate_prompt_template.format(
                context=context,
                question=query
            )
            
            # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚º
            inputs = self.tokenizer(
                prompt,
                return_tensors="pt",
                truncation=True,
                max_length=1024,
                padding=True
            )
            
            # ãƒ‡ãƒã‚¤ã‚¹è¨­å®š
            if torch.cuda.is_available():
                inputs = {k: v.to('cuda') for k, v in inputs.items()}
            
            # ç”Ÿæˆè¨­å®š
            generation_config_params = {
                "max_new_tokens": self.config.max_new_tokens,
                "repetition_penalty": self.config.repetition_penalty,
                "do_sample": self.config.do_sample,
                "num_beams": self.config.num_beams,
                "pad_token_id": self.tokenizer.pad_token_id,
                "eos_token_id": self.tokenizer.eos_token_id,
                "use_cache": True
            }

            if self.config.do_sample:
                generation_config_params["temperature"] = self.config.temperature
                generation_config_params["top_p"] = self.config.top_p

            generation_config = GenerationConfig(**generation_config_params)
            
            # å›ç­”ç”Ÿæˆ
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    generation_config=generation_config
                )
            
            # ãƒ‡ã‚³ãƒ¼ãƒ‰
            input_length = inputs['input_ids'].shape[1]
            generated_tokens = outputs[0][input_length:]
            response = self.tokenizer.decode(generated_tokens, skip_special_tokens=True)
            
            # å¾Œå‡¦ç†
            response = response.strip()
            if not response:
                response = "ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ãŒã€é©åˆ‡ãªå›ç­”ã‚’ç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸã€‚"
            
            return response
            
        except Exception as e:
            self.logger.error(f"âŒ å›ç­”ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            return f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"
    
    def query_rag(self, user_query: str, k: int = 5) -> Dict[str, Any]:
        """RAGã‚¯ã‚¨ãƒªå®Ÿè¡Œï¼ˆãƒ¡ã‚¤ãƒ³é–¢æ•°ï¼‰"""
        start_time = time.time()
        
        try:
            self.logger.info(f"ğŸ“ RAGã‚¯ã‚¨ãƒªé–‹å§‹: {user_query}")
            
            # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ¤œç´¢
            docs = self.retrieve_documents(user_query, k=k)
            
            # å›ç­”ç”Ÿæˆ
            response = self.generate_response(user_query, docs)
            
            end_time = time.time()
            processing_time = end_time - start_time
            
            # çµ±è¨ˆæ›´æ–°
            self.performance_stats['total_queries'] += 1
            current_avg = self.performance_stats['avg_response_time']
            total_queries = self.performance_stats['total_queries']
            self.performance_stats['avg_response_time'] = (
                (current_avg * (total_queries - 1) + processing_time) / total_queries
            )
            
            self.logger.info(f"âœ… RAGã‚¯ã‚¨ãƒªå®Œäº† ({processing_time:.3f}ç§’)")
            
            return {
                'answer': response,
                'retrieved_docs': docs,
                'processing_time': processing_time,
                'query': user_query,
                'timestamp': datetime.now().isoformat(),
                'model_used': 'FineTuned + LoRA',
                'retrieval_count': len(docs)
            }
            
        except Exception as e:
            self.logger.error(f"âŒ RAGã‚¯ã‚¨ãƒªã‚¨ãƒ©ãƒ¼: {e}")
            return {
                'answer': f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}",
                'retrieved_docs': [],
                'processing_time': time.time() - start_time,
                'query': user_query,
                'timestamp': datetime.now().isoformat(),
                'model_used': 'Error',
                'retrieval_count': 0
            }
    
    def get_system_status(self) -> Dict[str, Any]:
        """ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹å–å¾—"""
        status = {
            'model_loaded': self.model is not None,
            'tokenizer_loaded': self.tokenizer is not None,
            'vectorstore_ready': self.vectorstore is not None,
            'embeddings_ready': self.embeddings is not None,
            'performance_stats': self.performance_stats.copy()
        }
        
        # GPUæƒ…å ±
        if torch.cuda.is_available():
            status['gpu_available'] = True
            status['gpu_name'] = torch.cuda.get_device_name(0)
            status['gpu_memory_total'] = torch.cuda.get_device_properties(0).total_memory / 1e9
            status['gpu_memory_allocated'] = torch.cuda.memory_allocated(0) / 1e9
        else:
            status['gpu_available'] = False
        
        # ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢æƒ…å ±
        if self.vectorstore:
            try:
                collection = self.vectorstore._collection
                status['document_count'] = collection.count()
            except:
                status['document_count'] = 'Unknown'
        
        return status

def create_streamlit_app():
    """Streamlitã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ä½œæˆ"""
    st.set_page_config(
        page_title="ãƒ•ãƒ«ã‚¹ãƒšãƒƒã‚¯ç‰ˆRAG",
        layout="wide",
        initial_sidebar_state="expanded"
    )
    
    st.title("ğŸš€ ãƒ•ãƒ«ã‚¹ãƒšãƒƒã‚¯ç‰ˆRAGã‚·ã‚¹ãƒ†ãƒ ")
    st.caption("ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ« + æœ€é«˜æ€§èƒ½RAG = ã‚¨ãƒ³ã‚¿ãƒ¼ãƒ—ãƒ©ã‚¤ã‚ºã‚°ãƒ¬ãƒ¼ãƒ‰")
    
    # è¨­å®šèª­ã¿è¾¼ã¿
    config = Config()
    
    # ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
    @st.cache_resource
    def load_rag_system():
        return FullSpecRAGSystem(config)
    
    try:
        rag_system = load_rag_system()
        system_status = rag_system.get_system_status()
        
    except Exception as e:
        st.error(f"âŒ ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
        st.stop()
    
    # ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆæ§‹æˆ
    col1, col2 = st.columns([2, 1])
    
    with col2:
        # ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±ã‚µã‚¤ãƒ‰ãƒãƒ¼
        st.header("ğŸ”§ ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹")
        
        # åŸºæœ¬æƒ…å ±
        if system_status['model_loaded']:
            st.success("âœ… ãƒ¢ãƒ‡ãƒ«: èª­ã¿è¾¼ã¿å®Œäº†")
        else:
            st.error("âŒ ãƒ¢ãƒ‡ãƒ«: èª­ã¿è¾¼ã¿å¤±æ•—")
        
        if system_status['vectorstore_ready']:
            st.success("âœ… ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢: æº–å‚™å®Œäº†")
            if 'document_count' in system_status:
                st.metric("ğŸ“„ ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°", f"{system_status['document_count']:,}")
        else:
            st.error("âŒ ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢: æº–å‚™æœªå®Œäº†")
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹çµ±è¨ˆ
        st.subheader("ğŸ“Š ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹")
        perf = system_status['performance_stats']
        st.metric("âš¡ å‡¦ç†æ¸ˆã¿ã‚¯ã‚¨ãƒª", perf['total_queries'])
        st.metric("â± å¹³å‡å¿œç­”æ™‚é–“", f"{perf['avg_response_time']:.3f}ç§’")
        st.metric("ğŸš€ ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿æ™‚é–“", f"{perf['model_load_time']:.2f}ç§’")
        
        # GPUæƒ…å ±
        if system_status['gpu_available']:
            st.subheader("ğŸ® GPUæƒ…å ±")
            st.info(f"GPU: {system_status['gpu_name']}")
            st.metric("VRAM Total", f"{system_status['gpu_memory_total']:.1f}GB")
            st.metric("VRAMä½¿ç”¨ä¸­", f"{system_status['gpu_memory_allocated']:.2f}GB")
        else:
            st.warning("ğŸ’» CPUå‹•ä½œ")
        
        # ãƒ¢ãƒ‡ãƒ«æƒ…å ±
        st.subheader("ğŸ¤– ãƒ¢ãƒ‡ãƒ«æƒ…å ±")
        st.info(f"**ãƒ™ãƒ¼ã‚¹**: {config.base_model_name}")
        st.info(f"**LoRA**: {os.path.basename(config.lora_adapter_path)}")
        st.info(f"**åŸ‹ã‚è¾¼ã¿**: {config.embeddings_model}")
        
        # æœ€é©åŒ–è¨­å®š
        st.subheader("âš™ï¸ æœ€é©åŒ–è¨­å®š")
        if config.use_4bit_quant:
            st.success("âœ… 4bité‡å­åŒ–")
        if config.use_flash_attention_2:
            st.success("âœ… Flash Attention 2")
        
        # ã‚·ã‚¹ãƒ†ãƒ ç‰¹å¾´
        st.subheader("âœ¨ ã‚·ã‚¹ãƒ†ãƒ ç‰¹å¾´")
        features = [
            "ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«",
            "LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼çµ±åˆ",
            "4bité‡å­åŒ–æœ€é©åŒ–",
            "Flash Attention 2å¯¾å¿œ",
            "ãƒãƒ«ãƒGPUå¯¾å¿œ",
            "ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ¤œç´¢",
            "ã‚¨ãƒ³ã‚¿ãƒ¼ãƒ—ãƒ©ã‚¤ã‚ºã‚°ãƒ¬ãƒ¼ãƒ‰"
        ]
        
        for feature in features:
            st.success(f"âœ… {feature}")
    
    with col1:
        # ãƒ¡ã‚¤ãƒ³ãƒãƒ£ãƒƒãƒˆã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹
        st.header("ğŸ’¬ RAGãƒãƒ£ãƒƒãƒˆ")
        
        # ã‚µãƒ³ãƒ—ãƒ«è³ªå•
        st.subheader("ğŸ’¡ ã‚µãƒ³ãƒ—ãƒ«è³ªå•")
        sample_cols = st.columns(2)
        
        sample_queries = [
            "å»ºç¯‰åŸºæº–æ³•ã®å£é¢æä»•æ§˜ã«ã¤ã„ã¦æ•™ãˆã¦",
            "è€åŠ›å£ã®åŸºæº–ã¯ä½•ã§ã™ã‹ï¼Ÿ",
            "åŸºç¤ã®ç«‹ä¸Šé«˜ã•ã®è¦å®šã¯ï¼Ÿ",
            "é‹¼è£½æŸã®ä½¿ç”¨åŸºæº–ã«ã¤ã„ã¦",
            "æ§‹é€ æã®å“è³ªåŸºæº–ã¯ï¼Ÿ",
            "é‡‘ç‰©ã®å–ã‚Šä»˜ã‘æ–¹æ³•ã¯ï¼Ÿ"
        ]
        
        for i, sample in enumerate(sample_queries):
            col = sample_cols[i % 2]
            if col.button(sample, key=f"sample_{i}"):
                st.session_state['sample_query'] = sample
        
        # ãƒãƒ£ãƒƒãƒˆå±¥æ­´åˆæœŸåŒ–
        if "messages" not in st.session_state:
            st.session_state.messages = []
        
        # ã‚µãƒ³ãƒ—ãƒ«è³ªå•å‡¦ç†
        user_input = None
        if 'sample_query' in st.session_state:
            user_input = st.session_state['sample_query']
            del st.session_state['sample_query']
        
        # ä¼šè©±å±¥æ­´è¡¨ç¤º
        chat_container = st.container()
        with chat_container:
            for message in st.session_state.messages:
                with st.chat_message(message["role"]):
                    st.markdown(message["content"])
        
        # è³ªå•å…¥åŠ›
        if not user_input:
            user_input = st.chat_input("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„...")
        
        if user_input:
            # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸è¡¨ç¤º
            with st.chat_message("user"):
                st.markdown(user_input)
            st.session_state.messages.append({"role": "user", "content": user_input})
            
            # AIå¿œç­”ç”Ÿæˆ
            with st.chat_message("assistant"):
                with st.spinner("ğŸ” ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã§åˆ†æä¸­..."):
                    result = rag_system.query_rag(user_input)
                
                # å›ç­”è¡¨ç¤º
                st.markdown(result["answer"])
                
                # è©³ç´°æƒ…å ±å±•é–‹
                with st.expander("ğŸ“Š å‡¦ç†è©³ç´°"):
                    detail_cols = st.columns(4)
                    
                    with detail_cols[0]:
                        st.metric("å‡¦ç†æ™‚é–“", f"{result['processing_time']:.3f}ç§’")
                    with detail_cols[1]:
                        st.metric("æ¤œç´¢çµæœæ•°", result['retrieval_count'])
                    with detail_cols[2]:
                        st.metric("ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«", result['model_used'])
                    with detail_cols[3]:
                        st.metric("å‡¦ç†æ™‚åˆ»", result['timestamp'].split('T')[1][:8])
                    
                    # æ¤œç´¢ã•ã‚ŒãŸãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆè¡¨ç¤º
                    if result['retrieved_docs']:
                        st.subheader("ğŸ¯ æ¤œç´¢ã•ã‚ŒãŸãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ")
                        for i, doc in enumerate(result['retrieved_docs']):
                            with st.expander(f"ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ {i+1}"):
                                st.text_area("å†…å®¹", doc.page_content, height=100, disabled=True)
                                if doc.metadata:
                                    st.json(doc.metadata)
            
            # å›ç­”ã‚’ã‚»ãƒƒã‚·ãƒ§ãƒ³ã«è¿½åŠ 
            st.session_state.messages.append({"role": "assistant", "content": result["answer"]})
        
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³çµ±è¨ˆ
        if st.session_state.messages:
            st.subheader("ğŸ“ˆ ã‚»ãƒƒã‚·ãƒ§ãƒ³çµ±è¨ˆ")
            user_messages = [m for m in st.session_state.messages if m["role"] == "user"]
            assistant_messages = [m for m in st.session_state.messages if m["role"] == "assistant"]
            
            stat_cols = st.columns(3)
            with stat_cols[0]:
                st.metric("è³ªå•æ•°", len(user_messages))
            with stat_cols[1]:
                st.metric("å›ç­”æ•°", len(assistant_messages))
            with stat_cols[2]:
                total_chars = sum(len(m["content"]) for m in assistant_messages)
                st.metric("ç”Ÿæˆæ–‡å­—æ•°", f"{total_chars:,}")

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    try:
        create_streamlit_app()
    except KeyboardInterrupt:
        print("\nğŸ›‘ ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³çµ‚äº†")
    except Exception as e:
        print(f"âŒ ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚¨ãƒ©ãƒ¼: {e}")

if __name__ == "__main__":
    main()