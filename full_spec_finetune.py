# full_spec_finetune.py - ãƒ•ãƒ«ã‚¹ãƒšãƒƒã‚¯ç‰ˆãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°
# æœ€é«˜æ€§èƒ½ãƒ»å¤šæ©Ÿèƒ½ãƒ»é«˜åº¦ãªè¨­å®šã‚’çµ±åˆã—ãŸãƒ—ãƒ­ãƒ•ã‚§ãƒƒã‚·ãƒ§ãƒŠãƒ«ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°

import torch
import os
import gc
import json
import logging
import wandb
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Any
from datasets import load_dataset, load_from_disk, Dataset, DatasetDict
from transformers import (
    AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments,
    DataCollatorForSeq2Seq, EarlyStoppingCallback, ProgressCallback,
    set_seed, TrainerCallback
)
from peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training
from transformers import BitsAndBytesConfig
# # import deepspeed

class FullSpecFineTuner:
    """ãƒ•ãƒ«ã‚¹ãƒšãƒƒã‚¯ç‰ˆãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, config_path: Optional[str] = None):
        self.setup_logging()
        self.config = self.load_config(config_path)
        self.setup_directories()
        self.model = None
        self.tokenizer = None
        self.best_eval_loss = float('inf')
        
        # Weights & BiasesåˆæœŸåŒ–
        if self.config.get('use_wandb', False):
            wandb.init(
                project=self.config.get('wandb_project', 'rag-finetune'),
                name=self.config.get('run_name', f'run_{datetime.now().strftime("%Y%m%d_%H%M%S")}'),
                config=self.config
            )
    
    def load_config(self, config_path: Optional[str] = None) -> Dict[str, Any]:
        """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿"""
        default_config = {
            # ãƒ¢ãƒ‡ãƒ«è¨­å®š
            "model_name": "Qwen/Qwen1.5-1.8B",
            "model_alternatives": [
                "elyza/ELYZA-japanese-Llama-2-13b-instruct",
                "elyza/ELYZA-japanese-Llama-2-7b-instruct",
                "stabilityai/japanese-stablelm-instruct-alpha-7b-v2"
            ],
            
            # ãƒ‡ãƒ¼ã‚¿è¨­å®š
            "train_data_path": "/home/ncnadmin/my_rag_project/enhanced_training_dataset.jsonl",
            "eval_data_path": None,  # è‡ªå‹•åˆ†å‰²
            "validation_split": 0.1,
            "max_train_samples": None,  # å…¨ãƒ‡ãƒ¼ã‚¿ä½¿ç”¨
            "max_eval_samples": 1000,
            
            # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼è¨­å®š
            "max_length": 512,
            "padding_side": "right",
            "truncation_strategy": "longest_first",
            
            # ãƒ¢ãƒ‡ãƒ«æœ€é©åŒ–è¨­å®š
            "use_quantization": True,
            "quantization_config": {
                "load_in_8bit": True,
            },
            "use_gradient_checkpointing": True,
            "use_flash_attention": False,
            
            # LoRAè¨­å®š
            "lora_config": {
                "r": 16,
                "lora_alpha": 32,
                "lora_dropout": 0.1,
                "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
                "bias": "none",
                "task_type": "CAUSAL_LM"
            },
            
            # å­¦ç¿’è¨­å®š
            "num_train_epochs": 3,
            "per_device_train_batch_size": 1,
            "per_device_eval_batch_size": 1,
            "gradient_accumulation_steps": 16,
            "learning_rate": 2e-4,
            "weight_decay": 0.01,
            "warmup_ratio": 0.03,
            "lr_scheduler_type": "cosine",
            "max_grad_norm": 1.0,
            
            # è©•ä¾¡ãƒ»ä¿å­˜è¨­å®š
            "evaluation_strategy": "steps",
            "eval_steps": 200,
            "save_strategy": "steps",
            "save_steps": 200,
            "save_total_limit": 3,
            "load_best_model_at_end": True,
            "metric_for_best_model": "eval_loss",
            "greater_is_better": False,
            
            # ãƒ­ã‚°è¨­å®š
            "logging_strategy": "steps",
            "logging_steps": 50,
            "report_to": ["tensorboard"],
            "use_wandb": False,
            "wandb_project": "rag-finetune",
            
            # æœ€é©åŒ–è¨­å®š
            "optim": "paged_adamw_8bit",
            "adam_beta1": 0.9,
            "adam_beta2": 0.999,
            "adam_epsilon": 1e-8,
            "max_steps": -1,
            
            # ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼è¨­å®š
            "dataloader_num_workers": 4,
            "dataloader_pin_memory": True,
            "group_by_length": True,
            "length_column_name": "length",
            
            # é«˜åº¦ãªè¨­å®š
            "use_early_stopping": True,
            "early_stopping_patience": 3,
            "early_stopping_threshold": 0.001,
            "use_deepspeed": False,
            "deepspeed_config": None,
            
            # å‡ºåŠ›è¨­å®š
            "output_dir": "./full_spec_rag_model",
            "cache_dir": "cache/full_spec",
            "run_name": f"full_spec_run_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
            
            # GPUæœ€é©åŒ–
            "bf16": True,
            "fp16": False,
            "tf32": True,
            "dataloader_drop_last": False,
            "prediction_loss_only": True,
            
            # å†ç¾æ€§
            "seed": 42,
            "data_seed": 42,
        }
        
        if config_path and os.path.exists(config_path):
            with open(config_path, 'r', encoding='utf-8') as f:
                user_config = json.load(f)
                default_config.update(user_config)
        
        return default_config
    
    def setup_logging(self):
        """ãƒ­ã‚°è¨­å®š"""
        log_format = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        logging.basicConfig(
            level=logging.INFO,
            format=log_format,
            handlers=[
                logging.FileHandler(f'finetune_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    def setup_directories(self):
        """ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ"""
        dirs = [
            self.config['output_dir'],
            self.config['cache_dir'],
            os.path.dirname(self.config['cache_dir'] + '/tokenized'),
            'logs',
            'checkpoints'
        ]
        for dir_path in dirs:
            Path(dir_path).mkdir(parents=True, exist_ok=True)
    
    def check_environment(self) -> Dict[str, Any]:
        """å®Ÿè¡Œç’°å¢ƒãƒã‚§ãƒƒã‚¯"""
        self.logger.info("=== å®Ÿè¡Œç’°å¢ƒãƒã‚§ãƒƒã‚¯ ===")
        
        env_info = {
            "cuda_available": torch.cuda.is_available(),
            "gpu_count": torch.cuda.device_count() if torch.cuda.is_available() else 0,
            "pytorch_version": torch.__version__,
        }
        
        if env_info["cuda_available"]:
            current_gpu = torch.cuda.current_device()
            env_info.update({
                "current_gpu": current_gpu,
                "gpu_name": torch.cuda.get_device_name(current_gpu),
                "total_memory": torch.cuda.get_device_properties(current_gpu).total_memory / 1e9,
                "allocated_memory": torch.cuda.memory_allocated(current_gpu) / 1e9,
                "reserved_memory": torch.cuda.memory_reserved(current_gpu) / 1e9,
            })
            
            self.logger.info(f"âœ… CUDAåˆ©ç”¨å¯èƒ½ - {env_info['gpu_name']}")
            self.logger.info(f"   VRAM: {env_info['total_memory']:.1f}GB")
            self.logger.info(f"   ä½¿ç”¨ä¸­: {env_info['allocated_memory']:.2f}GB")
        else:
            self.logger.error("âŒ CUDAãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
            
        return env_info
    
    
    
    def load_and_prepare_data(self) -> Dataset:
        """ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ãƒ»å‰å‡¦ç†"""
        self.logger.info("ğŸ“Š ãƒ‡ãƒ¼ã‚¿æº–å‚™é–‹å§‹")
        
        # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        if self.config["train_data_path"].endswith('.jsonl'):
            raw_dataset = load_dataset("json", data_files={"train": self.config["train_data_path"]}, split="train")
        else:
            raw_dataset = load_dataset(self.config["train_data_path"], split="train")
        
        self.logger.info(f"å…ƒãƒ‡ãƒ¼ã‚¿: {len(raw_dataset):,}ä»¶")
        
        # ãƒ‡ãƒ¼ã‚¿å“è³ªãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        def quality_filter(example):
            # This dataset uses 'prompt' and 'response'
            input_text = example.get('prompt', '')
            output_text = example.get('response', '')
            
            # Loosen the filter for this specific dataset format
            if not input_text or not output_text:
                return False
            if len(input_text) > 2000 or len(output_text) > 1000:
                return False
                
            return True
        
        filtered_dataset = raw_dataset.filter(quality_filter)
        self.logger.info(f"å“è³ªãƒ•ã‚£ãƒ«ã‚¿å¾Œ: {len(filtered_dataset):,}ä»¶")
        
        # ã‚µãƒ³ãƒ—ãƒ«æ•°åˆ¶é™
        if self.config["max_train_samples"] and len(filtered_dataset) > self.config["max_train_samples"]:
            filtered_dataset = filtered_dataset.select(range(self.config["max_train_samples"]))
            self.logger.info(f"ã‚µãƒ³ãƒ—ãƒ«åˆ¶é™å¾Œ: {len(filtered_dataset):,}ä»¶")
        
        # å‰å‡¦ç†é–¢æ•°
        def preprocess_function(examples):
            # This dataset uses 'prompt' and 'response'
            input_text = examples.get("prompt", "")
            output_text = examples.get("response", "")
            
            input_text = input_text.strip()
            output_text = output_text.strip()
            
            # Simplified prompt for prediction task
            prompt = f"### å…¥åŠ›:\n{input_text}\n\n### å¿œç­”:\n"
            
            return {"prompt": prompt, "completion": output_text}
        
        # å‰å‡¦ç†é©ç”¨
        dataset = filtered_dataset.map(preprocess_function, remove_columns=filtered_dataset.column_names)
        
        # è©•ä¾¡ãƒ‡ãƒ¼ã‚¿åˆ†å‰²
        if self.config["eval_data_path"]:
            eval_dataset = load_dataset("json", data_files={"eval": self.config["eval_data_path"]}, split="eval")
            eval_dataset = eval_dataset.map(preprocess_function, remove_columns=eval_dataset.column_names)
            final_dataset = {"train": dataset, "eval": eval_dataset}
        else:
            split_dataset = dataset.train_test_split(
                test_size=self.config["validation_split"], 
                seed=self.config["data_seed"]
            )
            final_dataset = {"train": split_dataset["train"], "eval": split_dataset["test"]}
        
        self.logger.info(f"å­¦ç¿’ãƒ‡ãƒ¼ã‚¿: {len(final_dataset['train']):,}ä»¶")
        self.logger.info(f"è©•ä¾¡ãƒ‡ãƒ¼ã‚¿: {len(final_dataset['eval']):,}ä»¶")
        
        return final_dataset
    
    def setup_tokenizer(self) -> AutoTokenizer:
        """ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"""
        self.logger.info("ğŸ”¤ ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼æº–å‚™")
        
        tokenizer = AutoTokenizer.from_pretrained(
            self.config["model_name"], 
            trust_remote_code=True,
            padding_side=self.config["padding_side"]
        )
        
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        return tokenizer
    
    def tokenize_data(self, dataset: Dataset, tokenizer: AutoTokenizer) -> Dataset:
        """ãƒ‡ãƒ¼ã‚¿ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚º"""
        self.logger.info("âš¡ ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚ºå®Ÿè¡Œ")
        
        cache_path = os.path.join(self.config["cache_dir"], "tokenized")
        
        def tokenize_function(examples):
            prompts = examples["prompt"]
            completions = examples["completion"]
            
            full_texts = [p + c + tokenizer.eos_token for p, c in zip(prompts, completions)]
            
            model_inputs = tokenizer(
                full_texts,
                max_length=self.config["max_length"],
                truncation=True,
                padding=False,
            )
            
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆéƒ¨åˆ†ã‚’ãƒã‚¹ã‚¯
            prompt_inputs = tokenizer(prompts, truncation=True, padding=False)
            labels = []
            
            for i, input_ids in enumerate(model_inputs["input_ids"]):
                prompt_len = len(prompt_inputs["input_ids"][i])
                label = input_ids.copy()
                label[:prompt_len] = [-100] * prompt_len
                labels.append(label)
            
            model_inputs["labels"] = labels
            
            # é•·ã•æƒ…å ±è¿½åŠ ï¼ˆgroup_by_lengthç”¨ï¼‰
            model_inputs["length"] = [len(ids) for ids in model_inputs["input_ids"]]
            
            return model_inputs
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç¢ºèª
        try:
            tokenized_dataset = load_from_disk(cache_path)
            self.logger.info("âœ… ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿")
        except:
            self.logger.info("â³ ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚ºå®Ÿè¡Œä¸­...")
            tokenized_dataset = {}
            
            for split in dataset.keys():
                tokenized_dataset[split] = dataset[split].map(
                    tokenize_function,
                    batched=True,
                    remove_columns=["prompt", "completion"],
                    num_proc=self.config["dataloader_num_workers"]
                )
            
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜
            tokenized_dataset_obj = DatasetDict(tokenized_dataset)
            tokenized_dataset_obj.save_to_disk(cache_path)
            self.logger.info("âœ… ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚ºå®Œäº†ï¼†ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜")
        
        return tokenized_dataset
    
    def setup_model(self, tokenizer: AutoTokenizer) -> AutoModelForCausalLM:
        """ãƒ¢ãƒ‡ãƒ«ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"""
        self.logger.info(f"ğŸ¤– ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿: {self.config['model_name']}")
        
        # ãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢
        torch.cuda.empty_cache()
        gc.collect()
        
        # é‡å­åŒ–è¨­å®š
        bnb_config = None
        if self.config["use_quantization"]:
            qconfig = self.config["quantization_config"]
            bnb_config = BitsAndBytesConfig(
                load_in_8bit=qconfig.get("load_in_8bit", False),
            )
        
        # ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
        model = AutoModelForCausalLM.from_pretrained(
            self.config["model_name"],
            quantization_config=bnb_config,
            trust_remote_code=True,
            torch_dtype=torch.bfloat16 if self.config["bf16"] else torch.float16,
            attn_implementation="sdpa" if self.config["use_flash_attention"] else None,
            use_safetensors=True,
        )
        
        # å‹¾é…ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ
        if self.config["use_gradient_checkpointing"]:
            model.gradient_checkpointing_enable()
        
        # kbitå­¦ç¿’æº–å‚™
        if self.config["use_quantization"]:
            model = prepare_model_for_kbit_training(model)

        # LoRAè¨­å®š
        lora_config = LoraConfig(
            r=self.config["lora_config"]["r"],
            lora_alpha=self.config["lora_config"]["lora_alpha"],
            lora_dropout=self.config["lora_config"]["lora_dropout"],
            bias=self.config["lora_config"]["bias"],
            task_type=TaskType[self.config["lora_config"]["task_type"]],
            target_modules=self.config["lora_config"]["target_modules"],
        )
        model = get_peft_model(model, lora_config)
        
        
        
        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æƒ…å ±
        trainable_params = model.num_parameters(only_trainable=True)
        total_params = model.num_parameters()
        
        self.logger.info(f"âœ… LoRAé©ç”¨å®Œäº†")
        self.logger.info(f"   å­¦ç¿’å¯èƒ½ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {trainable_params:,}")
        self.logger.info(f"   å…¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {total_params:,}")
        self.logger.info(f"   å­¦ç¿’å¯èƒ½ç‡: {100 * trainable_params / total_params:.3f}%")
        
        return model
    
    class CustomCallback(TrainerCallback):
        """ã‚«ã‚¹ã‚¿ãƒ ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯"""
        
        def __init__(self, tuner):
            self.tuner = tuner
        
        def on_evaluate(self, args, state, control, model, logs=None, **kwargs):
            if logs and "eval_loss" in logs:
                if logs["eval_loss"] < self.tuner.best_eval_loss:
                    self.tuner.best_eval_loss = logs["eval_loss"]
                    self.tuner.logger.info(f"ğŸ¯ æ–°ã—ã„æœ€è‰¯è©•ä¾¡æå¤±: {logs['eval_loss']:.4f}")
    
    def run_training(self):
        """ãƒ¡ã‚¤ãƒ³å­¦ç¿’å®Ÿè¡Œ"""
        self.logger.info("ğŸš€ ãƒ•ãƒ«ã‚¹ãƒšãƒƒã‚¯ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°é–‹å§‹")
        
        # å†ç¾æ€§è¨­å®š
        set_seed(self.config["seed"])
        
        # ç’°å¢ƒãƒã‚§ãƒƒã‚¯ãƒ»æœ€é©åŒ–
        env_info = self.check_environment()
        
        
        # ãƒ‡ãƒ¼ã‚¿æº–å‚™
        dataset = self.load_and_prepare_data()
        
        # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼
        tokenizer = self.setup_tokenizer()
        
        # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚º
        tokenized_dataset = self.tokenize_data(dataset, tokenizer)
        
        # ãƒ¢ãƒ‡ãƒ«
        model = self.setup_model(tokenizer)
        
        # ãƒ‡ãƒ¼ã‚¿ã‚³ãƒ¬ã‚¯ã‚¿ãƒ¼
        data_collator = DataCollatorForSeq2Seq(
            tokenizer=tokenizer,
            padding="longest",
            label_pad_token_id=-100,
        )
        
        # å­¦ç¿’è¨­å®š
        training_args = TrainingArguments(
            output_dir=self.config["output_dir"],
            num_train_epochs=self.config["num_train_epochs"],
            per_device_train_batch_size=self.config["per_device_train_batch_size"],
            per_device_eval_batch_size=self.config["per_device_eval_batch_size"],
            gradient_accumulation_steps=self.config["gradient_accumulation_steps"],
            learning_rate=self.config["learning_rate"],
            weight_decay=self.config["weight_decay"],
            warmup_ratio=self.config["warmup_ratio"],
            lr_scheduler_type=self.config["lr_scheduler_type"],
            max_grad_norm=self.config["max_grad_norm"],
            
            eval_strategy=self.config["evaluation_strategy"],
            eval_steps=self.config["eval_steps"],
            save_strategy=self.config["save_strategy"],
            save_steps=self.config["save_steps"],
            save_total_limit=self.config["save_total_limit"],
            load_best_model_at_end=self.config["load_best_model_at_end"],
            metric_for_best_model=self.config["metric_for_best_model"],
            greater_is_better=self.config["greater_is_better"],
            
            logging_strategy=self.config["logging_strategy"],
            logging_steps=self.config["logging_steps"],
            report_to=self.config["report_to"],
            
            optim=self.config["optim"],
            adam_beta1=self.config["adam_beta1"],
            adam_beta2=self.config["adam_beta2"],
            adam_epsilon=self.config["adam_epsilon"],
            max_steps=self.config["max_steps"],
            
            dataloader_num_workers=self.config["dataloader_num_workers"],
            dataloader_pin_memory=self.config["dataloader_pin_memory"],
            group_by_length=self.config["group_by_length"],
            length_column_name=self.config["length_column_name"],
            
            bf16=self.config["bf16"],
            fp16=self.config["fp16"],
            tf32=self.config["tf32"],
            dataloader_drop_last=self.config["dataloader_drop_last"],
            prediction_loss_only=self.config["prediction_loss_only"],
            
            run_name=self.config["run_name"],
            seed=self.config["seed"],
            data_seed=self.config["data_seed"],
            
            remove_unused_columns=False,
        )
        
        # ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯æº–å‚™
        callbacks = [self.CustomCallback(self)]
        if self.config["use_early_stopping"]:
            callbacks.append(EarlyStoppingCallback(
                early_stopping_patience=self.config["early_stopping_patience"],
                early_stopping_threshold=self.config["early_stopping_threshold"]
            ))
        
        # ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ä½œæˆ
        trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=tokenized_dataset["train"],
            eval_dataset=tokenized_dataset["eval"],
            data_collator=data_collator,
            tokenizer=tokenizer,
            callbacks=callbacks
        )
        
        # å­¦ç¿’å®Ÿè¡Œ
        self.logger.info("ğŸ¯ å­¦ç¿’é–‹å§‹...")
        trainer.train()
        
        # ä¿å­˜
        self.logger.info("ğŸ’¾ ãƒ¢ãƒ‡ãƒ«ä¿å­˜ä¸­...")
        trainer.save_model()
        tokenizer.save_pretrained(self.config["output_dir"])
        
        # è¨­å®šä¿å­˜
        with open(os.path.join(self.config["output_dir"], "training_config.json"), 'w', encoding='utf-8') as f:
            json.dump(self.config, f, indent=2, ensure_ascii=False)
        
        self.logger.info("ğŸ‰ ãƒ•ãƒ«ã‚¹ãƒšãƒƒã‚¯å­¦ç¿’å®Œäº†ï¼")
        self.logger.info(f"ğŸ“ ä¿å­˜å…ˆ: {self.config['output_dir']}")
        self.logger.info(f"ğŸ† æœ€è‰¯è©•ä¾¡æå¤±: {self.best_eval_loss:.4f}")
        
        return trainer

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("ğŸš€ ãƒ•ãƒ«ã‚¹ãƒšãƒƒã‚¯ç‰ˆãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°")
    print("=" * 60)
    
    # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
    config_path = "/home/ncnadmin/my_rag_project/finetune_config.json"
    
    # ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å®Ÿè¡Œ
    tuner = FullSpecFineTuner(config_path)
    trainer = tuner.run_training()
    
    print("\nâœ… ã™ã¹ã¦å®Œäº†ï¼")
    print("\næ¬¡ã®æ‰‹é †:")
    print("1. config.py ã® lora_adapter_path ã‚’ä»¥ä¸‹ã«å¤‰æ›´:")
    print(f"   self.lora_adapter_path = '{tuner.config['output_dir']}'")
    print("2. ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ã‚’è©•ä¾¡")
    print("3. å¿…è¦ã«å¿œã˜ã¦vLLMæœ€é©åŒ–å®Ÿè¡Œ")

if __name__ == "__main__":
    main()