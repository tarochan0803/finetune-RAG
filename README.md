# ğŸš€ Advanced RAG System with Fine-tuning

ã‚¨ãƒ³ã‚¿ãƒ¼ãƒ—ãƒ©ã‚¤ã‚ºã‚°ãƒ¬ãƒ¼ãƒ‰ã®é«˜åº¦ãªRAGã‚·ã‚¹ãƒ†ãƒ  & ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°çµ±åˆç’°å¢ƒ

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://python.org)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)](https://pytorch.org)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

## ğŸŒŸ ç‰¹å¾´

### ğŸ¯ é«˜æ€§èƒ½RAGã‚·ã‚¹ãƒ†ãƒ 
- **ğŸ”¥ æœ€æ–°æŠ€è¡“çµ±åˆ**: ChromaDB + LoRA + é‡å­åŒ–æœ€é©åŒ–
- **âš¡ é«˜é€Ÿæ¤œç´¢**: é«˜åº¦ãªãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã¨ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢
- **ğŸ¨ ç¾ã—ã„UI**: Streamlit Webã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹
- **ğŸ“Š è©³ç´°åˆ†æ**: æ¤œç´¢ç²¾åº¦ãƒ»æ€§èƒ½ãƒ¡ãƒˆãƒªã‚¯ã‚¹
- **ğŸ’¡ è‡ªå‹•å…¥åŠ›**: RAGã‚’æ´»ç”¨ã—ãŸãƒ•ã‚©ãƒ¼ãƒ ã®è‡ªå‹•å…¥åŠ›æ©Ÿèƒ½

### ğŸ§  å…ˆé€²çš„ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°
- **ğŸ”„ ç¶™ç¶šå­¦ç¿’**: çŸ¥è­˜ã®å¿˜å´ã‚’é˜²ãEWCæŠ€è¡“
- **ğŸ“š ã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ å­¦ç¿’**: æ®µéšçš„ãªé›£æ˜“åº¦èª¿æ•´
- **ğŸ¤– è‡ªå‹•æœ€é©åŒ–**: GPUç’°å¢ƒã«å¿œã˜ãŸè¨­å®šè‡ªå‹•èª¿æ•´
- **ğŸ“ˆ åŒ…æ‹¬çš„è©•ä¾¡**: Weights & Biasesçµ±åˆ

### âš™ï¸ æŸ”è»Ÿãªè¨­å®š
- **ğŸ›ï¸ å¤šå±¤æ§‹æˆ**: è»½é‡ç‰ˆã‹ã‚‰ä¼æ¥­ç‰ˆã¾ã§å¯¾å¿œ
- **ğŸ”§ ãƒ—ãƒ©ã‚°ã‚¤ãƒ³å¯¾å¿œ**: MS-Swift + Megatron-Coreçµ±åˆ
- **ğŸ“Š è©³ç´°ãƒ­ã‚°**: TensorBoard + Wandb + ç‹¬è‡ªãƒ¡ãƒˆãƒªã‚¯ã‚¹

---

## ğŸš€ ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ

### 1ï¸âƒ£ ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
```bash
git clone https://github.com/tarochan0803/finetune-RAG.git
cd finetune-RAG
pip install -r requirements.txt
```

### 2ï¸âƒ£ ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆæ¨å¥¨ï¼‰
```bash
# é«˜æ€§èƒ½ç‰ˆ
python3 full_spec_finetune.py

# ã¾ãŸã¯è»½é‡ç‰ˆ
python3 auto_optimized_finetune.py
```

### 3ï¸âƒ£ è¨­å®šæ›´æ–°
`config.py`ã‚’é–‹ãã€`lora_adapter_path`ã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã®å‡ºåŠ›å…ˆã«è¨­å®šã—ã¾ã™ã€‚
```python
# config.py
lora_adapter_path = "./tourokuten_finetune_model_full"  # ä¾‹: ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å‡ºåŠ›å…ˆ
```

### 4ï¸âƒ£ RAGã‚·ã‚¹ãƒ†ãƒ èµ·å‹•
```bash
# Web UIä»˜ãé«˜æ€§èƒ½ç‰ˆ
streamlit run RAGapp.py
```

---

## ğŸ“ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ§‹æˆ

```
finetune-RAG/
â”œâ”€â”€ ğŸ¯ RAGã‚·ã‚¹ãƒ†ãƒ 
â”‚   â”œâ”€â”€ RAGapp.py                  # ãƒ¡ã‚¤ãƒ³ã®é«˜æ€§èƒ½RAGã‚¢ãƒ—ãƒªï¼ˆStreamlit UIï¼‰
â”‚   â”œâ”€â”€ final_rag_app.py           # (æ—§)é«˜æ€§èƒ½RAG
â”‚   â”œâ”€â”€ simple_rag_app.py          # (æ—§)è»½é‡RAG
â”‚   â””â”€â”€ immediate_rag_system.py    # å³å¸­RAG
â”œâ”€â”€ ğŸ§  ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°
â”‚   â”œâ”€â”€ full_spec_finetune.py      # ãƒ•ãƒ«ã‚¹ãƒšãƒƒã‚¯ç‰ˆï¼ˆæ¨å¥¨ï¼‰
â”‚   â”œâ”€â”€ auto_optimized_finetune.py # è‡ªå‹•æœ€é©åŒ–ç‰ˆ
â”‚   â”œâ”€â”€ continual_learning_finetune.py # ç¶™ç¶šå­¦ç¿’ç‰ˆ
â”‚   â”œâ”€â”€ scale_up_finetune.py       # å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ç‰ˆ
â”‚   â””â”€â”€ cpu_optimized_finetune.py  # CPUæœ€é©åŒ–ç‰ˆ
â”œâ”€â”€ ğŸ”§ çµ±åˆã‚·ã‚¹ãƒ†ãƒ 
â”‚   â””â”€â”€ megatron_swift_finetune.py # MS-Swift + Megatron
â”œâ”€â”€ âš™ï¸ è¨­å®šãƒ»ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
â”‚   â”œâ”€â”€ config.py                  # ãƒ¡ã‚¤ãƒ³è¨­å®š
â”‚   â”œâ”€â”€ utils.py                   # ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
â”‚   â”œâ”€â”€ rag_query_utils.py         # ã‚¯ã‚¨ãƒªå‡¦ç†
â”‚   â”œâ”€â”€ form_auto_fill.py          # ãƒ•ã‚©ãƒ¼ãƒ è‡ªå‹•å…¥åŠ›UI
â”‚   â”œâ”€â”€ auto_fill_utils.py         # ãƒ•ã‚©ãƒ¼ãƒ è‡ªå‹•å…¥åŠ›ãƒ­ã‚¸ãƒƒã‚¯
â”‚   â””â”€â”€ company_master.py          # å·¥å‹™åº—ãƒã‚¹ã‚¿æ©Ÿèƒ½
â”œâ”€â”€ ğŸ“Š ãƒ‡ãƒ¼ã‚¿å‡¦ç†
â”‚   â”œâ”€â”€ enhanced_training_data_generator.py
â”‚   â””â”€â”€ advanced_data_augmenter.py
â””â”€â”€ ğŸ“š ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ
    â”œâ”€â”€ README.md                  # ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«
    â”œâ”€â”€ USAGE_GUIDE.md             # è©³ç´°ä½¿ç”¨ã‚¬ã‚¤ãƒ‰
    â””â”€â”€ docs/                      # è¿½åŠ ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ
```

---

## ğŸ¯ ä½¿ç”¨æ–¹æ³•ï¼ˆç›®çš„åˆ¥ï¼‰

### ğŸ¥‡ åˆå¿ƒè€…å‘ã‘ï¼ˆæœ€çŸ­çµŒè·¯ï¼‰
```bash
# 1. è»½é‡ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆ30åˆ†ï¼‰
python3 auto_optimized_finetune.py

# 2. è¨­å®šæ›´æ–°
sed -i 's|lora_adapter_path = ".*"|lora_adapter_path = "./optimized_rag_model"|' config.py

# 3. RAGèµ·å‹•
streamlit run RAGapp.py
```

### ğŸ¥ˆ æ¨™æº–ãƒ¦ãƒ¼ã‚¶ãƒ¼å‘ã‘ï¼ˆé«˜å“è³ªï¼‰
```bash
# 1. é«˜æ€§èƒ½ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆ1-2æ™‚é–“ï¼‰
python3 full_spec_finetune.py

# 2. è¨­å®šæ›´æ–°
sed -i 's|lora_adapter_path = ".*"|lora_adapter_path = "./tourokuten_finetune_model_full"|' config.py

# 3. é«˜æ€§èƒ½RAGèµ·å‹•
streamlit run RAGapp.py
```

### ğŸ¥‰ ä¸Šç´šãƒ¦ãƒ¼ã‚¶ãƒ¼å‘ã‘ï¼ˆæœ€é«˜æ€§èƒ½ï¼‰
```bash
# 1. åˆå›ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°
python3 full_spec_finetune.py

# 2. ç¶™ç¶šå­¦ç¿’ï¼ˆè¤‡æ•°ã‚¿ã‚¹ã‚¯ï¼‰
python3 continual_learning_finetune.py --task-name "åŒ»ç™‚å¯¾è©±" --data-path "medical.jsonl"
python3 continual_learning_finetune.py --task-name "æŠ€è¡“Q&A" --data-path "tech.jsonl"

# 3. æ€§èƒ½è©•ä¾¡
python3 continual_learning_finetune.py --evaluate

# 4. RAGèµ·å‹•
streamlit run RAGapp.py
```

---

## ğŸ”§ è¨­å®šã‚ªãƒ—ã‚·ãƒ§ãƒ³

### GPUç’°å¢ƒåˆ¥æ¨å¥¨è¨­å®š

| GPU | VRAM | æ¨å¥¨ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚° | æ¨å¥¨RAG |
|-----|------|----------------------|---------|
| RTX 4090 | 24GB | `full_spec_finetune.py` | `RAGapp.py` |
| RTX 4080 | 16GB | `auto_optimized_finetune.py` | `RAGapp.py` |
| RTX 4070 | 12GB | `auto_optimized_finetune.py` | `RAGapp.py` |
| RTX 4060 | 8GB | `cpu_optimized_finetune.py` | `RAGapp.py` |

### ãƒ¢ãƒ‡ãƒ«é¸æŠ
```python
# config.py ã§å¤‰æ›´å¯èƒ½
base_model_name = "Qwen/Qwen1.5-1.8B"  # ç¾åœ¨ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ¢ãƒ‡ãƒ«
# base_model_name = "elyza/ELYZA-japanese-Llama-2-7b-instruct"  # 7Bæ¨™æº–
# base_model_name = "elyza/ELYZA-japanese-Llama-2-13b-instruct"  # 13Bé«˜æ€§èƒ½
```

---

## ğŸš€ é«˜åº¦ãªæ©Ÿèƒ½

### ğŸ”„ ç¶™ç¶šå­¦ç¿’
æ—¢å­˜ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã«æ–°ã—ã„çŸ¥è­˜ã‚’è¿½åŠ ï¼š
```bash
python3 continual_learning_finetune.py \
  --task-name "æ–°ã—ã„ãƒ‰ãƒ¡ã‚¤ãƒ³" \
  --data-path "new_domain.jsonl" \
  --description "å°‚é–€åˆ†é‡ã®çŸ¥è­˜è¿½åŠ "
```

### ğŸ“Š æ€§èƒ½è©•ä¾¡
```bash
# å…¨ã‚¿ã‚¹ã‚¯ã®æ€§èƒ½è©•ä¾¡
python3 continual_learning_finetune.py --evaluate

# è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
python3 continual_learning_finetune.py --report
```

### ğŸ”§ MS-Swift + Megatronçµ±åˆ
å¤§è¦æ¨¡åˆ†æ•£å­¦ç¿’ï¼š
```bash
python3 megatron_swift_finetune.py \
  --model "Qwen/Qwen3-8B-Base" \
  --dataset "kunishou/ApolloCorpus-ja"
```

---

## ğŸ“Š æ€§èƒ½ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯

### ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¯”è¼ƒ

| ãƒ¢ãƒ¼ãƒ‰ | å“è³ª | é€Ÿåº¦ | ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ | è¤‡é›‘ã• |
|--------|------|------|-----------|--------|
| `auto_optimized` | â­â­â­ | â­â­â­â­â­ | â­â­â­â­ | â­ |
| `full_spec` | â­â­â­â­â­ | â­â­â­ | â­â­â­ | â­â­â­ |
| `continual_learning` | â­â­â­â­â­ | â­â­ | â­â­ | â­â­ |

### RAGæ€§èƒ½æ¯”è¼ƒ

| ãƒ¢ãƒ¼ãƒ‰ | ç²¾åº¦ | é€Ÿåº¦ | ãƒªã‚½ãƒ¼ã‚¹ | UIå“è³ª |
|--------|------|------|----------|--------|
| `RAGapp.py` | â­â­â­â­â­ | â­â­â­ | â­â­â­ | â­â­â­â­â­ |
| `simple_rag` | â­â­â­ | â­â­â­â­â­ | â­â­â­â­â­ | â­â­ |

---

## ğŸ› ï¸ ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º

### ãƒ‡ãƒ¼ã‚¿å½¢å¼
```jsonl
{"instruction": "è³ªå•ã™ã‚‹", "input": "ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›", "output": "æœŸå¾…ã™ã‚‹å›ç­”"}
{"instruction": "", "input": "ç°¡å˜ãªè³ªå•", "output": "å›ç­”"}
```

### è¨­å®šä¾‹
```python
# config.py ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºä¾‹
class Config:
    # ãƒ¢ãƒ‡ãƒ«è¨­å®š
    base_model_name = "your-model-path"
    lora_adapter_path = "./your-tuned-model"
    
    # RAGè¨­å®š
    chunk_size = 500
    chunk_overlap = 50
    rag_variant_k = [5, 7, 10] # æ¤œç´¢ã™ã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°
    
    # UIè¨­å®š
    app_title = "ã‚«ã‚¹ã‚¿ãƒ RAGã‚·ã‚¹ãƒ†ãƒ "
```

---

## ğŸ” ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### ã‚ˆãã‚ã‚‹å•é¡Œ

#### 1. GPU VRAMä¸è¶³
```bash
# ã‚ˆã‚Šè»½é‡ãªè¨­å®šã‚’ä½¿ç”¨
python3 cpu_optimized_finetune.py
```

#### 2. ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼
```python
# config.py ã®ãƒ‘ã‚¹ã‚’ç¢ºèª
lora_adapter_path = "./actual_output_directory"
```

#### 3. æ€§èƒ½ãŒä½ã„å ´åˆ
```bash
# ã‚ˆã‚Šè‰¯ã„ãƒ‡ãƒ¼ã‚¿ã§ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°
python3 full_spec_finetune.py

# ç¶™ç¶šå­¦ç¿’ã§æ”¹å–„
python3 continual_learning_finetune.py --task-name "æ”¹å–„" --data-path "quality_data.jsonl"
```

---

## ğŸ“š è©³ç´°ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ

- [ğŸ“– USAGE_GUIDE.md](USAGE_GUIDE.md) - è©³ç´°ãªä½¿ç”¨æ–¹æ³•
- [ğŸ”§ è¨­å®šãƒªãƒ•ã‚¡ãƒ¬ãƒ³ã‚¹](docs/config_reference.md)
- [ğŸ§ª APIä»•æ§˜](docs/api_reference.md)
- [ğŸš€ ãƒ‡ãƒ—ãƒ­ã‚¤ã‚¬ã‚¤ãƒ‰](docs/deployment.md)

---

## ğŸ¤ ã‚³ãƒ³ãƒˆãƒªãƒ“ãƒ¥ãƒ¼ã‚·ãƒ§ãƒ³

ãƒ—ãƒ«ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒ»Issueå ±å‘Šæ­“è¿ï¼

1. Fork the repository
2. Create feature branch (`git checkout -b feature/amazing-feature`)
3. Commit changes (`git commit -m 'Add amazing feature'`)
4. Push to branch (`git push origin feature/amazing-feature`)
5. Open Pull Request

---

## ğŸ“„ ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

MIT License - è©³ç´°ã¯[LICENSE](LICENSE)ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‚ç…§

---

## ğŸ™ è¬è¾

- [Hugging Face Transformers](https://huggingface.co/transformers/)
- [ChromaDB](https://www.trychroma.com/)
- [MS-Swift](https://github.com/modelscope/ms-swift)
- [PEFT](https://github.com/huggingface/peft)

---

## ğŸ“ ã‚µãƒãƒ¼ãƒˆ

- ğŸ› ãƒã‚°å ±å‘Š: [Issues](https://github.com/tarochan0803/finetune-RAG/issues)
- ğŸ’¬ è³ªå•ãƒ»è­°è«–: [Discussions](https://github.com/tarochan0803/finetune-RAG/discussions)
- ğŸ“§ Email: taro0803tennis@gmail.com

---

<div align="center">

**â­ ã“ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãŒå½¹ç«‹ã£ãŸã‚‰ã‚¹ã‚¿ãƒ¼ã‚’ãŠé¡˜ã„ã—ã¾ã™ï¼**

Made with â¤ï¸ by [tarochan0803]

</div>
