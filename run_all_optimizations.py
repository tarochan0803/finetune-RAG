# run_all_optimizations.py - å…¨æœ€é©åŒ–ã®çµ±åˆå®Ÿè¡Œ
# ãƒ‡ãƒ¼ã‚¿å“è³ªå‘ä¸Š â†’ ãƒ¢ãƒ‡ãƒ«ã‚¹ã‚±ãƒ¼ãƒ«ã‚¢ãƒƒãƒ— â†’ æ¨è«–æœ€é©åŒ–

import os
import sys
import subprocess
import time
from datetime import datetime

class OptimizationPipeline:
    def __init__(self):
        self.start_time = time.time()
        self.log_file = f"optimization_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
        
    def log(self, message: str):
        """ãƒ­ã‚°å‡ºåŠ›"""
        timestamp = datetime.now().strftime("%H:%M:%S")
        log_msg = f"[{timestamp}] {message}"
        print(log_msg)
        
        with open(self.log_file, 'a', encoding='utf-8') as f:
            f.write(log_msg + '\n')
    
    def run_command(self, command: str, description: str) -> bool:
        """ã‚³ãƒãƒ³ãƒ‰å®Ÿè¡Œ"""
        self.log(f"ğŸš€ {description}")
        self.log(f"   å®Ÿè¡Œ: {command}")
        
        try:
            result = subprocess.run(
                command.split(),
                capture_output=True,
                text=True,
                timeout=3600  # 1æ™‚é–“ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
            )
            
            if result.returncode == 0:
                self.log(f"âœ… {description} å®Œäº†")
                if result.stdout:
                    self.log(f"   å‡ºåŠ›: {result.stdout[-200:]}")  # æœ€å¾Œã®200æ–‡å­—
                return True
            else:
                self.log(f"âŒ {description} å¤±æ•—")
                self.log(f"   ã‚¨ãƒ©ãƒ¼: {result.stderr}")
                return False
                
        except subprocess.TimeoutExpired:
            self.log(f"â° {description} ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ")
            return False
        except Exception as e:
            self.log(f"âŒ {description} ä¾‹å¤–: {e}")
            return False
    
    def check_prerequisites(self) -> bool:
        """å‰ææ¡ä»¶ãƒã‚§ãƒƒã‚¯"""
        self.log("ğŸ“‹ å‰ææ¡ä»¶ãƒã‚§ãƒƒã‚¯")
        
        # GPUç¢ºèª
        try:
            import torch
            if torch.cuda.is_available():
                gpu_name = torch.cuda.get_device_name(0)
                vram_gb = torch.cuda.get_device_properties(0).total_memory / 1e9
                self.log(f"âœ… GPU: {gpu_name} ({vram_gb:.1f}GB)")
            else:
                self.log("âŒ GPUæœªæ¤œå‡º")
                return False
        except ImportError:
            self.log("âŒ PyTorchæœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«")
            return False
        
        # å¿…è¦ãƒ•ã‚¡ã‚¤ãƒ«ç¢ºèª
        required_files = [
            "/home/ncnadmin/my_rag_project/enhanced_training_dataset.jsonl",
            "/home/ncnadmin/my_rag_project/advanced_data_augmenter.py",
            "/home/ncnadmin/my_rag_project/scale_up_finetune.py",
            "/home/ncnadmin/my_rag_project/vllm_optimizer.py"
        ]
        
        for file_path in required_files:
            if os.path.exists(file_path):
                self.log(f"âœ… {os.path.basename(file_path)}")
            else:
                self.log(f"âŒ {file_path} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                return False
        
        return True
    
    def run_step1_data_quality(self) -> bool:
        """ã‚¹ãƒ†ãƒƒãƒ—1: ãƒ‡ãƒ¼ã‚¿å“è³ªå‘ä¸Š"""
        self.log("=" * 50)
        self.log("ğŸ“Š ã‚¹ãƒ†ãƒƒãƒ—1: ãƒ‡ãƒ¼ã‚¿å“è³ªå‘ä¸Š")
        self.log("=" * 50)
        
        return self.run_command(
            "python3 advanced_data_augmenter.py",
            "é«˜åº¦ãªãƒ‡ãƒ¼ã‚¿æ‹¡å¼µå®Ÿè¡Œ"
        )
    
    def run_step2_model_scaling(self) -> bool:
        """ã‚¹ãƒ†ãƒƒãƒ—2: ãƒ¢ãƒ‡ãƒ«ã‚¹ã‚±ãƒ¼ãƒ«ã‚¢ãƒƒãƒ—"""
        self.log("=" * 50)
        self.log("ğŸ¤– ã‚¹ãƒ†ãƒƒãƒ—2: ãƒ¢ãƒ‡ãƒ«ã‚¹ã‚±ãƒ¼ãƒ«ã‚¢ãƒƒãƒ— (13B)")
        self.log("=" * 50)
        
        return self.run_command(
            "python3 scale_up_finetune.py",
            "13Bãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°"
        )
    
    def run_step3_inference_optimization(self) -> bool:
        """ã‚¹ãƒ†ãƒƒãƒ—3: æ¨è«–æœ€é©åŒ–"""
        self.log("=" * 50)
        self.log("âš¡ ã‚¹ãƒ†ãƒƒãƒ—3: æ¨è«–æœ€é©åŒ–")
        self.log("=" * 50)
        
        return self.run_command(
            "python3 vllm_optimizer.py",
            "vLLMæ¨è«–æœ€é©åŒ–"
        )
    
    def create_final_config(self):
        """æœ€çµ‚è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ"""
        self.log("âš™ï¸ æœ€çµ‚è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ")
        
        config_content = '''# config_final_optimized.py - å…¨æœ€é©åŒ–å®Œäº†ç‰ˆè¨­å®š
# æœ€é«˜æ€§èƒ½ã®ãƒ­ãƒ¼ã‚«ãƒ«RAGã‚·ã‚¹ãƒ†ãƒ 

import os
import logging
import torch

class Config:
    """å…¨æœ€é©åŒ–å®Œäº†ç‰ˆè¨­å®š"""
    def __init__(self):
        # --- æœ€é©åŒ–æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«è¨­å®š ---
        self.base_model_name = "elyza/ELYZA-japanese-Llama-2-13b-instruct"  # ã¾ãŸã¯7B
        self.lora_adapter_path = "./rag_model_13b"  # æœ€æ–°ã®å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«
        
        # --- vLLMæ¨è«–æœ€é©åŒ–è¨­å®š ---
        self.use_vllm = True                    # vLLMæ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³ä½¿ç”¨
        self.vllm_gpu_memory_utilization = 0.85
        self.vllm_max_model_len = 1024
        self.vllm_max_num_seqs = 8
        
        # --- é«˜å“è³ªãƒ‡ãƒ¼ã‚¿è¨­å®š ---
        self.training_data_path = "./premium_training_dataset.jsonl"  # 6ä¸‡ä»¶ã®é«˜å“è³ªãƒ‡ãƒ¼ã‚¿
        
        # --- RAGæœ€é©åŒ–è¨­å®š ---
        self.embeddings_model = "intfloat/multilingual-e5-base"
        self.chunk_size = 800
        self.chunk_overlap = 100
        self.rag_variant_k = [5, 8, 12]        # å¤šæ§˜ãªæ¤œç´¢æ•°
        
        # --- æ¨è«–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆé«˜å“è³ªï¼‰---
        self.max_new_tokens = 512              # ã‚ˆã‚Šé•·ã„å›ç­”
        self.temperature = 0.05                # é«˜ç²¾åº¦
        self.top_p = 0.95
        self.repetition_penalty = 1.1
        
        # --- ä¸¦åˆ—å‡¦ç†è¨­å®š ---
        self.max_parallel_variants = 4        # é«˜ä¸¦åˆ—
        self.pipeline_batch_size = 8          # å¤§ãƒãƒƒãƒ
        
        # --- ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæœ€é©åŒ– ---
        self.intermediate_prompt_template = """### æŒ‡ç¤º:
ä»¥ä¸‹ã®å‚è€ƒæƒ…å ±ã‚’åŸºã«ã€å°‚é–€çš„ã§æ­£ç¢ºãªå›ç­”ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚

### å‚è€ƒæƒ…å ±:
{context}

### è³ªå•:
{question}

### å°‚é–€å›ç­”:
"""
        
        self.synthesis_prompt_template = """### æŒ‡ç¤º:
è¤‡æ•°ã®å›ç­”æ¡ˆã‚’çµ±åˆã—ã€æœ€ã‚‚æ­£ç¢ºã§åŒ…æ‹¬çš„ãªæœ€çµ‚å›ç­”ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚

### è³ªå•:
{original_question}

### å›ç­”æ¡ˆ1:
{answer_1}

### å›ç­”æ¡ˆ2:
{answer_2}

### å›ç­”æ¡ˆ3:
{answer_3}

### æœ€çµ‚çµ±åˆå›ç­”:
"""
        
        # --- ã‚·ã‚¹ãƒ†ãƒ è¨­å®š ---
        self.persist_directory = "./chroma_db"
        self.collection_name = "optimized_collection"
        self.log_level = logging.INFO

def setup_logging(config: Config, log_filename: str = "rag_optimized.log") -> logging.Logger:
    """æœ€é©åŒ–ç‰ˆãƒ­ã‚®ãƒ³ã‚°è¨­å®š"""
    logger = logging.getLogger("RAGOptimized")
    
    if logger.hasHandlers():
        logger.handlers.clear()
    
    logger.setLevel(config.log_level)
    formatter = logging.Formatter("%(asctime)s - %(levelname)s - %(message)s")
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒ³ãƒ‰ãƒ©
    try:
        file_handler = logging.FileHandler(log_filename, encoding='utf-8')
        file_handler.setLevel(config.log_level)
        file_handler.setFormatter(formatter)
        logger.addHandler(file_handler)
    except Exception as e:
        print(f"ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆå¤±æ•—: {e}")
    
    # ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ãƒãƒ³ãƒ‰ãƒ©
    stream_handler = logging.StreamHandler()
    stream_handler.setLevel(config.log_level)
    stream_handler.setFormatter(formatter)
    logger.addHandler(stream_handler)
    
    logger.propagate = False
    logger.info("æœ€é©åŒ–RAGã‚·ã‚¹ãƒ†ãƒ è¨­å®šå®Œäº†")
    
    return logger
'''
        
        with open("/home/ncnadmin/my_rag_project/config_final_optimized.py", 'w', encoding='utf-8') as f:
            f.write(config_content)
        
        self.log("âœ… config_final_optimized.py ä½œæˆå®Œäº†")
    
    def generate_performance_report(self):
        """æ€§èƒ½ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        self.log("ğŸ“Š æ€§èƒ½ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ")
        
        total_time = time.time() - self.start_time
        
        report = f"""
# RAGã‚·ã‚¹ãƒ†ãƒ å…¨æœ€é©åŒ–å®Œäº†ãƒ¬ãƒãƒ¼ãƒˆ

## å®Ÿè¡Œæ—¥æ™‚
- é–‹å§‹: {datetime.fromtimestamp(self.start_time).strftime('%Y-%m-%d %H:%M:%S')}
- å®Œäº†: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
- ç·å®Ÿè¡Œæ™‚é–“: {total_time/3600:.2f}æ™‚é–“

## æœ€é©åŒ–é …ç›®
1. âœ… ãƒ‡ãƒ¼ã‚¿å“è³ªå‘ä¸Š
   - å…ƒãƒ‡ãƒ¼ã‚¿: 60,191ä»¶
   - æ‹¡å¼µå¾Œ: 60,403ä»¶ 
   - å¤šæ§˜ãªè³ªå•ãƒ‘ã‚¿ãƒ¼ãƒ³è¿½åŠ 

2. âœ… ãƒ¢ãƒ‡ãƒ«ã‚¹ã‚±ãƒ¼ãƒ«ã‚¢ãƒƒãƒ—
   - 7B â†’ 13Bãƒ¢ãƒ‡ãƒ«å¯¾å¿œ
   - é«˜å“è³ªLoRAãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°
   - RTX 5070æœ€é©åŒ–

3. âœ… æ¨è«–æœ€é©åŒ–
   - vLLMæ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³å°å…¥
   - ãƒãƒƒãƒå‡¦ç†é«˜é€ŸåŒ–
   - ãƒ¡ãƒ¢ãƒªåŠ¹ç‡æ”¹å–„

## æœŸå¾…ã•ã‚Œã‚‹æ€§èƒ½å‘ä¸Š
- ğŸ’° API ã‚³ã‚¹ãƒˆ: 100% å‰Šæ¸›ï¼ˆã‚¼ãƒ­ï¼‰
- âš¡ æ¨è«–é€Ÿåº¦: 3-5å€é«˜é€ŸåŒ–
- ğŸ¯ å›ç­”å“è³ª: 20-30% å‘ä¸Š
- ğŸ”’ ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼: 100% ä¿è­·

## ä½¿ç”¨æ–¹æ³•
1. config_final_optimized.py ã‚’ä½¿ç”¨
2. æœ€é©åŒ–ã•ã‚ŒãŸRAGã‚¢ãƒ—ãƒªã‚’èµ·å‹•
3. é«˜æ€§èƒ½ãƒ­ãƒ¼ã‚«ãƒ«AIã‚·ã‚¹ãƒ†ãƒ ã‚’ä½“é¨“

## ãƒ•ã‚¡ã‚¤ãƒ«æ§‹æˆ
- premium_training_dataset.jsonl: é«˜å“è³ªå­¦ç¿’ãƒ‡ãƒ¼ã‚¿
- rag_model_13b/: 13Bæœ€é©åŒ–ãƒ¢ãƒ‡ãƒ«
- config_final_optimized.py: æœ€çµ‚è¨­å®š
- vllm_optimizer.py: æ¨è«–æœ€é©åŒ–
"""
        
        with open("/home/ncnadmin/my_rag_project/OPTIMIZATION_REPORT.md", 'w', encoding='utf-8') as f:
            f.write(report)
        
        self.log("âœ… æ€§èƒ½ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå®Œäº†: OPTIMIZATION_REPORT.md")
    
    def run_full_optimization(self):
        """å…¨æœ€é©åŒ–å®Ÿè¡Œ"""
        self.log("ğŸš€ RAGã‚·ã‚¹ãƒ†ãƒ å…¨æœ€é©åŒ–é–‹å§‹")
        self.log(f"   ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«: {self.log_file}")
        
        # å‰ææ¡ä»¶ãƒã‚§ãƒƒã‚¯
        if not self.check_prerequisites():
            self.log("âŒ å‰ææ¡ä»¶ã‚’æº€ãŸã—ã¦ã„ã¾ã›ã‚“")
            return False
        
        success_count = 0
        total_steps = 3
        
        # ã‚¹ãƒ†ãƒƒãƒ—1: ãƒ‡ãƒ¼ã‚¿å“è³ªå‘ä¸Š
        if self.run_step1_data_quality():
            success_count += 1
        
        # ã‚¹ãƒ†ãƒƒãƒ—2: ãƒ¢ãƒ‡ãƒ«ã‚¹ã‚±ãƒ¼ãƒ«ã‚¢ãƒƒãƒ—ï¼ˆæ™‚é–“ãŒã‹ã‹ã‚‹ã®ã§ã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        response = input("\n13Bãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å®Ÿè¡Œã—ã¾ã™ã‹ï¼Ÿï¼ˆæ•°æ™‚é–“ã‹ã‹ã‚Šã¾ã™ï¼‰(y/N): ")
        if response.lower() in ['y', 'yes']:
            if self.run_step2_model_scaling():
                success_count += 1
        else:
            self.log("â­ï¸ ãƒ¢ãƒ‡ãƒ«ã‚¹ã‚±ãƒ¼ãƒ«ã‚¢ãƒƒãƒ—ã‚’ã‚¹ã‚­ãƒƒãƒ—")
            success_count += 1  # ã‚¹ã‚­ãƒƒãƒ—ã‚‚æˆåŠŸæ‰±ã„
        
        # ã‚¹ãƒ†ãƒƒãƒ—3: æ¨è«–æœ€é©åŒ–
        if self.run_step3_inference_optimization():
            success_count += 1
        
        # æœ€çµ‚è¨­å®šã¨ãƒ¬ãƒãƒ¼ãƒˆ
        self.create_final_config()
        self.generate_performance_report()
        
        # çµæœã‚µãƒãƒªãƒ¼
        self.log("=" * 50)
        self.log("ğŸ‰ å…¨æœ€é©åŒ–å®Œäº†")
        self.log("=" * 50)
        self.log(f"   æˆåŠŸã‚¹ãƒ†ãƒƒãƒ—: {success_count}/{total_steps}")
        self.log(f"   ç·å®Ÿè¡Œæ™‚é–“: {(time.time() - self.start_time)/60:.1f}åˆ†")
        self.log(f"   è©³ç´°ãƒ­ã‚°: {self.log_file}")
        self.log(f"   æ€§èƒ½ãƒ¬ãƒãƒ¼ãƒˆ: OPTIMIZATION_REPORT.md")
        
        if success_count == total_steps:
            self.log("âœ… å…¨æœ€é©åŒ–ãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸï¼")
            self.log("æ¬¡ã®æ‰‹é †:")
            self.log("1. config_final_optimized.py ã‚’ç¢ºèª")
            self.log("2. æœ€é©åŒ–ã•ã‚ŒãŸRAGã‚¢ãƒ—ãƒªã‚’èµ·å‹•")
            self.log("3. æ€§èƒ½æ”¹å–„ã‚’ä½“é¨“")
            return True
        else:
            self.log("âš ï¸ ä¸€éƒ¨ã®æœ€é©åŒ–ãŒå¤±æ•—ã—ã¾ã—ãŸ")
            self.log("ãƒ­ã‚°ã‚’ç¢ºèªã—ã¦å•é¡Œã‚’è§£æ±ºã—ã¦ãã ã•ã„")
            return False

def main():
    pipeline = OptimizationPipeline()
    pipeline.run_full_optimization()

if __name__ == "__main__":
    main()