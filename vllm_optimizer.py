# vllm_optimizer.py - vLLMã«ã‚ˆã‚‹æ¨è«–é«˜é€ŸåŒ–
# RTX 5070å‘ã‘æœ€é©åŒ–æ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³

import torch
import os
import time
import asyncio
from typing import List, Dict, Any, Optional, AsyncGenerator
from dataclasses import dataclass
import json

# vLLMã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ç¢ºèªä»˜ãï¼‰
try:
    from vllm import LLM, SamplingParams
    from vllm.engine.arg_utils import AsyncEngineArgs
    from vllm.engine.async_llm_engine import AsyncLLMEngine
    VLLM_AVAILABLE = True
    print("âœ… vLLMåˆ©ç”¨å¯èƒ½")
except ImportError:
    VLLM_AVAILABLE = False
    print("âŒ vLLMæœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« - æ¨™æº–æ¨è«–ã‚’ä½¿ç”¨")

@dataclass
class OptimizationConfig:
    """æ¨è«–æœ€é©åŒ–è¨­å®š"""
    model_path: str
    max_model_len: int = 1024
    gpu_memory_utilization: float = 0.8
    tensor_parallel_size: int = 1
    max_num_seqs: int = 16
    max_num_batched_tokens: int = 2048

class VLLMOptimizer:
    """vLLMæ¨è«–æœ€é©åŒ–ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, config: OptimizationConfig):
        self.config = config
        self.llm = None
        self.async_engine = None
        self.sampling_params = None
        
    def setup_vllm_engine(self):
        """vLLMã‚¨ãƒ³ã‚¸ãƒ³åˆæœŸåŒ–"""
        if not VLLM_AVAILABLE:
            print("âŒ vLLMãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
            return False
            
        try:
            print("ğŸ”§ vLLMã‚¨ãƒ³ã‚¸ãƒ³åˆæœŸåŒ–ä¸­...")
            
            # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
            self.sampling_params = SamplingParams(
                temperature=0.1,
                top_p=0.9,
                max_tokens=256,
                repetition_penalty=1.15,
                stop=["<|endoftext|>", "###"]
            )
            
            # åŒæœŸã‚¨ãƒ³ã‚¸ãƒ³
            self.llm = LLM(
                model=self.config.model_path,
                tensor_parallel_size=self.config.tensor_parallel_size,
                gpu_memory_utilization=self.config.gpu_memory_utilization,
                max_model_len=self.config.max_model_len,
                max_num_seqs=self.config.max_num_seqs,
                max_num_batched_tokens=self.config.max_num_batched_tokens,
                trust_remote_code=True,
                enforce_eager=True,  # RTX 5070å¯¾å¿œ
            )
            
            print("âœ… vLLMã‚¨ãƒ³ã‚¸ãƒ³åˆæœŸåŒ–å®Œäº†")
            return True
            
        except Exception as e:
            print(f"âŒ vLLMã‚¨ãƒ³ã‚¸ãƒ³åˆæœŸåŒ–å¤±æ•—: {e}")
            return False
    
    def setup_async_engine(self):
        """éåŒæœŸvLLMã‚¨ãƒ³ã‚¸ãƒ³åˆæœŸåŒ–"""
        try:
            print("ğŸ”§ éåŒæœŸvLLMã‚¨ãƒ³ã‚¸ãƒ³åˆæœŸåŒ–ä¸­...")
            
            engine_args = AsyncEngineArgs(
                model=self.config.model_path,
                tensor_parallel_size=self.config.tensor_parallel_size,
                gpu_memory_utilization=self.config.gpu_memory_utilization,
                max_model_len=self.config.max_model_len,
                max_num_seqs=self.config.max_num_seqs,
                trust_remote_code=True,
                enforce_eager=True,
            )
            
            self.async_engine = AsyncLLMEngine.from_engine_args(engine_args)
            print("âœ… éåŒæœŸvLLMã‚¨ãƒ³ã‚¸ãƒ³åˆæœŸåŒ–å®Œäº†")
            return True
            
        except Exception as e:
            print(f"âŒ éåŒæœŸvLLMã‚¨ãƒ³ã‚¸ãƒ³åˆæœŸåŒ–å¤±æ•—: {e}")
            return False
    
    def generate_batch(self, prompts: List[str]) -> List[str]:
        """ãƒãƒƒãƒæ¨è«–ï¼ˆåŒæœŸï¼‰"""
        if not self.llm:
            print("âŒ vLLMã‚¨ãƒ³ã‚¸ãƒ³ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“")
            return []
        
        try:
            start_time = time.time()
            outputs = self.llm.generate(prompts, self.sampling_params)
            end_time = time.time()
            
            results = []
            for output in outputs:
                generated_text = output.outputs[0].text.strip()
                results.append(generated_text)
            
            throughput = len(prompts) / (end_time - start_time)
            print(f"âš¡ ãƒãƒƒãƒæ¨è«–å®Œäº†: {len(prompts)}ä»¶, {throughput:.2f}ä»¶/ç§’")
            
            return results
            
        except Exception as e:
            print(f"âŒ ãƒãƒƒãƒæ¨è«–ã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    async def generate_async(self, prompt: str) -> AsyncGenerator[str, None]:
        """éåŒæœŸã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°æ¨è«–"""
        if not self.async_engine:
            print("âŒ éåŒæœŸvLLMã‚¨ãƒ³ã‚¸ãƒ³ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“")
            return
        
        try:
            request_id = f"req_{time.time()}"
            
            # æ¨è«–é–‹å§‹
            results_generator = self.async_engine.generate(
                prompt, 
                self.sampling_params, 
                request_id
            )
            
            # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‡ºåŠ›
            async for request_output in results_generator:
                if request_output.outputs:
                    new_text = request_output.outputs[0].text
                    yield new_text
                    
        except Exception as e:
            print(f"âŒ éåŒæœŸæ¨è«–ã‚¨ãƒ©ãƒ¼: {e}")
            yield f"ã‚¨ãƒ©ãƒ¼: {e}"

class OptimizedRAGInference:
    """æœ€é©åŒ–RAGæ¨è«–ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, model_path: str):
        self.model_path = model_path
        self.optimizer = None
        self.fallback_model = None
        
    def initialize(self):
        """æ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³åˆæœŸåŒ–"""
        print("ğŸš€ æœ€é©åŒ–RAGæ¨è«–ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–")
        
        # GPU VRAMç¢ºèª
        if torch.cuda.is_available():
            vram_gb = torch.cuda.get_device_properties(0).total_memory / 1e9
            print(f"   GPU VRAM: {vram_gb:.1f}GB")
            
            # VRAMã«å¿œã˜ãŸè¨­å®šèª¿æ•´
            if vram_gb >= 12:
                config = OptimizationConfig(
                    model_path=self.model_path,
                    max_model_len=1024,
                    gpu_memory_utilization=0.85,
                    max_num_seqs=8,
                    max_num_batched_tokens=1024
                )
            else:
                config = OptimizationConfig(
                    model_path=self.model_path,
                    max_model_len=512,
                    gpu_memory_utilization=0.7,
                    max_num_seqs=4,
                    max_num_batched_tokens=512
                )
        else:
            print("âŒ GPUæœªæ¤œå‡º - CPUæ¨è«–ã¯éå¯¾å¿œ")
            return False
        
        # vLLMåˆæœŸåŒ–è©¦è¡Œ
        self.optimizer = VLLMOptimizer(config)
        
        if self.optimizer.setup_vllm_engine():
            print("âœ… vLLMé«˜é€Ÿæ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³æº–å‚™å®Œäº†")
            return True
        else:
            print("âš ï¸ vLLMåˆæœŸåŒ–å¤±æ•— - ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ¨è«–ã‚’ä½¿ç”¨")
            return self.setup_fallback()
    
    def setup_fallback(self):
        """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ¨è«–è¨­å®š"""
        try:
            from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
            from peft import PeftModel
            
            print("ğŸ”„ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ¨è«–è¨­å®šä¸­...")
            
            # ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
            base_model = AutoModelForCausalLM.from_pretrained(
                "elyza/ELYZA-japanese-Llama-2-7b-instruct",
                torch_dtype=torch.bfloat16,
                device_map="auto",
                trust_remote_code=True
            )
            
            # LoRAã‚¢ãƒ€ãƒ—ã‚¿é©ç”¨
            if os.path.exists(self.model_path):
                model = PeftModel.from_pretrained(base_model, self.model_path)
            else:
                model = base_model
            
            tokenizer = AutoTokenizer.from_pretrained(self.model_path)
            
            self.fallback_model = pipeline(
                "text-generation",
                model=model,
                tokenizer=tokenizer,
                torch_dtype=torch.bfloat16,
                device_map="auto"
            )
            
            print("âœ… ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ¨è«–æº–å‚™å®Œäº†")
            return True
            
        except Exception as e:
            print(f"âŒ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ¨è«–è¨­å®šå¤±æ•—: {e}")
            return False
    
    def generate_optimized(self, prompts: List[str]) -> List[str]:
        """æœ€é©åŒ–æ¨è«–å®Ÿè¡Œ"""
        if self.optimizer and self.optimizer.llm:
            return self.optimizer.generate_batch(prompts)
        elif self.fallback_model:
            results = []
            for prompt in prompts:
                output = self.fallback_model(
                    prompt,
                    max_new_tokens=256,
                    temperature=0.1,
                    do_sample=False
                )
                results.append(output[0]['generated_text'][len(prompt):].strip())
            return results
        else:
            print("âŒ æ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
            return []
    
    def benchmark_performance(self):
        """æ€§èƒ½ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯"""
        print("ğŸ“Š æ€§èƒ½ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ")
        
        test_prompts = [
            "### å…¥åŠ›:\næ ªå¼ä¼šç¤¾ä¸‰å»ºã®å£ä»•æ§˜ã«ã¤ã„ã¦\n### å¿œç­”:\n",
            "### å…¥åŠ›:\nä»®ç­‹äº¤ã®æ¨™æº–ä»•æ§˜\n### å¿œç­”:\n",
            "### å…¥åŠ›:\né‹¼è£½æŸã®ä½¿ç”¨åŸºæº–\n### å¿œç­”:\n"
        ] * 5  # 15ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        
        # ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—
        _ = self.generate_optimized(test_prompts[:1])
        
        # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ
        start_time = time.time()
        results = self.generate_optimized(test_prompts)
        end_time = time.time()
        
        total_time = end_time - start_time
        throughput = len(test_prompts) / total_time
        avg_latency = total_time / len(test_prompts)
        
        print(f"ğŸ“ˆ æ€§èƒ½çµæœ:")
        print(f"   å‡¦ç†ä»¶æ•°: {len(test_prompts)}")
        print(f"   ç·å‡¦ç†æ™‚é–“: {total_time:.2f}ç§’")
        print(f"   ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: {throughput:.2f}ä»¶/ç§’")
        print(f"   å¹³å‡ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·: {avg_latency:.3f}ç§’")
        
        return {
            "throughput": throughput,
            "latency": avg_latency,
            "total_time": total_time
        }

def install_vllm():
    """vLLMã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«"""
    print("ğŸ“¦ vLLMã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ä¸­...")
    import subprocess
    import sys
    
    try:
        subprocess.check_call([
            sys.executable, "-m", "pip", "install", 
            "vllm", "--extra-index-url", "https://download.pytorch.org/whl/cu121"
        ])
        print("âœ… vLLMã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å®Œäº†")
        return True
    except subprocess.CalledProcessError as e:
        print(f"âŒ vLLMã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å¤±æ•—: {e}")
        return False

def main():
    print("ğŸ”¥ æ¨è«–æœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ ")
    
    # vLLMã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ç¢ºèª
    if not VLLM_AVAILABLE:
        response = input("vLLMã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¾ã™ã‹ï¼Ÿ (y/N): ")
        if response.lower() in ['y', 'yes']:
            if install_vllm():
                print("ğŸ”„ ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’å†å®Ÿè¡Œã—ã¦ãã ã•ã„")
                return
    
    # ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹ç¢ºèª
    model_candidates = [
        "./rag_model_13b",
        "./rag_finetuned_model", 
        "./rag_model_light",
        "./optimized_rag_model"
    ]
    
    model_path = None
    for path in model_candidates:
        if os.path.exists(path):
            model_path = path
            break
    
    if not model_path:
        print("âŒ ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        print("å…ˆã«ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„")
        return
    
    print(f"ğŸ¤– ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: {model_path}")
    
    # æ¨è«–ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
    inference_system = OptimizedRAGInference(model_path)
    
    if inference_system.initialize():
        # æ€§èƒ½ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
        benchmark_results = inference_system.benchmark_performance()
        
        print("\nğŸ‰ æ¨è«–æœ€é©åŒ–å®Œäº†ï¼")
        print("æ¬¡ã®æ‰‹é †:")
        print("1. RAGã‚¢ãƒ—ãƒªã§ã“ã®æœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ ã‚’ä½¿ç”¨")
        print("2. æœ¬ç•ªç’°å¢ƒã§ã®æ€§èƒ½æ¸¬å®š")
        print("3. å¿…è¦ã«å¿œã˜ã¦ã•ã‚‰ãªã‚‹æœ€é©åŒ–")
    else:
        print("âŒ æ¨è«–æœ€é©åŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ")

if __name__ == "__main__":
    main()