# quick_finetune.py - RTX 5070æœ€é©åŒ–ç‰ˆ
# 12GB VRAMå‘ã‘è»½é‡ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°

import torch
import os
from datasets import load_dataset
from transformers import (
    AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments,
    DataCollatorForSeq2Seq
)
from peft import LoraConfig, get_peft_model, TaskType
from transformers import BitsAndBytesConfig

# RTX 5070æœ€é©åŒ–è¨­å®š
MODEL_NAME = "elyza/ELYZA-japanese-Llama-2-7b-instruct"
JSONL_PATH = "/home/ncnadmin/my_rag_project/enhanced_training_dataset.jsonl"
OUTPUT_DIR = "./rag_finetuned_model"

print("ğŸš€ RTX 5070å‘ã‘è»½é‡ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°é–‹å§‹")

# ãƒ‡ãƒ¼ã‚¿æº–å‚™
def preprocess_function(examples):
    instruction = examples.get("instruction", "").strip()
    input_text = examples.get("input", "").strip()
    output_text = examples.get("output", "").strip()
    
    if instruction:
        prompt = f"### æŒ‡ç¤º:\n{instruction}\n\n### å…¥åŠ›:\n{input_text}\n\n### å¿œç­”:\n"
    else:
        prompt = f"### å…¥åŠ›:\n{input_text}\n\n### å¿œç­”:\n"
    
    return {"text": prompt + output_text + "<|endoftext|>"}

print("ğŸ“Š ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­...")
raw_dataset = load_dataset("json", data_files={"train": JSONL_PATH}, split="train")
# ã‚µãƒ³ãƒ—ãƒ«æ•°ã‚’åˆ¶é™ï¼ˆé«˜é€ŸåŒ–ã®ãŸã‚ï¼‰
sample_size = min(5000, len(raw_dataset))  # 5000ã‚µãƒ³ãƒ—ãƒ«ã«åˆ¶é™
raw_dataset = raw_dataset.select(range(sample_size))

dataset = raw_dataset.map(preprocess_function, remove_columns=raw_dataset.column_names)
dataset = dataset.train_test_split(test_size=0.1, seed=42)

print(f"å­¦ç¿’ãƒ‡ãƒ¼ã‚¿: {len(dataset['train'])}ä»¶, è©•ä¾¡ãƒ‡ãƒ¼ã‚¿: {len(dataset['test'])}ä»¶")

# ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼
print("ğŸ”¤ ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼æº–å‚™ä¸­...")
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

def tokenize_function(examples):
    return tokenizer(
        examples["text"],
        truncation=True,
        padding=False,
        max_length=512,  # çŸ­ã‚ã«è¨­å®šï¼ˆãƒ¡ãƒ¢ãƒªç¯€ç´„ï¼‰
    )

tokenized_dataset = dataset.map(
    tokenize_function,
    batched=True,
    remove_columns=["text"]
)

# ãƒ¢ãƒ‡ãƒ«è¨­å®šï¼ˆè»½é‡åŒ–ï¼‰
print("ğŸ¤– ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ä¸­...")
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16,  # float16ã§è»½é‡åŒ–
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
)

model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    quantization_config=bnb_config,
    device_map="auto",
    trust_remote_code=True,
)

# è»½é‡LoRAè¨­å®š
lora_config = LoraConfig(
    task_type=TaskType.CAUSAL_LM,
    r=4,  # è»½é‡åŒ–ã®ãŸã‚å°ã•ã
    lora_alpha=8,
    lora_dropout=0.05,
    target_modules=["q_proj", "v_proj"],  # æœ€å°é™ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
)

model = get_peft_model(model, lora_config)
print(f"âœ… LoRAé©ç”¨å®Œäº†ï¼ˆå­¦ç¿’å¯èƒ½ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {model.num_parameters(only_trainable=True):,}ï¼‰")

# ãƒ‡ãƒ¼ã‚¿ã‚³ãƒ¬ã‚¯ã‚¿ãƒ¼
data_collator = DataCollatorForSeq2Seq(
    tokenizer=tokenizer,
    padding=True,
    return_tensors="pt",
)

# è»½é‡å­¦ç¿’è¨­å®š
training_args = TrainingArguments(
    output_dir=OUTPUT_DIR,
    num_train_epochs=2,  # ã‚¨ãƒãƒƒã‚¯æ•°å‰Šæ¸›
    per_device_train_batch_size=1,
    per_device_eval_batch_size=1,
    gradient_accumulation_steps=4,  # å®ŸåŠ¹ãƒãƒƒãƒã‚µã‚¤ã‚º=4
    learning_rate=5e-5,
    fp16=True,  # fp16ã§è»½é‡åŒ–
    optim="adamw_torch",  # æ¨™æº–optimizer
    dataloader_num_workers=0,  # ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°å‰Šæ¸›
    
    evaluation_strategy="steps",
    eval_steps=200,
    save_strategy="steps",
    save_steps=200,
    save_total_limit=1,  # ä¿å­˜æ•°åˆ¶é™
    load_best_model_at_end=True,
    
    logging_steps=50,
    report_to=[],  # ãƒ­ã‚°ç„¡åŠ¹åŒ–
    
    remove_unused_columns=False,
    dataloader_pin_memory=False,  # ãƒ¡ãƒ¢ãƒªç¯€ç´„
)

# ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ä½œæˆ
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset["train"],
    eval_dataset=tokenized_dataset["test"],
    data_collator=data_collator,
    tokenizer=tokenizer,
)

print("ğŸ¯ å­¦ç¿’é–‹å§‹...")
try:
    trainer.train()
    
    print("ğŸ’¾ ãƒ¢ãƒ‡ãƒ«ä¿å­˜ä¸­...")
    trainer.save_model()
    tokenizer.save_pretrained(OUTPUT_DIR)
    
    print("ğŸ‰ å­¦ç¿’å®Œäº†ï¼")
    print(f"ğŸ“ ä¿å­˜å…ˆ: {OUTPUT_DIR}")
    print("\næ¬¡ã®æ‰‹é †:")
    print("1. config.py ã® lora_adapter_path ã‚’ä»¥ä¸‹ã«å¤‰æ›´:")
    print(f"   self.lora_adapter_path = '{OUTPUT_DIR}'")
    print("2. RAGã‚¢ãƒ—ãƒªã‚’èµ·å‹•ã—ã¦æ€§èƒ½ç¢ºèª")
    
except Exception as e:
    print(f"âŒ ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {e}")
    print("ãƒ¡ãƒ¢ãƒªä¸è¶³ã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚„ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·ã‚’èª¿æ•´ã—ã¦ãã ã•ã„ã€‚")