# scale_up_finetune.py - 13Bãƒ¢ãƒ‡ãƒ«å¯¾å¿œå¤§è¦æ¨¡ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°
# RTX 5070ã§ã‚‚å‹•ä½œã™ã‚‹ã‚ˆã†æœ€é©åŒ–

import torch
import os
from datasets import load_dataset, load_from_disk
from transformers import (
    AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments,
    DataCollatorForSeq2Seq, EarlyStoppingCallback
)
from peft import LoraConfig, get_peft_model, TaskType
from transformers import BitsAndBytesConfig
import gc

class ScaledUpFineTuner:
    def __init__(self):
        # 13Bãƒ¢ãƒ‡ãƒ«è¨­å®š
        self.model_options = [
            "elyza/ELYZA-japanese-Llama-2-13b-instruct",  # ç¬¬1é¸æŠ
            "rinna/youri-7b-instruction",                  # ç¬¬2é¸æŠï¼ˆ7Bé«˜æ€§èƒ½ï¼‰
            "stabilityai/japanese-stablelm-instruct-alpha-7b-v2"  # ç¬¬3é¸æŠ
        ]
        
        self.jsonl_path = "/home/ncnadmin/my_rag_project/premium_training_dataset.jsonl"
        self.cache_dir = "cache/scaled_tokenized"
        self.output_dir = "./rag_model_13b"
        
        # RTX 5070æœ€é©åŒ–è¨­å®š
        self.batch_size = 1
        self.grad_accumulation = 16  # å®ŸåŠ¹ãƒãƒƒãƒã‚µã‚¤ã‚º=16
        self.max_length = 768
        self.lora_r = 16
        self.learning_rate = 1e-4
        
        print("ğŸš€ 13Bãƒ¢ãƒ‡ãƒ«å¯¾å¿œãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°è¨­å®š")
        
    def check_model_availability(self) -> str:
        """åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ã‚’ç¢ºèª"""
        print("ğŸ“‹ ãƒ¢ãƒ‡ãƒ«å¯ç”¨æ€§ãƒã‚§ãƒƒã‚¯ä¸­...")
        
        for model_name in self.model_options:
            try:
                print(f"   è©¦è¡Œ: {model_name}")
                tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
                print(f"âœ… åˆ©ç”¨å¯èƒ½: {model_name}")
                return model_name
            except Exception as e:
                print(f"âŒ åˆ©ç”¨ä¸å¯: {model_name} - {e}")
                continue
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        fallback_model = "elyza/ELYZA-japanese-Llama-2-7b-instruct"
        print(f"ğŸ”„ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: {fallback_model}")
        return fallback_model
    
    def optimize_for_model_size(self, model_name: str):
        """ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã«å¿œã˜ãŸè¨­å®šæœ€é©åŒ–"""
        if "13b" in model_name.lower():
            print("âš¡ 13Bãƒ¢ãƒ‡ãƒ«å‘ã‘è¨­å®š")
            self.batch_size = 1
            self.grad_accumulation = 8  # ãƒ¡ãƒ¢ãƒªç¯€ç´„
            self.max_length = 512       # çŸ­ç¸®
            self.lora_r = 8            # LoRAå‰Šæ¸›
        elif "7b" in model_name.lower():
            print("ğŸ”§ 7Bãƒ¢ãƒ‡ãƒ«å‘ã‘è¨­å®š")
            self.batch_size = 1
            self.grad_accumulation = 12
            self.max_length = 768
            self.lora_r = 16
        
        print(f"   ãƒãƒƒãƒã‚µã‚¤ã‚º: {self.batch_size}")
        print(f"   å‹¾é…è“„ç©: {self.grad_accumulation}")
        print(f"   æœ€å¤§é•·: {self.max_length}")
        print(f"   LoRA r: {self.lora_r}")
    
    def prepare_data(self):
        """ãƒ‡ãƒ¼ã‚¿æº–å‚™"""
        print("ğŸ“Š é«˜å“è³ªãƒ‡ãƒ¼ã‚¿æº–å‚™ä¸­...")
        
        def preprocess_function(examples):
            instruction = examples.get("instruction", "").strip()
            input_text = examples.get("input", "").strip()
            output_text = examples.get("output", "").strip()
            
            if instruction:
                prompt = f"### æŒ‡ç¤º:\n{instruction}\n\n### å…¥åŠ›:\n{input_text}\n\n### å¿œç­”:\n"
            else:
                prompt = f"### å…¥åŠ›:\n{input_text}\n\n### å¿œç­”:\n"
            
            return {"prompt": prompt, "completion": output_text}
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰
        raw_dataset = load_dataset("json", data_files={"train": self.jsonl_path}, split="train")
        print(f"   å…ƒãƒ‡ãƒ¼ã‚¿: {len(raw_dataset):,}ä»¶")
        
        # å“è³ªãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆé•·ã™ãã‚‹ãƒ»çŸ­ã™ãã‚‹ãƒ‡ãƒ¼ã‚¿ã‚’é™¤å¤–ï¼‰
        def quality_filter(example):
            input_len = len(example.get('input', ''))
            output_len = len(example.get('output', ''))
            return 10 <= input_len <= 500 and 5 <= output_len <= 200
        
        filtered_dataset = raw_dataset.filter(quality_filter)
        print(f"   å“è³ªãƒ•ã‚£ãƒ«ã‚¿å¾Œ: {len(filtered_dataset):,}ä»¶")
        
        # å‰å‡¦ç†é©ç”¨
        dataset = filtered_dataset.map(preprocess_function, remove_columns=filtered_dataset.column_names)
        dataset = dataset.train_test_split(test_size=0.1, seed=42)
        
        print(f"   å­¦ç¿’ãƒ‡ãƒ¼ã‚¿: {len(dataset['train']):,}ä»¶")
        print(f"   è©•ä¾¡ãƒ‡ãƒ¼ã‚¿: {len(dataset['test']):,}ä»¶")
        
        return dataset
    
    def tokenize_data(self, dataset, tokenizer):
        """ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚ºï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥åˆ©ç”¨ï¼‰"""
        print("ğŸ”¤ ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚ºå®Ÿè¡Œ...")
        
        def tokenize_function(examples):
            prompts = examples["prompt"]
            completions = examples["completion"]
            
            full_texts = [p + c + tokenizer.eos_token for p, c in zip(prompts, completions)]
            
            model_inputs = tokenizer(
                full_texts,
                max_length=self.max_length,
                truncation=True,
                padding=False,
            )
            
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆéƒ¨åˆ†ã‚’ãƒã‚¹ã‚¯
            prompt_inputs = tokenizer(prompts, truncation=True, padding=False)
            labels = []
            
            for i, input_ids in enumerate(model_inputs["input_ids"]):
                prompt_len = len(prompt_inputs["input_ids"][i])
                label = input_ids.copy()
                label[:prompt_len] = [-100] * prompt_len
                labels.append(label)
            
            model_inputs["labels"] = labels
            return model_inputs
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç¢ºèª
        try:
            tokenized_dataset = load_from_disk(self.cache_dir)
            print("âœ… ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿")
        except:
            print("â³ ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚ºå®Ÿè¡Œä¸­...")
            tokenized_dataset = dataset.map(
                tokenize_function,
                batched=True,
                remove_columns=["prompt", "completion"],
                num_proc=4
            )
            tokenized_dataset.save_to_disk(self.cache_dir)
            print("âœ… ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚ºå®Œäº†ï¼†ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜")
        
        return tokenized_dataset
    
    def load_model(self, model_name: str):
        """ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ï¼ˆãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ï¼‰"""
        print(f"ğŸ¤– {model_name} èª­ã¿è¾¼ã¿ä¸­...")
        
        # ãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢
        torch.cuda.empty_cache()
        gc.collect()
        
        # é‡å­åŒ–è¨­å®šï¼ˆãƒ¡ãƒ¢ãƒªåŠ¹ç‡é‡è¦–ï¼‰
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch.bfloat16,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4",
        )
        
        # ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            quantization_config=bnb_config,
            device_map="auto",
            trust_remote_code=True,
            torch_dtype=torch.bfloat16,
            low_cpu_mem_usage=True,  # ãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–
        )
        
        model.gradient_checkpointing_enable()
        
        # LoRAè¨­å®š
        lora_config = LoraConfig(
            task_type=TaskType.CAUSAL_LM,
            r=self.lora_r,
            lora_alpha=self.lora_r * 2,
            lora_dropout=0.1,
            target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
        )
        
        model = get_peft_model(model, lora_config)
        
        trainable_params = model.num_parameters(only_trainable=True)
        total_params = model.num_parameters()
        print(f"âœ… LoRAé©ç”¨å®Œäº†")
        print(f"   å­¦ç¿’å¯èƒ½ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {trainable_params:,}")
        print(f"   å…¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {total_params:,}")
        print(f"   å­¦ç¿’å¯èƒ½ç‡: {100 * trainable_params / total_params:.2f}%")
        
        return model
    
    def run_training(self):
        """ãƒ¡ã‚¤ãƒ³ã®å­¦ç¿’å®Ÿè¡Œ"""
        print("ğŸ¯ ã‚¹ã‚±ãƒ¼ãƒ«ã‚¢ãƒƒãƒ—ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°é–‹å§‹")
        
        # 1. ãƒ¢ãƒ‡ãƒ«é¸æŠ
        model_name = self.check_model_availability()
        self.optimize_for_model_size(model_name)
        
        # 2. ãƒ‡ãƒ¼ã‚¿æº–å‚™
        dataset = self.prepare_data()
        
        # 3. ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼
        tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        # 4. ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚º
        tokenized_dataset = self.tokenize_data(dataset, tokenizer)
        
        # 5. ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
        model = self.load_model(model_name)
        
        # 6. ãƒ‡ãƒ¼ã‚¿ã‚³ãƒ¬ã‚¯ã‚¿ãƒ¼
        data_collator = DataCollatorForSeq2Seq(
            tokenizer=tokenizer,
            padding="longest",
            label_pad_token_id=-100,
        )
        
        # 7. å­¦ç¿’è¨­å®š
        training_args = TrainingArguments(
            output_dir=self.output_dir,
            num_train_epochs=3,
            per_device_train_batch_size=self.batch_size,
            per_device_eval_batch_size=self.batch_size,
            gradient_accumulation_steps=self.grad_accumulation,
            learning_rate=self.learning_rate,
            lr_scheduler_type="cosine",
            warmup_steps=200,
            bf16=True,
            optim="paged_adamw_8bit",
            dataloader_num_workers=2,
            
            evaluation_strategy="steps",
            eval_steps=200,
            save_strategy="steps",
            save_steps=200,
            save_total_limit=2,
            load_best_model_at_end=True,
            metric_for_best_model="eval_loss",
            
            logging_steps=20,
            report_to=["tensorboard"],
            
            remove_unused_columns=False,
            group_by_length=True,
            dataloader_pin_memory=False,
        )
        
        # 8. ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼
        trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=tokenized_dataset["train"],
            eval_dataset=tokenized_dataset["test"],
            data_collator=data_collator,
            tokenizer=tokenizer,
            callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]
        )
        
        # 9. å­¦ç¿’å®Ÿè¡Œ
        print("ğŸš€ å­¦ç¿’é–‹å§‹...")
        trainer.train()
        
        # 10. ä¿å­˜
        print("ğŸ’¾ ãƒ¢ãƒ‡ãƒ«ä¿å­˜ä¸­...")
        trainer.save_model()
        tokenizer.save_pretrained(self.output_dir)
        
        print("ğŸ‰ ã‚¹ã‚±ãƒ¼ãƒ«ã‚¢ãƒƒãƒ—å­¦ç¿’å®Œäº†ï¼")
        print(f"ğŸ“ ä¿å­˜å…ˆ: {self.output_dir}")
        print(f"ğŸ¤– ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: {model_name}")
        print("\næ¬¡ã®æ‰‹é †:")
        print("1. config.py ã® lora_adapter_path ã‚’ä»¥ä¸‹ã«å¤‰æ›´:")
        print(f"   self.lora_adapter_path = '{self.output_dir}'")
        print("2. vLLMæœ€é©åŒ–ã‚’å®Ÿè¡Œ")

if __name__ == "__main__":
    tuner = ScaledUpFineTuner()
    tuner.run_training()