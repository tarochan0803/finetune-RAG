# RAGapp_optimized.py - æœ€é©åŒ–å®Œäº†ç‰ˆRAGã‚¢ãƒ—ãƒª
import streamlit as st
import pandas as pd
import os
import datetime
import sys
import logging
import json
import torch
from typing import Optional, Tuple, List, Dict, Any

# æœ€é©åŒ–ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    from config_optimized import Config, setup_logging
    from rag_query_utils import initialize_pipeline, ask_question_ensemble_stream
    from utils import format_document_snippet, normalize_str, preprocess_query
except ImportError as e:
    st.error(f"Import Error: {e}")
    st.stop()

# ã‚°ãƒ­ãƒ¼ãƒãƒ«è¨­å®š
config = Config()
logger = setup_logging(config, log_filename="rag_optimized_app.log")

@st.cache_resource
def load_optimized_pipeline():
    """æœ€é©åŒ–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åˆæœŸåŒ–"""
    logger.info("æœ€é©åŒ–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åˆæœŸåŒ–ä¸­...")
    try:
        pipeline_components = initialize_pipeline(config, logger)
        if not all(comp is not None for comp in pipeline_components):
            logger.error("ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åˆæœŸåŒ–å¤±æ•—")
            return (None,) * 4
        logger.info("æœ€é©åŒ–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åˆæœŸåŒ–å®Œäº†")
        return pipeline_components
    except Exception as e:
        logger.critical(f"ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
        return (None,) * 4

def main():
    st.set_page_config(page_title="æœ€é©åŒ–RAG", layout="centered")
    st.title("ğŸš€ æœ€é©åŒ–RAGã‚·ã‚¹ãƒ†ãƒ ")
    st.caption("é«˜å“è³ªãƒ‡ãƒ¼ã‚¿ + ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚° + æ¨è«–æœ€é©åŒ–")
    
    # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åˆæœŸåŒ–
    with st.spinner("æœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–ä¸­..."):
        vectordb, intermediate_llm, tokenizer, embedding_function = load_optimized_pipeline()
    
    if vectordb is None:
        st.error("ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ")
        st.stop()
    
    # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹åˆæœŸåŒ–
    if "query_history" not in st.session_state:
        st.session_state.query_history = []
    if "current_answer_stream" not in st.session_state:
        st.session_state.current_answer_stream = None
    
    # ã‚µã‚¤ãƒ‰ãƒãƒ¼
    with st.sidebar:
        st.header("âš™ï¸ æœ€é©åŒ–è¨­å®š")
        st.success("âœ… é«˜å“è³ªãƒ‡ãƒ¼ã‚¿ï¼ˆ60,403ä»¶ï¼‰")
        st.success("âœ… ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿")
        st.success("âœ… æ¨è«–æœ€é©åŒ–")
        
        if torch.cuda.is_available():
            gpu_name = torch.cuda.get_device_name(0)
            st.info(f"GPU: {gpu_name}")
    
    # ä¼šè©±å±¥æ­´è¡¨ç¤º
    for message in st.session_state.query_history:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])
    
    # è³ªå•å…¥åŠ›
    if user_input := st.chat_input("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„..."):
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸è¡¨ç¤º
        with st.chat_message("user"):
            st.markdown(user_input)
        st.session_state.query_history.append({"role": "user", "content": user_input})
        
        # AIå¿œç­”ç”Ÿæˆ
        with st.chat_message("assistant"):
            response_placeholder = st.empty()
            
            try:
                # æœ€é©åŒ–ã‚¯ã‚¨ãƒªå‡¦ç†
                response = ask_question_ensemble_stream(
                    vectordb=vectordb,
                    intermediate_llm=intermediate_llm,
                    tokenizer=tokenizer,
                    embedding_function=embedding_function,
                    config=config,
                    query=preprocess_query(user_input),
                    logger=logger
                )
                
                # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°è¡¨ç¤º
                full_response = ""
                for chunk in response.get("result_stream", []):
                    full_response += chunk
                    response_placeholder.markdown(full_response + "â–Œ")
                
                response_placeholder.markdown(full_response)
                st.session_state.query_history.append({"role": "assistant", "content": full_response})
                
            except Exception as e:
                error_msg = f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}"
                response_placeholder.error(error_msg)
                st.session_state.query_history.append({"role": "assistant", "content": error_msg})

if __name__ == "__main__":
    main()
