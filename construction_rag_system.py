#!/usr/bin/env python3
"""
å»ºè¨­æ¥­ç•Œç‰¹åŒ–RAGã‚·ã‚¹ãƒ†ãƒ  - ãƒ¡ã‚¤ãƒ³ã‚¯ãƒ©ã‚¹
ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ¤œç´¢ã€ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã€Gemini APIçµ±åˆ
"""

import logging
import os
import json
import asyncio
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass
from concurrent.futures import ThreadPoolExecutor, as_completed
import time

import chromadb
from chromadb.config import Settings
from sentence_transformers import SentenceTransformer
import torch
from transformers import (
    AutoTokenizer, AutoModelForCausalLM, 
    BitsAndBytesConfig, TextStreamer
)
from peft import PeftModel
import google.generativeai as genai

from config import Config, setup_logging

@dataclass
class SearchResult:
    """æ¤œç´¢çµæœã‚’æ ¼ç´ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹"""
    document: str
    metadata: Dict[str, Any]
    score: float
    company: str
    source: str

@dataclass
class RAGResponse:
    """RAGå¿œç­”ã‚’æ ¼ç´ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹"""
    query: str
    final_answer: str
    intermediate_answers: List[str]
    search_results: List[List[SearchResult]]
    processing_time: float
    confidence_score: float

class ConstructionRAGSystem:
    """å»ºè¨­æ¥­ç•Œç‰¹åŒ–RAGã‚·ã‚¹ãƒ†ãƒ ã®ãƒ¡ã‚¤ãƒ³ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, config: Config):
        self.config = config
        self.logger = setup_logging(config, "construction_rag.log")
        
        # åˆæœŸåŒ–ãƒ•ãƒ©ã‚°
        self._embedding_model = None
        self._chroma_client = None
        self._collection = None
        self._tokenizer = None
        self._base_model = None
        self._fine_tuned_model = None
        
        # Gemini APIåˆæœŸåŒ–
        if config.gemini_api_key:
            genai.configure(api_key=config.gemini_api_key)
            self.gemini_model = genai.GenerativeModel(config.synthesizer_model_name)
            self.logger.info("Gemini APIåˆæœŸåŒ–å®Œäº†")
        else:
            self.gemini_model = None
            self.logger.warning("Gemini APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
    
    def initialize(self) -> bool:
        """ã‚·ã‚¹ãƒ†ãƒ å…¨ä½“ã‚’åˆæœŸåŒ–"""
        try:
            self.logger.info("å»ºè¨­æ¥­ç•ŒRAGã‚·ã‚¹ãƒ†ãƒ ã‚’åˆæœŸåŒ–ä¸­...")
            
            # åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–
            self._init_embedding_model()
            
            # ChromaDBã®åˆæœŸåŒ–
            self._init_chroma_db()
            
            # ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–
            self._init_fine_tuned_model()
            
            self.logger.info("âœ… RAGã‚·ã‚¹ãƒ†ãƒ ã®åˆæœŸåŒ–ãŒå®Œäº†ã—ã¾ã—ãŸ")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ RAGã‚·ã‚¹ãƒ†ãƒ ã®åˆæœŸåŒ–ã«å¤±æ•—: {e}")
            return False
    
    def _init_embedding_model(self):
        """åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–"""
        self.logger.info(f"åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­: {self.config.embeddings_model}")
        self._embedding_model = SentenceTransformer(self.config.embeddings_model)
        self.logger.info("åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–å®Œäº†")
    
    def _init_chroma_db(self):
        """ChromaDBã‚’åˆæœŸåŒ–"""
        self.logger.info("ChromaDBã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’åˆæœŸåŒ–ä¸­...")
        self._chroma_client = chromadb.PersistentClient(
            path=self.config.persist_directory,
            settings=Settings(anonymized_telemetry=False, allow_reset=True)
        )
        
        try:
            self._collection = self._chroma_client.get_collection(self.config.collection_name)
            count = self._collection.count()
            self.logger.info(f"æ—¢å­˜ã®ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‚’èª­ã¿è¾¼ã¿: {count} ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ")
        except Exception as e:
            self.logger.error(f"ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—: {e}")
            raise
    
    def _init_fine_tuned_model(self):
        """ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–"""
        self.logger.info("ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­...")
        
        try:
            # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®èª­ã¿è¾¼ã¿
            self._tokenizer = AutoTokenizer.from_pretrained(
                self.config.base_model_name,
                trust_remote_code=True
            )
            
            # ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿
            self._base_model = AutoModelForCausalLM.from_pretrained(
                self.config.base_model_name,
                torch_dtype=self.config.model_load_dtype,
                device_map="cpu" if self.config.force_cpu else "auto",
                trust_remote_code=True
            )
            
            # LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã®èª­ã¿è¾¼ã¿
            if os.path.exists(self.config.lora_adapter_path):
                self._fine_tuned_model = PeftModel.from_pretrained(
                    self._base_model,
                    self.config.lora_adapter_path
                )
                self.logger.info("LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼èª­ã¿è¾¼ã¿å®Œäº†")
            else:
                self._fine_tuned_model = self._base_model
                self.logger.warning("LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¾ã™")
            
            # ãƒ‘ãƒƒãƒ‰ãƒˆãƒ¼ã‚¯ãƒ³ã®è¨­å®š
            if self._tokenizer.pad_token is None:
                self._tokenizer.pad_token = self._tokenizer.eos_token
            
            self.logger.info("ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–å®Œäº†")
            
        except Exception as e:
            self.logger.error(f"ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–ã«å¤±æ•—: {e}")
            raise
    
    def ensemble_search(self, query: str, num_variants: int = 3) -> List[List[SearchResult]]:
        """ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ¤œç´¢ã‚’å®Ÿè¡Œ"""
        self.logger.info(f"ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ¤œç´¢ã‚’å®Ÿè¡Œ: '{query}'")
        
        all_results = []
        
        # ç•°ãªã‚‹æ¤œç´¢æˆ¦ç•¥ã‚’å®Ÿè¡Œ
        strategies = [
            ("åŸºæœ¬æ¤œç´¢", lambda q: self._basic_search(q, k=self.config.rag_variant_k[0])),
            ("æ‹¡å¼µæ¤œç´¢", lambda q: self._expanded_search(q, k=self.config.rag_variant_k[1])),
            ("ä¼šç¤¾ç‰¹åŒ–æ¤œç´¢", lambda q: self._company_focused_search(q, k=self.config.rag_variant_k[2]))
        ]
        
        for strategy_name, search_func in strategies[:num_variants]:
            try:
                self.logger.info(f"{strategy_name}ã‚’å®Ÿè¡Œä¸­...")
                results = search_func(query)
                all_results.append(results)
                self.logger.info(f"{strategy_name}å®Œäº†: {len(results)} ä»¶ã®çµæœ")
            except Exception as e:
                self.logger.error(f"{strategy_name}ã§ã‚¨ãƒ©ãƒ¼: {e}")
                all_results.append([])
        
        return all_results
    
    def _basic_search(self, query: str, k: int = 5) -> List[SearchResult]:
        """åŸºæœ¬çš„ãªã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢"""
        results = self._collection.query(
            query_texts=[query],
            n_results=k
        )
        
        search_results = []
        for i in range(len(results['documents'][0])):
            search_results.append(SearchResult(
                document=results['documents'][0][i],
                metadata=results['metadatas'][0][i],
                score=1 - results['distances'][0][i],  # è·é›¢ã‚’é¡ä¼¼åº¦ã«å¤‰æ›
                company=results['metadatas'][0][i].get('company', 'ä¸æ˜'),
                source='basic_search'
            ))
        
        return search_results
    
    def _expanded_search(self, query: str, k: int = 5) -> List[SearchResult]:
        """æ‹¡å¼µã‚¯ã‚¨ãƒªã«ã‚ˆã‚‹æ¤œç´¢"""
        # ã‚¯ã‚¨ãƒªæ‹¡å¼µ
        expanded_queries = [query]
        
        # ç°¡å˜ãªã‚¯ã‚¨ãƒªæ‹¡å¼µï¼ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰è¿½åŠ ï¼‰
        if "ä¼šç¤¾" in query or "ä¼æ¥­" in query:
            expanded_queries.append(query + " å»ºè¨­")
        if "ä»•æ§˜" in query:
            expanded_queries.append(query + " æ¡ä»¶")
        
        all_results = []
        for exp_query in expanded_queries:
            results = self._collection.query(
                query_texts=[exp_query],
                n_results=k//len(expanded_queries) + 1
            )
            
            for i in range(len(results['documents'][0])):
                all_results.append(SearchResult(
                    document=results['documents'][0][i],
                    metadata=results['metadatas'][0][i],
                    score=1 - results['distances'][0][i],
                    company=results['metadatas'][0][i].get('company', 'ä¸æ˜'),
                    source='expanded_search'
                ))
        
        # é‡è¤‡å‰Šé™¤ã¨ã‚¹ã‚³ã‚¢é †ã‚½ãƒ¼ãƒˆ
        unique_results = {}
        for result in all_results:
            doc_id = result.metadata.get('line_number', result.document[:50])
            if doc_id not in unique_results or result.score > unique_results[doc_id].score:
                unique_results[doc_id] = result
        
        sorted_results = sorted(unique_results.values(), key=lambda x: x.score, reverse=True)
        return sorted_results[:k]
    
    def _company_focused_search(self, query: str, k: int = 5) -> List[SearchResult]:
        """ä¼šç¤¾ç‰¹åŒ–æ¤œç´¢"""
        # ä¼šç¤¾åãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        potential_company_keywords = ["æ ªå¼ä¼šç¤¾", "æœ‰é™ä¼šç¤¾", "å»ºè¨­", "å·¥å‹™åº—", "ãƒã‚¦ã‚¹"]
        
        company_filter = {}
        for keyword in potential_company_keywords:
            if keyword in query:
                # ä¼šç¤¾é–¢é€£ã®æ¤œç´¢ã¨ã—ã¦å‡¦ç†
                results = self._collection.query(
                    query_texts=[query],
                    n_results=k*2,  # ã‚ˆã‚Šå¤šãå–å¾—ã—ã¦å¾Œã§ãƒ•ã‚£ãƒ«ã‚¿
                    where={"type": {"$ne": "invalid"}}  # åŸºæœ¬çš„ãªãƒ•ã‚£ãƒ«ã‚¿
                )
                break
        else:
            # é€šå¸¸ã®æ¤œç´¢
            results = self._collection.query(
                query_texts=[query],
                n_results=k
            )
        
        search_results = []
        for i in range(len(results['documents'][0])):
            search_results.append(SearchResult(
                document=results['documents'][0][i],
                metadata=results['metadatas'][0][i],
                score=1 - results['distances'][0][i],
                company=results['metadatas'][0][i].get('company', 'ä¸æ˜'),
                source='company_focused_search'
            ))
        
        return search_results[:k]
    
    def generate_intermediate_answer(self, query: str, context: str) -> str:
        """ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã§ä¸­é–“å›ç­”ã‚’ç”Ÿæˆ"""
        prompt = self.config.intermediate_prompt_template.format(
            context=context,
            question=query
        )
        
        try:
            inputs = self._tokenizer(
                prompt,
                return_tensors="pt",
                truncation=True,
                max_length=2048
            )
            
            with torch.no_grad():
                outputs = self._fine_tuned_model.generate(
                    inputs.input_ids,
                    max_new_tokens=self.config.max_new_tokens,
                    temperature=self.config.temperature,
                    do_sample=self.config.do_sample,
                    num_beams=self.config.num_beams,
                    repetition_penalty=self.config.repetition_penalty,
                    pad_token_id=self._tokenizer.pad_token_id,
                    eos_token_id=self._tokenizer.eos_token_id
                )
            
            response = self._tokenizer.decode(
                outputs[0][inputs.input_ids.shape[1]:],
                skip_special_tokens=True
            ).strip()
            
            return response
            
        except Exception as e:
            self.logger.error(f"ä¸­é–“å›ç­”ç”Ÿæˆã§ã‚¨ãƒ©ãƒ¼: {e}")
            return "æä¾›ã•ã‚ŒãŸæƒ…å ±ã‹ã‚‰ã¯åˆ¤æ–­ã§ãã¾ã›ã‚“ã€‚"
    
    def synthesize_final_answer(self, query: str, intermediate_answers: List[str], 
                              contexts: List[str]) -> str:
        """Gemini APIã§æœ€çµ‚å›ç­”ã‚’çµ±åˆ"""
        if not self.gemini_model:
            # Gemini APIãŒåˆ©ç”¨ã§ããªã„å ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            valid_answers = [ans for ans in intermediate_answers if "åˆ¤æ–­ã§ãã¾ã›ã‚“" not in ans]
            if valid_answers:
                return valid_answers[0]
            else:
                return "æä¾›ã•ã‚ŒãŸæƒ…å ±ã‹ã‚‰ã¯åˆ¤æ–­ã§ãã¾ã›ã‚“ã€‚"
        
        try:
            # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚¹ãƒ‹ãƒšãƒƒãƒˆã‚’ä½œæˆ
            context_snippets = []
            for ctx in contexts:
                snippet = ctx[:100] + "..." if len(ctx) > 100 else ctx
                context_snippets.append(snippet)
            
            synthesis_prompt = self.config.synthesis_prompt_template.format(
                original_question=query,
                answer_1=intermediate_answers[0] if len(intermediate_answers) > 0 else "å›ç­”ãªã—",
                answer_2=intermediate_answers[1] if len(intermediate_answers) > 1 else "å›ç­”ãªã—",
                answer_3=intermediate_answers[2] if len(intermediate_answers) > 2 else "å›ç­”ãªã—",
                context_1_snippet=context_snippets[0] if len(context_snippets) > 0 else "ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãªã—",
                context_2_snippet=context_snippets[1] if len(context_snippets) > 1 else "ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãªã—",
                context_3_snippet=context_snippets[2] if len(context_snippets) > 2 else "ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãªã—"
            )
            
            response = self.gemini_model.generate_content(synthesis_prompt)
            return response.text.strip()
            
        except Exception as e:
            self.logger.error(f"æœ€çµ‚å›ç­”çµ±åˆã§ã‚¨ãƒ©ãƒ¼: {e}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†
            valid_answers = [ans for ans in intermediate_answers if "åˆ¤æ–­ã§ãã¾ã›ã‚“" not in ans]
            if valid_answers:
                return valid_answers[0]
            else:
                return "æä¾›ã•ã‚ŒãŸæƒ…å ±ã‹ã‚‰ã¯åˆ¤æ–­ã§ãã¾ã›ã‚“ã€‚"
    
    def process_query(self, query: str) -> RAGResponse:
        """ã‚¯ã‚¨ãƒªã‚’å‡¦ç†ã—ã¦RAGå¿œç­”ã‚’ç”Ÿæˆ"""
        start_time = time.time()
        self.logger.info(f"ã‚¯ã‚¨ãƒªå‡¦ç†é–‹å§‹: '{query}'")
        
        try:
            # 1. ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ¤œç´¢
            search_results = self.ensemble_search(query)
            
            # 2. ä¸­é–“å›ç­”ç”Ÿæˆ
            intermediate_answers = []
            contexts = []
            
            for i, results in enumerate(search_results):
                if results:
                    # ä¸Šä½çµæœã‹ã‚‰ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’æ§‹ç¯‰
                    context = "\n".join([f"- {r.document}" for r in results[:3]])
                    contexts.append(context)
                    
                    # ä¸­é–“å›ç­”ã‚’ç”Ÿæˆ
                    answer = self.generate_intermediate_answer(query, context)
                    intermediate_answers.append(answer)
                else:
                    contexts.append("")
                    intermediate_answers.append("é–¢é€£æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            
            # 3. æœ€çµ‚å›ç­”çµ±åˆ
            final_answer = self.synthesize_final_answer(query, intermediate_answers, contexts)
            
            processing_time = time.time() - start_time
            
            # ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢è¨ˆç®—ï¼ˆç°¡æ˜“ç‰ˆï¼‰
            confidence_score = self._calculate_confidence(intermediate_answers, search_results)
            
            response = RAGResponse(
                query=query,
                final_answer=final_answer,
                intermediate_answers=intermediate_answers,
                search_results=search_results,
                processing_time=processing_time,
                confidence_score=confidence_score
            )
            
            self.logger.info(f"ã‚¯ã‚¨ãƒªå‡¦ç†å®Œäº†: {processing_time:.2f}ç§’, ä¿¡é ¼åº¦: {confidence_score:.2f}")
            return response
            
        except Exception as e:
            self.logger.error(f"ã‚¯ã‚¨ãƒªå‡¦ç†ã§ã‚¨ãƒ©ãƒ¼: {e}")
            return RAGResponse(
                query=query,
                final_answer=f"å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}",
                intermediate_answers=[],
                search_results=[],
                processing_time=time.time() - start_time,
                confidence_score=0.0
            )
    
    def _calculate_confidence(self, intermediate_answers: List[str], 
                            search_results: List[List[SearchResult]]) -> float:
        """ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—"""
        # ç°¡æ˜“çš„ãªä¿¡é ¼åº¦è¨ˆç®—
        valid_answers = len([ans for ans in intermediate_answers if "åˆ¤æ–­ã§ãã¾ã›ã‚“" not in ans])
        total_answers = len(intermediate_answers)
        
        # æ¤œç´¢çµæœã®å“è³ª
        avg_search_score = 0.0
        total_results = 0
        for results in search_results:
            if results:
                avg_search_score += sum(r.score for r in results[:3])
                total_results += len(results[:3])
        
        if total_results > 0:
            avg_search_score /= total_results
        
        # ç·åˆä¿¡é ¼åº¦
        answer_confidence = valid_answers / total_answers if total_answers > 0 else 0.0
        search_confidence = avg_search_score
        
        return (answer_confidence * 0.6 + search_confidence * 0.4)

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†ï¼ˆãƒ†ã‚¹ãƒˆç”¨ï¼‰"""
    config = Config()
    rag_system = ConstructionRAGSystem(config)
    
    if rag_system.initialize():
        # ãƒ†ã‚¹ãƒˆã‚¯ã‚¨ãƒª
        test_queries = [
            "æ ªå¼ä¼šç¤¾å¹³æˆå»ºè¨­ã®å£é¢æä»•æ§˜ã¯ä½•ã§ã™ã‹ï¼Ÿ",
            "å¤§å£ä»•æ§˜ã®ä¼šç¤¾ã‚’æ•™ãˆã¦ãã ã•ã„",
            "ç¾½æŸ„æã®ä¾›çµ¦ãŒã‚ã‚‹ä¼šç¤¾ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ"
        ]
        
        for query in test_queries:
            print(f"\nğŸ” ã‚¯ã‚¨ãƒª: {query}")
            response = rag_system.process_query(query)
            print(f"ğŸ“ å›ç­”: {response.final_answer}")
            print(f"â±ï¸  å‡¦ç†æ™‚é–“: {response.processing_time:.2f}ç§’")
            print(f"ğŸ“Š ä¿¡é ¼åº¦: {response.confidence_score:.2f}")

if __name__ == "__main__":
    main()