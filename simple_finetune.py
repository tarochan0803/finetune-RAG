# simple_finetune.py - ã‚·ãƒ³ãƒ—ãƒ«ç‰ˆãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°
# äº’æ›æ€§é‡è¦–ã§ç¢ºå®Ÿã«å‹•ä½œ

import torch
import os
from datasets import load_dataset
from transformers import (
    AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer,
    DataCollatorForLanguageModeling
)
from peft import LoraConfig, get_peft_model, TaskType
import warnings
warnings.filterwarnings("ignore")

print("ğŸš€ ã‚·ãƒ³ãƒ—ãƒ«ç‰ˆãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°é–‹å§‹")

# CPUå¼·åˆ¶
os.environ["CUDA_VISIBLE_DEVICES"] = ""

# è¨­å®š
MODEL_NAME = "elyza/ELYZA-japanese-Llama-2-7b-instruct"
OUTPUT_DIR = "./rag_simple_model"
JSONL_PATH = "/home/ncnadmin/my_rag_project/premium_training_dataset.jsonl"

print("ğŸ“Š ãƒ‡ãƒ¼ã‚¿æº–å‚™...")

# è»½é‡ãƒ‡ãƒ¼ã‚¿æº–å‚™
dataset = load_dataset("json", data_files={"train": JSONL_PATH}, split="train")
sample_dataset = dataset.select(range(200))  # 200ã‚µãƒ³ãƒ—ãƒ«ã®ã¿

def simple_format(example):
    input_text = example.get("input", "").strip()
    output_text = example.get("output", "").strip()
    text = f"è³ªå•: {input_text}\nå›ç­”: {output_text}<|endoftext|>"
    return {"text": text}

formatted_dataset = sample_dataset.map(simple_format)
train_test = formatted_dataset.train_test_split(test_size=0.1)

print(f"   å­¦ç¿’ãƒ‡ãƒ¼ã‚¿: {len(train_test['train'])}ä»¶")
print(f"   è©•ä¾¡ãƒ‡ãƒ¼ã‚¿: {len(train_test['test'])}ä»¶")

# ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼
print("ğŸ”¤ ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼è¨­å®š...")
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

def tokenize(examples):
    return tokenizer(
        examples["text"],
        truncation=True,
        padding=False,
        max_length=128,  # çŸ­ç¸®
    )

tokenized = train_test.map(tokenize, batched=True, remove_columns=["instruction", "input", "output", "text"])

# ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ï¼ˆCPUç”¨ï¼‰
print("ğŸ¤– ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿...")
model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    torch_dtype=torch.float32,
    trust_remote_code=True,
)

# æœ€å°LoRA
print("âš™ï¸ LoRAè¨­å®š...")
lora_config = LoraConfig(
    r=1,  # æœ€å°
    lora_alpha=2,
    target_modules=["q_proj"],
    lora_dropout=0.1,
    task_type=TaskType.CAUSAL_LM,
)

model = get_peft_model(model, lora_config)
print(f"å­¦ç¿’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {model.num_parameters(only_trainable=True):,}")

# ã‚·ãƒ³ãƒ—ãƒ«å­¦ç¿’è¨­å®š
print("ğŸ¯ å­¦ç¿’è¨­å®š...")
training_args = TrainingArguments(
    output_dir=OUTPUT_DIR,
    num_train_epochs=1,
    per_device_train_batch_size=1,
    learning_rate=1e-3,
    logging_steps=10,
    save_steps=100,
    remove_unused_columns=False,
    no_cuda=True,
)

# ãƒ‡ãƒ¼ã‚¿ã‚³ãƒ¬ã‚¯ã‚¿ãƒ¼
data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm=False,
)

# ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized["train"],
    data_collator=data_collator,
)

print("ğŸš€ å­¦ç¿’é–‹å§‹...")
try:
    trainer.train()
    
    print("ğŸ’¾ ä¿å­˜ä¸­...")
    model.save_pretrained(OUTPUT_DIR)
    tokenizer.save_pretrained(OUTPUT_DIR)
    
    print("ğŸ‰ ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å®Œäº†ï¼")
    print(f"ä¿å­˜å…ˆ: {OUTPUT_DIR}")
    
    # ãƒ†ã‚¹ãƒˆæ¨è«–
    print("\nğŸ§ª ãƒ†ã‚¹ãƒˆæ¨è«–...")
    test_input = "è³ªå•: æ ªå¼ä¼šç¤¾ä¸‰å»ºã®å£ä»•æ§˜ã«ã¤ã„ã¦\nå›ç­”: "
    inputs = tokenizer(test_input, return_tensors="pt")
    
    with torch.no_grad():
        outputs = model.generate(
            inputs.input_ids,
            max_new_tokens=50,
            temperature=0.7,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id
        )
    
    result = tokenizer.decode(outputs[0], skip_special_tokens=True)
    print(f"ãƒ†ã‚¹ãƒˆçµæœ: {result}")
    
except Exception as e:
    print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
    import traceback
    traceback.print_exc()