# cpu_optimized_finetune.py - CPUç‰ˆé«˜é€Ÿãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°
# RTX 5070ã®CUDAäº’æ›æ€§å•é¡Œã‚’å›é¿

import torch
import os
from datasets import load_dataset
from transformers import (
    AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer,
    DataCollatorForLanguageModeling
)
from peft import LoraConfig, get_peft_model, TaskType
import warnings
warnings.filterwarnings("ignore")

print("ğŸ”¥ CPUæœ€é©åŒ–ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆRTX 5070å¯¾å¿œï¼‰")

# CPUå¼·åˆ¶è¨­å®š
os.environ["CUDA_VISIBLE_DEVICES"] = ""
torch.cuda.is_available = lambda: False

# è»½é‡è¨­å®š
MODEL_NAME = "elyza/ELYZA-japanese-Llama-2-7b-instruct"
OUTPUT_DIR = "./rag_cpu_optimized_model"
JSONL_PATH = "/home/ncnadmin/my_rag_project/premium_training_dataset.jsonl"
SAMPLE_SIZE = 500  # è¶…è»½é‡ï¼ˆ500ã‚µãƒ³ãƒ—ãƒ«ï¼‰

print(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿æº–å‚™ä¸­...")

# ãƒ‡ãƒ¼ã‚¿æº–å‚™
dataset = load_dataset("json", data_files={"train": JSONL_PATH}, split="train")
print(f"   å…ƒãƒ‡ãƒ¼ã‚¿: {len(dataset):,}ä»¶")

# é«˜å“è³ªã‚µãƒ³ãƒ—ãƒ«é¸æŠï¼ˆã‚¨ãƒ©ãƒ¼ã®å°‘ãªã„ã‚‚ã®ï¼‰
def quality_filter(example):
    input_text = example.get('input', '')
    output_text = example.get('output', '')
    # æ—¥æœ¬èªã‚’å«ã¿ã€é©åˆ‡ãªé•·ã•ã®ãƒ‡ãƒ¼ã‚¿ã‚’é¸æŠ
    return (
        'æ ªå¼ä¼šç¤¾' in input_text and
        10 <= len(input_text) <= 200 and
        5 <= len(output_text) <= 100 and
        not '[Error' in output_text
    )

filtered_dataset = dataset.filter(quality_filter)
print(f"   å“è³ªãƒ•ã‚£ãƒ«ã‚¿å¾Œ: {len(filtered_dataset):,}ä»¶")

# æœ€é«˜å“è³ªã‚µãƒ³ãƒ—ãƒ«ã‚’é¸æŠ
selected_dataset = filtered_dataset.select(range(min(SAMPLE_SIZE, len(filtered_dataset))))
print(f"   é¸æŠãƒ‡ãƒ¼ã‚¿: {len(selected_dataset):,}ä»¶")

def format_prompt(example):
    instruction = example.get("instruction", "").strip()
    input_text = example.get("input", "").strip()
    output_text = example.get("output", "").strip()
    
    if instruction:
        text = f"### æŒ‡ç¤º:\n{instruction}\n\n### å…¥åŠ›:\n{input_text}\n\n### å¿œç­”:\n{output_text}<|endoftext|>"
    else:
        text = f"### å…¥åŠ›:\n{input_text}\n\n### å¿œç­”:\n{output_text}<|endoftext|>"
    return {"text": text}

dataset_formatted = selected_dataset.map(format_prompt)
dataset_split = dataset_formatted.train_test_split(test_size=0.1)
print(f"   å­¦ç¿’: {len(dataset_split['train'])}ä»¶, è©•ä¾¡: {len(dataset_split['test'])}ä»¶")

# ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼
print("ğŸ”¤ ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼åˆæœŸåŒ–...")
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

def tokenize_function(examples):
    return tokenizer(
        examples["text"],
        truncation=True,
        padding=False,
        max_length=256,  # çŸ­ç¸®
    )

tokenized_dataset = dataset_split.map(
    tokenize_function,
    batched=True,
    remove_columns=["instruction", "input", "output", "text"]
)

# CPUç‰ˆãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
print("ğŸ¤– CPUç‰ˆãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿...")
model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    torch_dtype=torch.float32,  # CPUç”¨
    trust_remote_code=True,
    device_map=None,  # CPUä½¿ç”¨
)

# è¶…è»½é‡LoRAè¨­å®š
print("âš™ï¸ è¶…è»½é‡LoRAè¨­å®š...")
lora_config = LoraConfig(
    r=2,  # æœ€å°
    lora_alpha=4,
    target_modules=["q_proj"],  # 1ã¤ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã¿
    lora_dropout=0.05,
    task_type=TaskType.CAUSAL_LM,
)

model = get_peft_model(model, lora_config)
trainable_params = model.num_parameters(only_trainable=True)
total_params = model.num_parameters()

print(f"âœ… LoRAé©ç”¨å®Œäº†")
print(f"   å­¦ç¿’å¯èƒ½ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {trainable_params:,}")
print(f"   å…¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {total_params:,}")
print(f"   å­¦ç¿’å¯èƒ½ç‡: {100 * trainable_params / total_params:.3f}%")

# è¶…è»½é‡å­¦ç¿’è¨­å®š
print("ğŸ¯ å­¦ç¿’è¨­å®š...")
training_args = TrainingArguments(
    output_dir=OUTPUT_DIR,
    num_train_epochs=1,  # 1ã‚¨ãƒãƒƒã‚¯ã®ã¿
    per_device_train_batch_size=1,
    gradient_accumulation_steps=2,
    learning_rate=5e-4,  # é«˜ã‚ã®å­¦ç¿’ç‡
    logging_steps=20,
    save_steps=200,
    eval_steps=200,
    evaluation_strategy="steps",
    save_total_limit=1,
    remove_unused_columns=False,
    dataloader_num_workers=0,  # CPUç”¨
    no_cuda=True,  # CPUå¼·åˆ¶
    fp16=False,  # CPUç”¨
)

# ãƒ‡ãƒ¼ã‚¿ã‚³ãƒ¬ã‚¯ã‚¿ãƒ¼
data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm=False,
)

# ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset["train"],
    eval_dataset=tokenized_dataset["test"],
    data_collator=data_collator,
)

print("ğŸš€ CPUå­¦ç¿’é–‹å§‹...")
print("   â€»CPUå­¦ç¿’ã®ãŸã‚æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™ãŒã€ç¢ºå®Ÿã«å‹•ä½œã—ã¾ã™")

try:
    trainer.train()
    
    print("ğŸ’¾ ãƒ¢ãƒ‡ãƒ«ä¿å­˜ä¸­...")
    model.save_pretrained(OUTPUT_DIR)
    tokenizer.save_pretrained(OUTPUT_DIR)
    
    print("ğŸ‰ CPUç‰ˆãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å®Œäº†ï¼")
    print(f"ğŸ“ ä¿å­˜å…ˆ: {OUTPUT_DIR}")
    print(f"ğŸ“Š å­¦ç¿’ãƒ‡ãƒ¼ã‚¿: {len(dataset_split['train'])}ä»¶")
    print(f"âš™ï¸ LoRAãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {trainable_params:,}")
    
    # configæ›´æ–°ç”¨ã®æƒ…å ±
    print("\nğŸ“ æ¬¡ã®æ‰‹é †:")
    print("1. config_optimized.py ã® lora_adapter_path ã‚’ä»¥ä¸‹ã«æ›´æ–°:")
    print(f"   self.lora_adapter_path = '{OUTPUT_DIR}'")
    print("2. RAGã‚¢ãƒ—ãƒªã‚’èµ·å‹•:")
    print("   streamlit run RAGapp_optimized.py")
    
except Exception as e:
    print(f"âŒ å­¦ç¿’ã‚¨ãƒ©ãƒ¼: {e}")
    print("ãƒ‡ãƒãƒƒã‚°ã®ãŸã‚è©³ç´°ã‚’ç¢ºèªã—ã¾ã™...")
    import traceback
    traceback.print_exc()