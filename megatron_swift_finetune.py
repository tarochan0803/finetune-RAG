#!/usr/bin/env python3
# megatron_swift_finetune.py - MS-Swift + Megatron-Core ãƒ•ãƒ«ã‚¹ãƒšãƒƒã‚¯ç‰ˆãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°çµ±åˆã‚·ã‚¹ãƒ†ãƒ 
"""
ã‚¨ãƒ³ã‚¿ãƒ¼ãƒ—ãƒ©ã‚¤ã‚ºã‚°ãƒ¬ãƒ¼ãƒ‰ã®å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ 
- MS-Swift + Megatron-Coreçµ±åˆ
- åˆ†æ•£å­¦ç¿’å¯¾å¿œï¼ˆTensor/Pipeline/Data Parallelï¼‰
- é«˜åº¦ãªæœ€é©åŒ–ï¼ˆFlash Attention, Mixed Precision, Gradient Checkpointingï¼‰
- åŒ…æ‹¬çš„ãªãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ï¼ˆWeights & Biases, TensorBoardï¼‰
- è‡ªå‹•ãƒªã‚½ãƒ¼ã‚¹æœ€é©åŒ–
"""

import os
import sys
import json
import logging
import shutil
import subprocess
import argparse
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional, Any, Union
from dataclasses import dataclass, asdict
import yaml

import torch
import torch.distributed as dist
from datasets import load_dataset
from transformers import AutoTokenizer
import wandb

@dataclass
class ModelConfig:
    """ãƒ¢ãƒ‡ãƒ«è¨­å®š"""
    model_id: str = "Qwen/Qwen3-8B-Base"
    model_alternatives: List[str] = None
    torch_dtype: str = "bfloat16"
    use_flash_attention: bool = True
    trust_remote_code: bool = True
    
    def __post_init__(self):
        if self.model_alternatives is None:
            self.model_alternatives = [
                "Qwen/Qwen3-14B-Base",
                "microsoft/DialoGPT-large",
                "elyza/ELYZA-japanese-Llama-2-7b-instruct"
            ]

@dataclass 
class DataConfig:
    """ãƒ‡ãƒ¼ã‚¿è¨­å®š"""
    dataset_name: str = "kunishou/ApolloCorpus-ja"
    text_column: str = "response_ja"
    output_dir: str = "data"
    output_filename: str = "apollo_cpt.jsonl"
    max_length: int = 32768
    truncation_strategy: str = "right"
    packing: bool = True
    streaming: bool = True
    num_proc: int = 64
    data_format: str = "cpt"  # "cpt" or "chat"

@dataclass
class TrainingConfig:
    """å­¦ç¿’è¨­å®š"""
    # åˆ†æ•£å­¦ç¿’è¨­å®š
    tensor_model_parallel_size: int = 2
    pipeline_model_parallel_size: int = 1
    context_parallel_size: int = 1
    sequence_parallel: bool = True
    
    # ãƒãƒƒãƒã‚µã‚¤ã‚ºè¨­å®š
    micro_batch_size: int = 2
    global_batch_size: int = 512
    
    # å­¦ç¿’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    learning_rate: float = 1e-5
    min_lr: float = 1e-6
    warmup_iters: int = 100
    train_iters: int = 1000
    
    # æœ€é©åŒ–è¨­å®š
    recompute_granularity: str = "full"
    recompute_method: str = "uniform"
    recompute_num_layers: int = 1
    cross_entropy_loss_fusion: bool = True
    
    # ä¿å­˜ãƒ»è©•ä¾¡è¨­å®š
    save_interval: int = 50
    eval_interval: int = 100
    log_interval: int = 1
    
    # ãã®ä»–ã®è¨­å®š
    finetune: bool = True
    no_save_optim: bool = True
    no_save_rng: bool = True
    num_workers: int = 1

@dataclass
class SystemConfig:
    """ã‚·ã‚¹ãƒ†ãƒ è¨­å®š"""
    # å®Ÿè¡Œç’°å¢ƒ
    cuda_visible_devices: str = "0,1,2,3,4,5,6,7"
    nproc_per_node: int = 8
    cuda_device_max_connections: int = 1
    
    # å‡ºåŠ›è¨­å®š
    output_base_dir: str = "outputs"
    checkpoint_dir: str = "checkpoints"
    log_dir: str = "logs"
    converted_model_dir: str = "converted_models"
    
    # ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°
    use_wandb: bool = True
    wandb_project: str = "megatron-swift-finetune"
    use_tensorboard: bool = True
    
    # ãã®ä»–
    seed: int = 42
    debug: bool = False

class MegatronSwiftFineTuner:
    """MS-Swift + Megatron-Coreçµ±åˆãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, config_path: Optional[str] = None):
        self.setup_logging()
        
        # è¨­å®šèª­ã¿è¾¼ã¿
        if config_path and os.path.exists(config_path):
            self.load_config_from_file(config_path)
        else:
            self.model_config = ModelConfig()
            self.data_config = DataConfig()
            self.training_config = TrainingConfig()
            self.system_config = SystemConfig()
        
        # ãƒ©ãƒ³IDç”Ÿæˆ
        self.run_id = datetime.now().strftime("%Y%m%d-%H%M")
        self.run_name = f"{self.get_safe_model_name()}-{self.run_id}"
        
        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªè¨­å®š
        self.setup_directories()
        
        # ç’°å¢ƒåˆæœŸåŒ–
        self.setup_environment()
        
        self.logger.info("ğŸš€ Megatron-Swiftçµ±åˆãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°åˆæœŸåŒ–å®Œäº†")
    
    def setup_logging(self):
        """ãƒ­ã‚°è¨­å®š"""
        log_format = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        logging.basicConfig(
            level=logging.INFO,
            format=log_format,
            handlers=[
                logging.FileHandler(f'megatron_swift_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    def load_config_from_file(self, config_path: str):
        """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿"""
        with open(config_path, 'r', encoding='utf-8') as f:
            if config_path.endswith('.yaml') or config_path.endswith('.yml'):
                config = yaml.safe_load(f)
            else:
                config = json.load(f)
        
        self.model_config = ModelConfig(**config.get('model', {}))
        self.data_config = DataConfig(**config.get('data', {}))
        self.training_config = TrainingConfig(**config.get('training', {}))
        self.system_config = SystemConfig(**config.get('system', {}))
    
    def get_safe_model_name(self) -> str:
        """ãƒ¢ãƒ‡ãƒ«åã‚’å®‰å…¨ãªæ–‡å­—åˆ—ã«å¤‰æ›"""
        model_name = self.model_config.model_id.split('/')[-1]
        return model_name.lower().replace(' ', '-').replace('_', '-')
    
    def setup_directories(self):
        """ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ"""
        base_dirs = [
            self.system_config.output_base_dir,
            self.data_config.output_dir,
        ]
        
        run_dirs = [
            f"{self.system_config.checkpoint_dir}/{self.run_name}",
            f"{self.system_config.log_dir}/{self.run_name}",
            f"{self.system_config.converted_model_dir}/{self.run_name}",
        ]
        
        for dir_path in base_dirs + run_dirs:
            Path(dir_path).mkdir(parents=True, exist_ok=True)
    
    def setup_environment(self):
        """ç’°å¢ƒè¨­å®š"""
        # CUDAè¨­å®š
        os.environ['CUDA_VISIBLE_DEVICES'] = self.system_config.cuda_visible_devices
        os.environ['CUDA_DEVICE_MAX_CONNECTIONS'] = str(self.system_config.cuda_device_max_connections)
        os.environ['NPROC_PER_NODE'] = str(self.system_config.nproc_per_node)
        
        # ãã®ä»–ã®ç’°å¢ƒå¤‰æ•°
        if self.system_config.debug:
            os.environ['TORCH_DISTRIBUTED_DEBUG'] = 'DETAIL'
            os.environ['NCCL_DEBUG'] = 'INFO'
        
        # Weights & BiasesåˆæœŸåŒ–
        if self.system_config.use_wandb:
            wandb.init(
                project=self.system_config.wandb_project,
                name=self.run_name,
                config={
                    "model": asdict(self.model_config),
                    "data": asdict(self.data_config), 
                    "training": asdict(self.training_config),
                    "system": asdict(self.system_config)
                }
            )
    
    def check_environment(self) -> Dict[str, Any]:
        """å®Ÿè¡Œç’°å¢ƒãƒã‚§ãƒƒã‚¯"""
        self.logger.info("=== å®Ÿè¡Œç’°å¢ƒãƒã‚§ãƒƒã‚¯ ===")
        
        env_info = {
            "cuda_available": torch.cuda.is_available(),
            "gpu_count": torch.cuda.device_count() if torch.cuda.is_available() else 0,
            "pytorch_version": torch.__version__,
            "python_version": sys.version,
        }
        
        if env_info["cuda_available"]:
            for i in range(env_info["gpu_count"]):
                gpu_props = torch.cuda.get_device_properties(i)
                env_info[f"gpu_{i}"] = {
                    "name": gpu_props.name,
                    "memory": gpu_props.total_memory / 1e9,
                    "compute_capability": f"{gpu_props.major}.{gpu_props.minor}"
                }
            
            self.logger.info(f"âœ… CUDAåˆ©ç”¨å¯èƒ½ - {env_info['gpu_count']}GPU")
            for i in range(env_info["gpu_count"]):
                gpu_info = env_info[f"gpu_{i}"]
                self.logger.info(f"   GPU{i}: {gpu_info['name']} ({gpu_info['memory']:.1f}GB)")
        else:
            self.logger.error("âŒ CUDAãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
            
        # MS-Swiftç¢ºèª
        try:
            import swift
            env_info["swift_version"] = swift.__version__
            self.logger.info(f"âœ… MS-Swift: {swift.__version__}")
        except ImportError:
            self.logger.error("âŒ MS-SwiftãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“")
            env_info["swift_version"] = None
        
        return env_info
    
    def create_cpt_jsonl(self):
        """CPTãƒ‡ãƒ¼ã‚¿æº–å‚™ï¼ˆæ”¹è‰¯ç‰ˆï¼‰"""
        self.logger.info("ğŸ“Š CPTãƒ‡ãƒ¼ã‚¿æº–å‚™é–‹å§‹")
        
        output_path = os.path.join(self.data_config.output_dir, self.data_config.output_filename)
        
        try:
            self.logger.info(f"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ '{self.data_config.dataset_name}' ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...")
            ds = load_dataset(self.data_config.dataset_name, split="train")
            self.logger.info(f"ãƒ­ãƒ¼ãƒ‰å®Œäº†ã€‚ç·ã‚¨ãƒ³ãƒˆãƒªæ•°: {len(ds):,}")
            
            written = 0
            skipped = 0
            
            with open(output_path, "w", encoding="utf-8") as f_out:
                for i, row in enumerate(ds):
                    if i % 10000 == 0 and i > 0:
                        self.logger.info(f"å‡¦ç†é€²æ—: {i:,}/{len(ds):,} ({i/len(ds)*100:.1f}%)")
                    
                    text = row.get(self.data_config.text_column, "")
                    if not text or not text.strip():
                        skipped += 1
                        continue
                    
                    # é•·ã™ãã‚‹ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                    if len(text) > 100000:  # 10ä¸‡æ–‡å­—ä»¥ä¸Šã¯é™¤å¤–
                        skipped += 1
                        continue
                    
                    if self.data_config.data_format == "cpt":
                        # Continuous Pre-Trainingå½¢å¼
                        formatted = {
                            "messages": [
                                {"role": "assistant", "content": text.strip()}
                            ]
                        }
                    else:
                        # Chatå½¢å¼
                        formatted = {
                            "messages": [
                                {"role": "user", "content": "ä»¥ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’è¦ç´„ã—ã¦ãã ã•ã„ã€‚"},
                                {"role": "assistant", "content": text.strip()}
                            ]
                        }
                    
                    f_out.write(json.dumps(formatted, ensure_ascii=False) + "\n")
                    written += 1
            
            self.logger.info(f"âœ… ãƒ‡ãƒ¼ã‚¿æº–å‚™å®Œäº†")
            self.logger.info(f"   æ›¸ãå‡ºã—: {written:,}ä»¶")
            self.logger.info(f"   ã‚¹ã‚­ãƒƒãƒ—: {skipped:,}ä»¶")
            self.logger.info(f"   å‡ºåŠ›: {output_path}")
            
        except Exception as e:
            self.logger.error(f"âŒ ãƒ‡ãƒ¼ã‚¿æº–å‚™ã‚¨ãƒ©ãƒ¼: {e}")
            raise
    
    def convert_to_mcore(self) -> str:
        """HuggingFaceãƒ¢ãƒ‡ãƒ«ã‚’Megatron-Coreå½¢å¼ã«å¤‰æ›"""
        self.logger.info("ğŸ”„ HF â†’ Megatron-Coreå¤‰æ›é–‹å§‹")
        
        model_name = self.get_safe_model_name()
        mcore_dir = f"{model_name}-mcore"
        
        cmd = [
            "swift", "export",
            "--model", self.model_config.model_id,
            "--to_mcore", "true",
            "--torch_dtype", self.model_config.torch_dtype,
            "--output_dir", mcore_dir
        ]
        
        env = {"CUDA_VISIBLE_DEVICES": "0"}
        
        try:
            self.logger.info(f"å®Ÿè¡Œã‚³ãƒãƒ³ãƒ‰: {' '.join(cmd)}")
            result = subprocess.run(cmd, env=env, check=True, capture_output=True, text=True)
            self.logger.info("âœ… Megatron-Coreå¤‰æ›å®Œäº†")
            self.logger.info(f"   å‡ºåŠ›: {mcore_dir}")
            return mcore_dir
        except subprocess.CalledProcessError as e:
            self.logger.error(f"âŒ å¤‰æ›ã‚¨ãƒ©ãƒ¼: {e}")
            self.logger.error(f"stdout: {e.stdout}")
            self.logger.error(f"stderr: {e.stderr}")
            raise
    
    def optimize_training_config(self, env_info: Dict[str, Any]):
        """ç’°å¢ƒã«å¿œã˜ãŸå­¦ç¿’è¨­å®šæœ€é©åŒ–"""
        gpu_count = env_info.get("gpu_count", 1)
        
        # GPUæ•°ã«å¿œã˜ãŸæœ€é©åŒ–
        if gpu_count >= 8:
            self.training_config.tensor_model_parallel_size = 4
            self.training_config.pipeline_model_parallel_size = 2
            self.training_config.micro_batch_size = 4
            self.training_config.global_batch_size = 1024
            self.logger.info("ğŸš€ 8GPUä»¥ä¸Šç”¨è¨­å®šé©ç”¨")
        elif gpu_count >= 4:
            self.training_config.tensor_model_parallel_size = 2
            self.training_config.pipeline_model_parallel_size = 2
            self.training_config.micro_batch_size = 2
            self.training_config.global_batch_size = 512
            self.logger.info("âš¡ 4-7GPUç”¨è¨­å®šé©ç”¨")
        elif gpu_count >= 2:
            self.training_config.tensor_model_parallel_size = 2
            self.training_config.pipeline_model_parallel_size = 1
            self.training_config.micro_batch_size = 1
            self.training_config.global_batch_size = 256
            self.logger.info("âš™ï¸ 2-3GPUç”¨è¨­å®šé©ç”¨")
        else:
            self.training_config.tensor_model_parallel_size = 1
            self.training_config.pipeline_model_parallel_size = 1
            self.training_config.micro_batch_size = 1
            self.training_config.global_batch_size = 128
            self.logger.info("ğŸ’§ 1GPUç”¨è¨­å®šé©ç”¨")
        
        # VRAMç·é‡ã«å¿œã˜ãŸèª¿æ•´
        total_vram = sum(env_info.get(f"gpu_{i}", {}).get("memory", 0) 
                        for i in range(gpu_count))
        
        if total_vram < 48:  # 48GBæœªæº€
            self.data_config.max_length = 16384
            self.training_config.recompute_granularity = "selective"
            self.logger.info("ğŸ’§ VRAMç¯€ç´„è¨­å®šé©ç”¨")
    
    def run_megatron_training(self, mcore_model_dir: str):
        """Megatron-Coreå­¦ç¿’å®Ÿè¡Œ"""
        self.logger.info("ğŸ¯ Megatron-Coreå­¦ç¿’é–‹å§‹")
        
        data_path = os.path.join(self.data_config.output_dir, self.data_config.output_filename)
        checkpoint_dir = f"{self.system_config.checkpoint_dir}/{self.run_name}"
        
        cmd = [
            "megatron", "pt",
            "--load", mcore_model_dir,
            "--dataset", data_path,
            
            # åˆ†æ•£è¨­å®š
            "--tensor_model_parallel_size", str(self.training_config.tensor_model_parallel_size),
            "--pipeline_model_parallel_size", str(self.training_config.pipeline_model_parallel_size),
            "--context_parallel_size", str(self.training_config.context_parallel_size),
            "--sequence_parallel", str(self.training_config.sequence_parallel).lower(),
            
            # ãƒãƒƒãƒè¨­å®š
            "--micro_batch_size", str(self.training_config.micro_batch_size),
            "--global_batch_size", str(self.training_config.global_batch_size),
            
            # æœ€é©åŒ–è¨­å®š
            "--recompute_granularity", self.training_config.recompute_granularity,
            "--recompute_method", self.training_config.recompute_method,
            "--recompute_num_layers", str(self.training_config.recompute_num_layers),
            
            # å­¦ç¿’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
            "--train_iters", str(self.training_config.train_iters),
            "--finetune", str(self.training_config.finetune).lower(),
            "--cross_entropy_loss_fusion", str(self.training_config.cross_entropy_loss_fusion).lower(),
            "--lr", str(self.training_config.learning_rate),
            "--lr_warmup_iters", str(self.training_config.warmup_iters),
            "--min_lr", str(self.training_config.min_lr),
            
            # ä¿å­˜ãƒ»ãƒ­ã‚°è¨­å®š
            "--save", checkpoint_dir,
            "--save_interval", str(self.training_config.save_interval),
            "--max_length", str(self.data_config.max_length),
            "--truncation_strategy", self.data_config.truncation_strategy,
            "--num_workers", str(self.training_config.num_workers),
            "--no_save_optim", str(self.training_config.no_save_optim).lower(),
            "--no_save_rng", str(self.training_config.no_save_rng).lower(),
            "--dataset_num_proc", str(self.data_config.num_proc),
            "--packing", str(self.data_config.packing).lower(),
            "--streaming", str(self.data_config.streaming).lower(),
            "--use_flash_attn", str(self.model_config.use_flash_attention).lower(),
            
            # ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°
            "--log_interval", str(self.training_config.log_interval),
        ]
        
        # Weights & Biasesè¨­å®š
        if self.system_config.use_wandb:
            cmd.extend([
                "--wandb_project", self.system_config.wandb_project,
                "--wandb_exp_name", self.run_name
            ])
        
        # ç’°å¢ƒå¤‰æ•°è¨­å®š
        env = os.environ.copy()
        env.update({
            "CUDA_DEVICE_MAX_CONNECTIONS": str(self.system_config.cuda_device_max_connections),
            "CUDA_VISIBLE_DEVICES": self.system_config.cuda_visible_devices,
            "NPROC_PER_NODE": str(self.system_config.nproc_per_node)
        })
        
        try:
            self.logger.info(f"å®Ÿè¡Œã‚³ãƒãƒ³ãƒ‰: {' '.join(cmd)}")
            result = subprocess.run(cmd, env=env, check=True, text=True)
            self.logger.info("âœ… Megatron-Coreå­¦ç¿’å®Œäº†")
            return checkpoint_dir
        except subprocess.CalledProcessError as e:
            self.logger.error(f"âŒ å­¦ç¿’ã‚¨ãƒ©ãƒ¼: {e}")
            raise
    
    def convert_to_hf(self, checkpoint_path: str) -> str:
        """Megatron-Coreãƒ¢ãƒ‡ãƒ«ã‚’HuggingFaceå½¢å¼ã«å¤‰æ›"""
        self.logger.info("ğŸ”„ Megatron-Core â†’ HFå¤‰æ›é–‹å§‹")
        
        converted_path = f"{self.system_config.converted_model_dir}/{self.run_name}"
        
        cmd = [
            "swift", "export",
            "--mcore_model", checkpoint_path,
            "--output_dir", converted_path,
            "--to_hf", "true",
            "--torch_dtype", self.model_config.torch_dtype
        ]
        
        env = {"CUDA_VISIBLE_DEVICES": "0"}
        
        try:
            self.logger.info(f"å®Ÿè¡Œã‚³ãƒãƒ³ãƒ‰: {' '.join(cmd)}")
            result = subprocess.run(cmd, env=env, check=True, capture_output=True, text=True)
            self.logger.info("âœ… HuggingFaceå¤‰æ›å®Œäº†")
            self.logger.info(f"   å‡ºåŠ›: {converted_path}")
            return converted_path
        except subprocess.CalledProcessError as e:
            self.logger.error(f"âŒ å¤‰æ›ã‚¨ãƒ©ãƒ¼: {e}")
            self.logger.error(f"stdout: {e.stdout}")
            self.logger.error(f"stderr: {e.stderr}")
            raise
    
    def create_inference_script(self, model_path: str):
        """æ¨è«–ã‚¹ã‚¯ãƒªãƒ—ãƒˆç”Ÿæˆ"""
        inference_script = f'''#!/usr/bin/env python3
# inference_{self.run_name}.py - æ¨è«–ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
from transformers import AutoModelForCausalLM, AutoTokenizer, StoppingCriteria, StoppingCriteriaList
import torch
import readline

model_path = "{model_path}"

print("ğŸ¤– ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ä¸­...")
tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    model_path,
    device_map="auto",
    torch_dtype=torch.{self.model_config.torch_dtype},
    trust_remote_code=True
).eval()

if tokenizer.pad_token_id is None:
    tokenizer.pad_token = tokenizer.eos_token

# åœæ­¢æ¡ä»¶è¨­å®š
im_end_id = tokenizer.convert_tokens_to_ids("<|im_end|>")
class EndOfAssistant(StoppingCriteria):
    def __call__(self, input_ids, scores, **kwargs):
        return input_ids[0, -1] == im_end_id

stop_list = StoppingCriteriaList([EndOfAssistant()])
system_prompt = "You are a helpful assistant."

print("âœ… ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†")

while True:
    q = input("\\n ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ > ").strip()
    if q.lower() in {{"exit", "quit"}}:
        break

    messages = [
        {{"role": "system", "content": system_prompt}},
        {{"role": "user", "content": q}}
    ]
    
    prompt = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )
    
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

    with torch.inference_mode():
        output = model.generate(
            **inputs,
            do_sample=True,
            temperature=0.7,
            top_k=40,
            top_p=0.9,
            max_new_tokens=256,
            repetition_penalty=1.1,
            no_repeat_ngram_size=4,
            pad_token_id=tokenizer.pad_token_id,
            eos_token_id=im_end_id,
            stopping_criteria=stop_list
        )

    gen_tokens = output[0, inputs.input_ids.shape[-1]:]
    reply = tokenizer.decode(gen_tokens, skip_special_tokens=True).split("<|im_end|>")[0].strip()

    print("\\n å¿œç­”:\\n" + reply)
'''
        
        script_path = f"inference_{self.run_name}.py"
        with open(script_path, 'w', encoding='utf-8') as f:
            f.write(inference_script)
        
        os.chmod(script_path, 0o755)  # å®Ÿè¡Œæ¨©é™ä»˜ä¸
        self.logger.info(f"âœ… æ¨è«–ã‚¹ã‚¯ãƒªãƒ—ãƒˆç”Ÿæˆ: {script_path}")
        return script_path
    
    def save_config(self):
        """è¨­å®šä¿å­˜"""
        config_dict = {
            "model": asdict(self.model_config),
            "data": asdict(self.data_config),
            "training": asdict(self.training_config),
            "system": asdict(self.system_config),
            "run_info": {
                "run_id": self.run_id,
                "run_name": self.run_name,
                "timestamp": datetime.now().isoformat()
            }
        }
        
        config_path = f"config_{self.run_name}.yaml"
        with open(config_path, 'w', encoding='utf-8') as f:
            yaml.dump(config_dict, f, default_flow_style=False, allow_unicode=True)
        
        self.logger.info(f"âœ… è¨­å®šä¿å­˜: {config_path}")
    
    def run_full_pipeline(self):
        """ãƒ•ãƒ«ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ"""
        self.logger.info("ğŸš€ ãƒ•ãƒ«ã‚¹ãƒšãƒƒã‚¯ ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³é–‹å§‹")
        
        try:
            # 1. ç’°å¢ƒãƒã‚§ãƒƒã‚¯
            env_info = self.check_environment()
            
            # 2. è¨­å®šæœ€é©åŒ–
            self.optimize_training_config(env_info)
            
            # 3. ãƒ‡ãƒ¼ã‚¿æº–å‚™
            self.create_cpt_jsonl()
            
            # 4. ãƒ¢ãƒ‡ãƒ«å¤‰æ›ï¼ˆHF â†’ Megatron-Coreï¼‰
            mcore_model_dir = self.convert_to_mcore()
            
            # 5. Megatron-Coreå­¦ç¿’
            checkpoint_dir = self.run_megatron_training(mcore_model_dir)
            
            # 6. æœ€æ–°ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆå–å¾—
            checkpoint_files = list(Path(checkpoint_dir).glob("**/pytorch_model.bin"))
            if not checkpoint_files:
                checkpoint_files = list(Path(checkpoint_dir).glob("**/model_optim_rng.pt"))
            
            if checkpoint_files:
                latest_checkpoint = max(checkpoint_files, key=os.path.getctime)
                checkpoint_path = str(latest_checkpoint.parent)
            else:
                raise FileNotFoundError("ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            
            # 7. ãƒ¢ãƒ‡ãƒ«å¤‰æ›ï¼ˆMegatron-Core â†’ HFï¼‰
            converted_model_path = self.convert_to_hf(checkpoint_path)
            
            # 8. æ¨è«–ã‚¹ã‚¯ãƒªãƒ—ãƒˆç”Ÿæˆ
            inference_script = self.create_inference_script(converted_model_path)
            
            # 9. è¨­å®šä¿å­˜
            self.save_config()
            
            # å®Œäº†å ±å‘Š
            self.logger.info("ğŸ‰ ãƒ•ãƒ«ã‚¹ãƒšãƒƒã‚¯ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å®Œäº†ï¼")
            self.logger.info(f"ğŸ“ å¤‰æ›æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«: {converted_model_path}")
            self.logger.info(f"ğŸ”§ æ¨è«–ã‚¹ã‚¯ãƒªãƒ—ãƒˆ: {inference_script}")
            self.logger.info(f"ğŸ“‹ è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«: config_{self.run_name}.yaml")
            
            if self.system_config.use_wandb:
                wandb.finish()
            
            return {
                "model_path": converted_model_path,
                "inference_script": inference_script,
                "run_name": self.run_name
            }
            
        except Exception as e:
            self.logger.error(f"âŒ ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚¨ãƒ©ãƒ¼: {e}")
            if self.system_config.use_wandb:
                wandb.finish(exit_code=1)
            raise

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    parser = argparse.ArgumentParser(description="MS-Swift + Megatron-Core ãƒ•ãƒ«ã‚¹ãƒšãƒƒã‚¯ç‰ˆãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°")
    parser.add_argument("--config", type=str, help="è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ (.json or .yaml)")
    parser.add_argument("--model", type=str, help="ãƒ¢ãƒ‡ãƒ«ID (ä¾‹: Qwen/Qwen3-8B-Base)")
    parser.add_argument("--dataset", type=str, help="ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå")
    parser.add_argument("--no-wandb", action="store_true", help="Weights & Biasesã‚’ç„¡åŠ¹åŒ–")
    
    args = parser.parse_args()
    
    print("ğŸš€ MS-Swift + Megatron-Core ãƒ•ãƒ«ã‚¹ãƒšãƒƒã‚¯ç‰ˆãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°")
    print("=" * 80)
    
    # ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å®Ÿè¡Œ
    tuner = MegatronSwiftFineTuner(args.config)
    
    # CLIå¼•æ•°ã§è¨­å®šä¸Šæ›¸ã
    if args.model:
        tuner.model_config.model_id = args.model
    if args.dataset:
        tuner.data_config.dataset_name = args.dataset
    if args.no_wandb:
        tuner.system_config.use_wandb = False
    
    result = tuner.run_full_pipeline()
    
    print("\\nğŸ‰ ã™ã¹ã¦å®Œäº†ï¼")
    print("\\næ¬¡ã®æ‰‹é †:")
    print(f"1. æ¨è«–ãƒ†ã‚¹ãƒˆ: python3 {result['inference_script']}")
    print("2. RAGã‚·ã‚¹ãƒ†ãƒ ã«çµ±åˆ")
    print("3. æ€§èƒ½è©•ä¾¡ãƒ»ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ")

if __name__ == "__main__":
    main()