# RAGapp_gemeni.py (Gemini API çµ±åˆ + é«˜åº¦ãª UI å®Œæˆç‰ˆ)

import streamlit as st
import pandas as pd
import os
import datetime
import sys
import logging
import pprint
import json # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ç”¨
import torch # GPUæƒ…å ±è¡¨ç¤ºç”¨
from typing import Optional, Tuple, List, Dict, Any # å‹ãƒ’ãƒ³ãƒˆç”¨ã«è¿½åŠ 

# --- ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ ---
try:
    # config_gemeni ã‹ã‚‰ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
    from config_gemeni import Config, setup_logging
    # rag_query_utils_gemeni ã‹ã‚‰ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
    from rag_query_utils_gemeni import initialize_pipeline, ask_question_ensemble_stream
    # utils_gemeni ã‹ã‚‰ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
    from utils_gemeni import format_document_snippet, normalize_str, preprocess_query
    # calculate_semantic_similarity ã¯ãƒ€ãƒŸãƒ¼ãªã®ã§ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆ
    # from utils_gemeni import calculate_semantic_similarity
except ImportError as e:
    print(f"Import Error in RAGapp_gemeni.py: {e}", file=sys.stderr)
    try: st.error(f"Import Error: {e}\nå¿…è¦ãªãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«(config_gemeni, rag_query_utils_gemeni, utils_gemeni)ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚"); st.stop()
    except Exception: sys.exit(f"Import Error: {e}")

# --- ã‚°ãƒ­ãƒ¼ãƒãƒ«è¨­å®š ---
try:
    config = Config()
    # ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«åã‚’å¤‰æ›´
    logger = setup_logging(config, log_filename="streamlit_app_gemeni.log")
    EVALUATION_FILE = "evaluation_log_gemeni.csv" # è©•ä¾¡ãƒ•ã‚¡ã‚¤ãƒ«åã‚‚å¤‰æ›´
except Exception as global_e:
    print(f"Global Setup Error: {global_e}", file=sys.stderr)
    try: st.error(f"Global Setup Error: {global_e}"); st.stop()
    except Exception: sys.exit(f"Global Setup Error: {global_e}")

# --- ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åˆæœŸåŒ– (ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä»˜ã) ---
@st.cache_resource
def load_pipeline_cached(lora_adapter_path: Optional[str] = None) -> tuple:
    """ä¸­é–“LLMã‚’å«ã‚€ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’ãƒ­ãƒ¼ãƒ‰ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ä»˜ãï¼‰"""
    logger.info(f"Attempting to initialize pipeline with LoRA: {lora_adapter_path}")
    try:
        # initialize_pipeline ã¯ rag_query_utils_gemeni ã‹ã‚‰ã‚¤ãƒ³ãƒãƒ¼ãƒˆã•ã‚ŒãŸã‚‚ã®
        pipeline_components = initialize_pipeline(config, logger, lora_adapter_path=lora_adapter_path)
        if not all(comp is not None for comp in pipeline_components): # ã„ãšã‚Œã‹ãŒNoneãªã‚‰å¤±æ•—
            logger.error("Pipeline initialization failed within load_pipeline_cached.")
            return (None,) * 4 # Noneã‚’4ã¤æŒã¤ã‚¿ãƒ—ãƒ«ã‚’è¿”ã™
        logger.info("Pipeline components initialized successfully.")
        return pipeline_components # (vectordb, intermediate_llm, tokenizer, embedding) ã®ã‚¿ãƒ—ãƒ«
    except Exception as e:
        logger.critical(f"Fatal error during pipeline initialization: {e}", exc_info=True)
        return (None,) * 4

# --- è©•ä¾¡è¨˜éŒ²é–¢æ•° ---
def record_evaluation(filename, timestamp, query, final_answer, source_docs, answer_evaluation, basis_evaluation, comment):
    filepath = os.path.join(os.path.dirname(__file__), filename)
    file_exists = os.path.isfile(filepath); fieldnames = ['timestamp', 'query', 'answer', 'source_docs_summary', 'answer_evaluation', 'basis_evaluation', 'comment']
    try:
        source_summary = "N/A"
        if source_docs: unique_sources = list(set(doc.metadata.get('source', 'N/A') for doc in source_docs)); source_summary = ", ".join(unique_sources)[:200]
        new_eval = pd.DataFrame([{'timestamp': timestamp, 'query': query, 'answer': final_answer, 'source_docs_summary': source_summary,
                                  'answer_evaluation': answer_evaluation, 'basis_evaluation': basis_evaluation, 'comment': comment}])
        new_eval.to_csv(filepath, mode='a', header=not file_exists, index=False, encoding='utf-8-sig', lineterminator='\n', columns=fieldnames)
        logger.info(f"Evaluation recorded to {filename}"); return True
    except Exception as e: logger.error(f"Failed record evaluation: {e}", exc_info=True); return False

# --- ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ€ãƒ¼ã‚¯ãƒ†ãƒ¼ãƒCSS (å¤‰æ›´ãªã—) ---
DARK_THEME_CSS = """
<style>
/* --- åŸºæœ¬ã‚¹ã‚¿ã‚¤ãƒ« --- */
body { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif, "Apple Color Emoji", "Segoe UI Emoji"; background-color: #0e1117; color: #fafafa; line-height: 1.6; }
.stApp { background-color: #0e1117; }
.main .block-container { max-width: 850px; margin: auto; padding: 1.5rem 2rem 3rem 2rem; }
/* --- è¦‹å‡ºã— --- */
h1 { color: #fafafa; font-size: 2rem; text-align: center; margin-bottom: 1.5rem; font-weight: 600;}
h3 { color: #3b7cff; font-size: 1.3rem; margin-top: 1.5rem; margin-bottom: 0.8rem; border-bottom: 1px solid #333; padding-bottom: 0.3rem;}
h4 { color: #ccc; font-size: 1.1rem; margin-top: 1.2rem; margin-bottom: 0.5rem; }
/* --- ãƒœã‚¿ãƒ³ --- */
div[data-testid="stButton"] > button { background-color: #3b7cff; color: white; padding: 0.5rem 1.2rem; border: none; border-radius: 5px; font-weight: 500; transition: background-color 0.2s ease-in-out, transform 0.1s ease-in-out; cursor: pointer;}
div[data-testid="stButton"] > button:hover { background-color: #5c9bff; transform: translateY(-1px); }
div[data-testid="stButton"] > button:active { background-color: #2a5db0; transform: translateY(0px); }
/* --- ãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ› --- */
div[data-testid="stTextArea"] textarea, div[data-testid="stChatInput"] textarea { border: 1px solid #555; background-color: #262730; color: #fafafa; padding: 0.6rem; border-radius: 5px; font-size: 1rem; width: 100%; box-sizing: border-box; }
div[data-testid="stTextArea"] textarea:focus, div[data-testid="stChatInput"] textarea:focus { border-color: #3b7cff; box-shadow: 0 0 0 2px rgba(59, 124, 255, 0.3); outline: none; }
/* --- æƒ…å ±è¡¨ç¤ºãƒœãƒƒã‚¯ã‚¹ --- */
div[data-testid="stInfo"], div[data-testid="stSuccess"], div[data-testid="stWarning"], div[data-testid="stError"] { border-radius: 5px; padding: 1rem 1.2rem; border: none; background-color: #262730; box-shadow: 0 1px 3px rgba(0,0,0,0.2); margin-bottom: 1rem; color: #fafafa; }
div[data-testid="stInfo"] { border-left: 5px solid #3b7cff; }
div[data-testid="stSuccess"] { border-left: 5px solid #3dd56d; }
div[data-testid="stWarning"] { border-left: 5px solid #ffc107; }
div[data-testid="stError"] { border-left: 5px solid #dc3545; }
/* --- ãƒãƒ£ãƒƒãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ --- */
div[data-testid="stChatMessage"] { background-color: #262730; border: 1px solid #333; border-radius: 8px; margin-bottom: 1rem; padding: 1rem 1.2rem; }
/* --- Expander --- */
div[data-testid="stExpander"] { border: 1px solid #333; border-radius: 5px; background-color: #1c1e24; margin-top: 2rem; box-shadow: 0 1px 3px rgba(0,0,0,0.2); }
div[data-testid="stExpander"] summary { font-weight: 500; color: #eee; font-size: 1rem; padding: 0.8rem 1rem; cursor: pointer; }
div[data-testid="stExpander"] summary:hover { background-color: #262730; }
div[data-testid="stExpanderDetails"] { padding: 0.5rem 1.5rem 1.5rem 1.5rem; border-top: 1px solid #333; background-color: #262730; }
/* --- ã‚³ãƒ¼ãƒ‰è¡¨ç¤º --- */
div[data-testid="stCodeBlock"] > pre { background-color: #0e1117 !important; border: 1px solid #333 !important; border-radius: 4px !important; padding: 0.8rem !important; color: #eee !important; font-family: 'Courier New', Courier, monospace !important; }
/* --- ãƒ©ã‚¸ã‚ªãƒœã‚¿ãƒ³ --- */
div[data-testid="stRadio"] label { color: #ccc; margin-right: 0.8rem; }
/* --- ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ  --- */
div[data-testid="stDataFrame"] { border: 1px solid #333; border-radius: 4px; }
</style>
"""

# --- Streamlit ã‚¢ãƒ—ãƒªã®ãƒ¡ã‚¤ãƒ³å‡¦ç† ---
def main():
    # ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«ã‚’å¤‰æ›´
    st.set_page_config(page_title="RAG Evaluation (Gemini)", layout="centered", initial_sidebar_state="collapsed")
    st.markdown(DARK_THEME_CSS, unsafe_allow_html=True)
    st.title("ä½•ã¨ãªãã§ç­”ãˆã‚‹å› (Geminiç‰ˆ)") # ã‚¿ã‚¤ãƒˆãƒ«å¤‰æ›´

    # --- LoRA ãƒ‘ã‚¹ã‚’ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã§ç®¡ç† ---
    if "lora_adapter_path" not in st.session_state:
        st.session_state.lora_adapter_path = config.lora_adapter_path

    # --- ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åˆæœŸåŒ–ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’åˆ©ç”¨ï¼‰---
    with st.spinner("ã‚·ã‚¹ãƒ†ãƒ ã‚’åˆæœŸåŒ–ä¸­... (åˆå›ã¾ãŸã¯LoRAå¤‰æ›´æ™‚)"):
        pipeline_components = load_pipeline_cached(lora_adapter_path=st.session_state.lora_adapter_path)

    if pipeline_components[0] is None: # vectordb ãŒ None ã‹ãƒã‚§ãƒƒã‚¯
        st.error("ã‚·ã‚¹ãƒ†ãƒ ã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ãƒ­ã‚°ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        st.stop()
    vectordb, intermediate_llm, tokenizer, embedding_function = pipeline_components
    logger.info("Pipeline components ready.")

    # --- ãã®ä»–ã®UIç”¨ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹åˆæœŸåŒ– ---
    ui_state_defaults = {
        "query_history": [], "current_query": "", "current_answer_stream": None,
        "current_source_docs": [], "evaluation_recorded_for_last_answer": False,
        "last_full_answer": "", "metadata_filter_str": '{}', "variant_answers": [],
        "variant_settings": [{"k": config.rag_variant_k[i] if i < len(config.rag_variant_k) else 3,
                              # ä¸­é–“LLMãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯Gemini APIã§ã¯ç›´æ¥ä½¿ã‚ãªã„ãŒUIè¦ç´ ã¨ã—ã¦æ®‹ã™
                              "temperature": config.temperature,
                              "top_p": config.top_p,
                              "repetition_penalty": config.repetition_penalty}
                             for i in range(getattr(config, 'num_default_variants', 3))] # configã‹ã‚‰ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆVariantæ•°ã‚’å–å¾—
    }
    for key, default_value in ui_state_defaults.items():
        if key not in st.session_state: st.session_state[key] = default_value

    # --- ã‚µã‚¤ãƒ‰ãƒãƒ¼ ---
    with st.sidebar:
        st.header("âš™ï¸ è¨­å®š")
        st.markdown("##### LoRA è¨­å®š (ä¸­é–“LLMç”¨)") # æ³¨é‡ˆè¿½åŠ 
        lora_path_input = st.text_input("LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ‘ã‚¹", value=st.session_state.lora_adapter_path or "", help="ç©ºæ¬„ã§LoRAç„¡åŠ¹ã€‚å¤‰æ›´å¾Œã¯[å†åˆæœŸåŒ–]å®Ÿè¡Œã€‚")
        if st.button("å†åˆæœŸåŒ– (LoRAé©ç”¨/è§£é™¤)"):
            new_path = lora_path_input.strip() if lora_path_input else None
            if new_path != st.session_state.lora_adapter_path:
                st.session_state.lora_adapter_path = new_path
                load_pipeline_cached.clear()
                st.toast("ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’å†åˆæœŸåŒ–ã—ã¾ã™...")
                st.rerun()
            else:
                st.toast("LoRAãƒ‘ã‚¹ã«å¤‰æ›´ãŒãªã„ãŸã‚ã€å†åˆæœŸåŒ–ã¯ã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã¾ã—ãŸã€‚")

        st.divider()
        st.markdown("##### ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«è¨­å®š")
        st.caption("å„Variantã®æ¤œç´¢ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ (k) ã‚’èª¿æ•´") # èª¬æ˜å¤‰æ›´
        for i in range(len(st.session_state.variant_settings)):
            with st.expander(f"Variant {i+1} è¨­å®š", expanded=(i==0)):
                settings = st.session_state.variant_settings[i]
                settings["k"] = st.number_input(f"æ¤œç´¢æ•° k (V{i+1})", 1, 20, int(settings.get("k", 3)), key=f"k_{i}")
                # ä¸­é–“ç”Ÿæˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ (temperature, top_p, rep_pen) ã¯è¡¨ç¤ºã®ã¿ã€ã¾ãŸã¯å‰Šé™¤ã—ã¦ã‚‚ã‚ˆã„
                st.caption(f"(ä¸­é–“LLM Temp: {settings.get('temperature', config.temperature):.2f}, Top_p: {settings.get('top_p', config.top_p):.2f}, RepPen: {settings.get('repetition_penalty', config.repetition_penalty):.2f})")

        st.divider()
        st.markdown("##### æ¤œç´¢ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼")
        filter_input = st.text_area("ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ (JSON)", st.session_state.metadata_filter_str, height=100, help='ä¾‹: {"type": "ä»•æ§˜"}')
        st.session_state.metadata_filter_str = filter_input

        st.divider()
        if st.button("è¡¨ç¤ºã‚¯ãƒªã‚¢"):
            keys_to_clear = ["query_history", "current_query", "current_answer_stream", "current_source_docs", "evaluation_recorded_for_last_answer", "last_full_answer", "variant_answers"]
            for key in keys_to_clear:
                if key in st.session_state: del st.session_state[key]
            st.toast("è¡¨ç¤ºã‚’ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸã€‚"); st.rerun()

        st.divider(); st.markdown("##### ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±")
        try:
            if torch.cuda.is_available():
                idx = torch.cuda.current_device(); name = torch.cuda.get_device_name(idx); alloc = torch.cuda.memory_allocated(idx)/1e9; reserved = torch.cuda.memory_reserved(idx)/1e9
                st.caption(f"GPU: {name}\nMem: A {alloc:.2f} / R {reserved:.2f} GB")
            else: st.caption("Mode: CPU")
        except Exception as gpu_e: st.warning(f"GPUæƒ…å ±å–å¾—å¤±æ•—: {gpu_e}")

    # --- ä¼šè©±å±¥æ­´ã®è¡¨ç¤º ---
    st.markdown("### ä¼šè©±å±¥æ­´")
    if not st.session_state.query_history: st.info("ã¾ã ä¼šè©±ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")
    else:
        for message in st.session_state.query_history:
            with st.chat_message(message["role"]): st.markdown(message["content"])

    # --- é€²è¡Œä¸­ã®å›ç­”è¡¨ç¤ºã‚¨ãƒªã‚¢ ---
    streaming_placeholder = st.empty()

    # --- è³ªå•å…¥åŠ› (`st.chat_input`) ---
    user_query_input = st.chat_input("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„...")

    # --- æ–°ã—ã„è³ªå•ãŒå…¥åŠ›ã•ã‚ŒãŸå ´åˆã®å‡¦ç† ---
    if user_query_input:
        # utils_gemeni ã‹ã‚‰ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ãŸé–¢æ•°ã‚’ä½¿ç”¨
        st.session_state.current_query = preprocess_query(user_query_input)
        st.session_state.evaluation_recorded_for_last_answer = False
        st.session_state.last_full_answer = ""
        st.session_state.variant_answers = []
        st.session_state.current_source_docs = []
        st.session_state.current_answer_stream = None

        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã®ãƒ‘ãƒ¼ã‚¹
        parsed_metadata_filter = None
        try:
            filter_str = st.session_state.metadata_filter_str.strip()
            if filter_str and filter_str != '{}': parsed_metadata_filter = json.loads(filter_str)
            if parsed_metadata_filter and not isinstance(parsed_metadata_filter, dict): st.warning("ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã¯JSONè¾æ›¸å½¢å¼ã§ã€‚", icon="âš ï¸"); parsed_metadata_filter = None
            elif parsed_metadata_filter: logger.info(f"Applying metadata filter: {parsed_metadata_filter}")
            else: logger.info("No metadata filter applied.")
        except json.JSONDecodeError: st.warning("ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼JSONå½¢å¼ä¸æ­£ã€‚", icon="âš ï¸"); parsed_metadata_filter = None

        # å›ç­”ç”Ÿæˆãƒ—ãƒ­ã‚»ã‚¹é–‹å§‹
        try:
            logger.info(f"Calling ask_question_ensemble_stream with query: {st.session_state.current_query}")
            # rag_query_utils_gemeni ã‹ã‚‰ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ãŸé–¢æ•°ã‚’ä½¿ç”¨
            response = ask_question_ensemble_stream(
                vectordb=vectordb,
                intermediate_llm=intermediate_llm, # å¼•æ•°ã¨ã—ã¦ã¯æ¸¡ã™ãŒå†…éƒ¨ã§ã¯ä½¿ã‚ã‚Œãªã„æƒ³å®š
                tokenizer=tokenizer,             # å¼•æ•°ã¨ã—ã¦ã¯æ¸¡ã™ãŒå†…éƒ¨ã§ã¯ä½¿ã‚ã‚Œãªã„æƒ³å®š
                embedding_function=embedding_function,
                config=config,
                query=st.session_state.current_query,
                logger=logger,
                metadata_filter=parsed_metadata_filter,
                variant_params=st.session_state.variant_settings # ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§è¨­å®šã•ã‚ŒãŸ k å€¤ãªã©
            )
            st.session_state.current_answer_stream = response.get("result_stream")
            st.session_state.current_source_docs = response.get("source_documents", [])
            st.session_state.variant_answers = response.get("variant_answers", [])
            logger.info("Response object received from ask_question_ensemble_stream.")
            # ãƒ¦ãƒ¼ã‚¶ãƒ¼è³ªå•ã‚’å±¥æ­´ã«è¿½åŠ 
            st.session_state.query_history.append({"role": "user", "content": st.session_state.current_query})

        except Exception as e:
            logger.error(f"Error during ask_question_ensemble_stream call: {e}", exc_info=True)
            st.error(f"è³ªå•å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            st.session_state.current_answer_stream = iter([f"ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {e}"])
            st.session_state.current_source_docs = []
            st.session_state.variant_answers = []
            # ãƒ¦ãƒ¼ã‚¶ãƒ¼è³ªå•å±¥æ­´è¿½åŠ 
            if not st.session_state.query_history or st.session_state.query_history[-1]['content'] != st.session_state.current_query:
                 st.session_state.query_history.append({"role": "user", "content": st.session_state.current_query})

        # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°è¡¨ç¤ºå‡¦ç†ã®ãŸã‚ã«å†å®Ÿè¡Œ
        st.rerun()

    # --- å›ç­”ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°è¡¨ç¤º ---
    if st.session_state.current_answer_stream:
        with streaming_placeholder.container():
             with st.chat_message("assistant"):
                 answer_placeholder = st.empty()
                 full_response = ""
                 logger.info("Starting answer streaming...")
                 try:
                     for chunk in st.session_state.current_answer_stream:
                         full_response += chunk
                         answer_placeholder.markdown(full_response + "â–Œ")
                     answer_placeholder.markdown(full_response)
                     st.session_state.last_full_answer = full_response
                     # å±¥æ­´è¿½åŠ  (é‡è¤‡ã‚„ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ä¸Šæ›¸ãã‚’è€ƒæ…®)
                     if not st.session_state.query_history or st.session_state.query_history[-1]['role'] == 'user':
                          st.session_state.query_history.append({"role": "assistant", "content": full_response})
                     elif st.session_state.query_history[-1]['role'] == 'assistant' and "ã‚¨ãƒ©ãƒ¼" in st.session_state.query_history[-1]['content']:
                          st.session_state.query_history[-1]['content'] = full_response # ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ä¸Šæ›¸ã
                     elif st.session_state.query_history[-1]['content'] != full_response: # ç›´å‰ã®å›ç­”ã¨ç•°ãªã‚‹å ´åˆã®ã¿è¿½åŠ ï¼ˆãƒªãƒ©ãƒ³å¯¾ç­–ï¼‰
                           st.session_state.query_history.append({"role": "assistant", "content": full_response})

                     logger.info("Streaming finished.")

                 except Exception as stream_e:
                     logger.error(f"Error during streaming display: {stream_e}", exc_info=True)
                     st.error(f"å›ç­”è¡¨ç¤ºä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {stream_e}")
                     st.session_state.last_full_answer = f"è¡¨ç¤ºã‚¨ãƒ©ãƒ¼: {stream_e}"
                     error_message = st.session_state.last_full_answer
                     # ã‚¨ãƒ©ãƒ¼å±¥æ­´è¿½åŠ  (é‡è¤‡è€ƒæ…®)
                     if not st.session_state.query_history or st.session_state.query_history[-1]['role'] == 'user':
                         st.session_state.query_history.append({"role": "assistant", "content": error_message})
                     elif st.session_state.query_history[-1]['content'] != error_message: # ç›´å‰ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¨ç•°ãªã‚‹å ´åˆã®ã¿
                          st.session_state.query_history[-1]['content'] = error_message # ç›´å‰ã®assistantãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ã‚¨ãƒ©ãƒ¼ã§ä¸Šæ›¸ã

                 finally:
                     st.session_state.current_answer_stream = None
                     st.rerun() # è©³ç´°è¡¨ç¤ºãªã©ã‚’æ›´æ–°ã™ã‚‹ãŸã‚ã«ãƒªãƒ©ãƒ³

    # --- è©³ç´°æƒ…å ±ã¨variantæ¯”è¼ƒãƒ»è©•ä¾¡ ---
    if st.session_state.last_full_answer:
        with st.expander("è©³ç´°æƒ…å ± (Variantæ¯”è¼ƒãƒ»å‚ç…§ãƒ‡ãƒ¼ã‚¿ãƒ»è©•ä¾¡)"):

            # variantã”ã¨ã®å›ç­”æ¯”è¼ƒ
            if st.session_state.variant_answers:
                st.markdown("#### Variant ã”ã¨ã®ä¸­é–“å›ç­” (Geminiã«ã‚ˆã‚‹)") # èª¬æ˜å¤‰æ›´
                for idx, v_ans in enumerate(st.session_state.variant_answers):
                    params = st.session_state.variant_settings[idx]
                    st.markdown(f"**Variant {idx+1}** (k={params['k']})") # kã®ã¿è¡¨ç¤º
                    st.text_area(f"V{idx+1}_Answer", v_ans, height=100, disabled=True, label_visibility="collapsed", key=f"v_ans_{idx}")
                st.divider()

            # å–å¾—ã—ãŸã‚½ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã®è¡¨ç¤º
            st.markdown("#### å‚ç…§ã•ã‚ŒãŸå¯èƒ½æ€§ã®ã‚ã‚‹ãƒ‡ãƒ¼ã‚¿")
            if st.session_state.current_source_docs:
                st.caption(f"é–¢é€£ãƒ‡ãƒ¼ã‚¿ãƒãƒ£ãƒ³ã‚¯ (ãƒ¦ãƒ‹ãƒ¼ã‚¯): {len(st.session_state.current_source_docs)} ä»¶")
                for i, doc in enumerate(st.session_state.current_source_docs):
                    meta_display = []; cols=config.metadata_display_columns; score=doc.metadata.get('rerank_score') # ãƒªãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚¹ã‚³ã‚¢ã‚‚è¡¨ç¤ºè©¦è¡Œ
                    for col_name in cols:
                        if v:=doc.metadata.get(col_name): meta_display.append(f"**{col_name[:4]}**: `{str(v)[:20]}`")
                    if score: meta_display.append(f"**Score**: `{score:.3f}`")
                    st.markdown(f"**CHUNK {i+1}** ({' | '.join(meta_display)})")
                    # utils_gemeni ã‹ã‚‰ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ãŸé–¢æ•°ã‚’ä½¿ç”¨
                    st.text_area(f"Content_{i+1}", doc.page_content, height=100, disabled=True, label_visibility="collapsed", key=f"src_content_{i}")
            else: st.info("å‚ç…§ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")
            st.divider()

            # è©•ä¾¡å…¥åŠ›ã‚»ã‚¯ã‚·ãƒ§ãƒ³
            st.markdown("#### ã“ã®æœ€çµ‚å›ç­”ã‚’è©•ä¾¡")
            if not st.session_state.evaluation_recorded_for_last_answer:
                 # è©•ä¾¡ãƒ•ã‚©ãƒ¼ãƒ ã®ã‚­ãƒ¼ã‚’ä¸€æ„ã«ã™ã‚‹
                 eval_key_suffix = f"_{len(st.session_state.query_history)}"
                 with st.form(f"evaluation_form_{eval_key_suffix}"):
                     eval_cols = st.columns(2)
                     with eval_cols[0]: ans_opts = ["æœªé¸æŠ", "âœ… OK", "âŒ NG", "ğŸ¤” éƒ¨åˆ†çš„"]; ans_eval = st.radio("å›ç­”?", ans_opts, key=f"eval_ans{eval_key_suffix}", horizontal=True, label_visibility="collapsed")
                     with eval_cols[1]: basis_opts = ["æœªé¸æŠ", "âœ… é©åˆ‡", "âŒ ä¸é©åˆ‡", "ğŸ¤· ä¸æ˜"]; bas_eval = st.radio("æ ¹æ‹ ?", basis_opts, key=f"eval_bas{eval_key_suffix}", horizontal=True, label_visibility="collapsed")
                     cmt = st.text_area("ã‚³ãƒ¡ãƒ³ãƒˆ", key=f"eval_com{eval_key_suffix}", height=80, placeholder="ã‚³ãƒ¡ãƒ³ãƒˆ...")
                     submitted = st.form_submit_button("è©•ä¾¡è¨˜éŒ²")
                     if submitted:
                         # ç›´å‰ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼è³ªå•ã‚’å–å¾—
                         user_query_index = -1
                         for j in range(len(st.session_state.query_history) -1, -1, -1):
                             if st.session_state.query_history[j]['role'] == 'user':
                                 user_query_index = j
                                 break
                         qry = st.session_state.query_history[user_query_index]['content'] if user_query_index != -1 else "N/A"

                         if ans_eval != "æœªé¸æŠ" and bas_eval != "æœªé¸æŠ":
                             ts=datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
                             ok=record_evaluation(EVALUATION_FILE, ts, qry, st.session_state.last_full_answer, st.session_state.current_source_docs, ans_eval, bas_eval, cmt)
                             if ok: st.toast("âœ”ï¸è¨˜éŒ²å®Œäº†"); st.session_state.evaluation_recorded_for_last_answer = True; st.rerun()
                             else: st.error("è¨˜éŒ²å¤±æ•—")
                         else: st.warning("ä¸¡æ–¹ã®è©•ä¾¡ã‚’é¸æŠ")
            else: st.success("âœ”ï¸ã“ã®å›ç­”ã¯è©•ä¾¡æ¸ˆã¿ã§ã™ã€‚")
            st.divider()

            # è©•ä¾¡ãƒ­ã‚°ã®è¡¨ç¤º
            st.markdown("#### è©•ä¾¡ãƒ­ã‚°ï¼ˆæœ€æ–°10ä»¶ï¼‰")
            eval_file_path = os.path.join(os.path.dirname(__file__), EVALUATION_FILE)
            @st.cache_data
            def load_evaluation_data(fp):
                if os.path.exists(fp):
                    try: return pd.read_csv(fp)
                    except pd.errors.EmptyDataError: return pd.DataFrame()
                    except Exception as e: logger.error(f"Log display error: {e}"); return None
                else: return None
            df_eval = load_evaluation_data(eval_file_path)
            if df_eval is not None:
                if not df_eval.empty:
                    cols = ['timestamp', 'query', 'answer_evaluation', 'basis_evaluation', 'comment']; st.dataframe(df_eval[cols].tail(10).reset_index(drop=True), use_container_width=True)
                    @st.cache_data
                    def get_csv_data(df): return df.to_csv(index=False, encoding='utf-8-sig').encode('utf-8-sig')
                    csv_d = get_csv_data(df_eval); st.download_button(label="å…¨ãƒ­ã‚°DL", data=csv_d, file_name=EVALUATION_FILE, mime='text/csv')
                else: st.info("è©•ä¾¡ãƒ­ã‚°ãªã—")
            else: st.info(f"ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«({EVALUATION_FILE})ãªã—")


# --- ã‚¹ã‚¯ãƒªãƒ—ãƒˆå®Ÿè¡Œ ---
if __name__ == "__main__":
    main()

# --- ã“ã“ã¾ã§ RAGapp_gemeni.py ---