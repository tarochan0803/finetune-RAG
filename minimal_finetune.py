# minimal_finetune.py - æœ€å°æ§‹æˆç‰ˆï¼ˆãƒ¡ãƒ¢ãƒªåŠ¹ç‡æœ€é‡è¦–ï¼‰

import torch
import os
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer
from peft import LoraConfig, get_peft_model, TaskType
from transformers import BitsAndBytesConfig, DataCollatorForLanguageModeling

print("ğŸ”¥ è¶…è»½é‡ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°é–‹å§‹")

# è¶…è»½é‡è¨­å®š
MODEL_NAME = "elyza/ELYZA-japanese-Llama-2-7b-instruct"
OUTPUT_DIR = "./rag_model_light"
SAMPLE_SIZE = 1000  # 1000ã‚µãƒ³ãƒ—ãƒ«ã®ã¿ä½¿ç”¨

# ãƒ‡ãƒ¼ã‚¿æº–å‚™ï¼ˆæœ€å°é™ï¼‰
print("ğŸ“Š ãƒ‡ãƒ¼ã‚¿æº–å‚™...")
dataset = load_dataset(
    "json", 
    data_files={"train": "/home/ncnadmin/my_rag_project/enhanced_training_dataset.jsonl"}, 
    split="train"
)
dataset = dataset.select(range(SAMPLE_SIZE))  # 1000ã‚µãƒ³ãƒ—ãƒ«ã®ã¿

def format_prompt(example):
    text = f"### å…¥åŠ›:\n{example['input']}\n### å¿œç­”:\n{example['output']}<|endoftext|>"
    return {"text": text}

dataset = dataset.map(format_prompt)
dataset = dataset.train_test_split(test_size=0.1)

# ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼
print("ğŸ”¤ ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼...")
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
tokenizer.pad_token = tokenizer.eos_token

def tokenize(examples):
    return tokenizer(examples["text"], truncation=True, padding=False, max_length=256)

tokenized = dataset.map(tokenize, batched=True, remove_columns=["instruction", "input", "output", "text"])

# 4bité‡å­åŒ–ãƒ¢ãƒ‡ãƒ«
print("ğŸ¤– ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿...")
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_quant_type="nf4",
)

model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    quantization_config=bnb_config,
    device_map="auto",
    trust_remote_code=True,
)

# æœ€å°LoRA
lora_config = LoraConfig(
    r=2,  # æœ€å°
    lora_alpha=4,
    target_modules=["q_proj"],  # 1ã¤ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã¿
    lora_dropout=0.1,
    task_type=TaskType.CAUSAL_LM,
)

model = get_peft_model(model, lora_config)
print(f"âœ… å­¦ç¿’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {model.num_parameters(only_trainable=True):,}")

# å­¦ç¿’è¨­å®šï¼ˆæœ€å°ï¼‰
training_args = TrainingArguments(
    output_dir=OUTPUT_DIR,
    num_train_epochs=1,  # 1ã‚¨ãƒãƒƒã‚¯ã®ã¿
    per_device_train_batch_size=1,
    gradient_accumulation_steps=2,
    learning_rate=1e-4,
    fp16=True,
    logging_steps=100,
    save_steps=500,
    eval_steps=500,
    evaluation_strategy="steps",
    save_total_limit=1,
    remove_unused_columns=False,
)

# ãƒ‡ãƒ¼ã‚¿ã‚³ãƒ¬ã‚¯ã‚¿ãƒ¼
data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm=False,
)

# ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized["train"],
    eval_dataset=tokenized["test"],
    data_collator=data_collator,
)

print("ğŸš€ å­¦ç¿’é–‹å§‹...")
trainer.train()

print("ğŸ’¾ ä¿å­˜...")
model.save_pretrained(OUTPUT_DIR)
tokenizer.save_pretrained(OUTPUT_DIR)

print(f"ğŸ‰ å®Œäº†ï¼ä¿å­˜å…ˆ: {OUTPUT_DIR}")
print("è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®lora_adapter_pathã‚’æ›´æ–°ã—ã¦ãã ã•ã„ã€‚")