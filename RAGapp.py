# RAGapp.py (ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ API çµ±åˆ + é«˜åº¦ãª UI å®Œæˆç‰ˆ - session_stateä¿®æ­£)

import streamlit as st
import pandas as pd
import os
import datetime
import sys
import logging
import pprint
import json # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ç”¨
import torch # GPUæƒ…å ±è¡¨ç¤ºç”¨
from typing import Optional, Tuple, List, Dict, Any # å‹ãƒ’ãƒ³ãƒˆç”¨ã«è¿½åŠ 

# --- ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ ---
try:
    from config import Config, setup_logging
    # ask_question_ensemble_stream ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
    from rag_query_utils import initialize_pipeline, ask_question_ensemble_stream
    # utils ã‹ã‚‰å¿…è¦ãªé–¢æ•°ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
    from utils import format_document_snippet, normalize_str, preprocess_query
    # calculate_semantic_similarity ã¯ãƒ€ãƒŸãƒ¼ãªã®ã§ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆ
    # from utils import calculate_semantic_similarity
except ImportError as e:
    print(f"Import Error in RAGapp.py: {e}", file=sys.stderr)
    try: st.error(f"Import Error: {e}\nå¿…è¦ãªãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚"); st.stop()
    except Exception: sys.exit(f"Import Error: {e}")

# --- ã‚°ãƒ­ãƒ¼ãƒãƒ«è¨­å®š ---
try:
    config = Config()
    logger = setup_logging(config, log_filename="streamlit_app_hybrid.log") # ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«åå¤‰æ›´
    EVALUATION_FILE = "evaluation_log.csv"
except Exception as global_e:
    print(f"Global Setup Error: {global_e}", file=sys.stderr)
    try: st.error(f"Global Setup Error: {global_e}"); st.stop()
    except Exception: sys.exit(f"Global Setup Error: {global_e}")

# --- ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åˆæœŸåŒ– (ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä»˜ã) ---
# ä¸­é–“LLM(ELYZA 7B), Tokenizer, VectorDB, Embedding ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥
@st.cache_resource
def load_pipeline_cached(lora_adapter_path: Optional[str] = None) -> tuple:
    """ä¸­é–“LLMã‚’å«ã‚€ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’ãƒ­ãƒ¼ãƒ‰ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ä»˜ãï¼‰"""
    logger.info(f"Attempting to initialize pipeline with LoRA: {lora_adapter_path}")
    try:
        # initialize_pipeline ã¯ vectordb, intermediate_llm, tokenizer, embedding ã‚’è¿”ã™
        pipeline_components = initialize_pipeline(config, logger, lora_adapter_path=lora_adapter_path)
        if not all(comp is not None for comp in pipeline_components): # ã„ãšã‚Œã‹ãŒNoneãªã‚‰å¤±æ•—
            logger.error("Pipeline initialization failed within load_pipeline_cached.")
            # Noneã‚’4ã¤æŒã¤ã‚¿ãƒ—ãƒ«ã‚’è¿”ã™
            return (None,) * 4
        logger.info("Pipeline components initialized successfully.")
        return pipeline_components # (vectordb, intermediate_llm, tokenizer, embedding) ã®ã‚¿ãƒ—ãƒ«
    except Exception as e:
        logger.critical(f"Fatal error during pipeline initialization: {e}", exc_info=True)
        return (None,) * 4

# --- è©•ä¾¡è¨˜éŒ²é–¢æ•° ---
def record_evaluation(filename, timestamp, query, final_answer, source_docs, answer_evaluation, basis_evaluation, comment):
    filepath = os.path.join(os.path.dirname(__file__), filename)
    file_exists = os.path.isfile(filepath); fieldnames = ['timestamp', 'query', 'answer', 'source_docs_summary', 'answer_evaluation', 'basis_evaluation', 'comment']
    try:
        source_summary = "N/A"
        if source_docs: unique_sources = list(set(doc.metadata.get('source', 'N/A') for doc in source_docs)); source_summary = ", ".join(unique_sources)[:200]
        new_eval = pd.DataFrame([{'timestamp': timestamp, 'query': query, 'answer': final_answer, 'source_docs_summary': source_summary,
                                  'answer_evaluation': answer_evaluation, 'basis_evaluation': basis_evaluation, 'comment': comment}])
        new_eval.to_csv(filepath, mode='a', header=not file_exists, index=False, encoding='utf-8-sig', lineterminator='\n', columns=fieldnames)
        logger.info(f"Evaluation recorded to {filename}"); return True
    except Exception as e: logger.error(f"Failed record evaluation: {e}", exc_info=True); return False

# --- ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ€ãƒ¼ã‚¯ãƒ†ãƒ¼ãƒCSS ---
DARK_THEME_CSS = """
<style>
/* --- åŸºæœ¬ã‚¹ã‚¿ã‚¤ãƒ« --- */
body { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif, "Apple Color Emoji", "Segoe UI Emoji"; background-color: #0e1117; color: #fafafa; line-height: 1.6; }
.stApp { background-color: #0e1117; }
.main .block-container { max-width: 850px; margin: auto; padding: 1.5rem 2rem 3rem 2rem; }
/* --- è¦‹å‡ºã— --- */
h1 { color: #fafafa; font-size: 2rem; text-align: center; margin-bottom: 1.5rem; font-weight: 600;}
h3 { color: #3b7cff; font-size: 1.3rem; margin-top: 1.5rem; margin-bottom: 0.8rem; border-bottom: 1px solid #333; padding-bottom: 0.3rem;}
h4 { color: #ccc; font-size: 1.1rem; margin-top: 1.2rem; margin-bottom: 0.5rem; }
/* --- ãƒœã‚¿ãƒ³ --- */
div[data-testid="stButton"] > button { background-color: #3b7cff; color: white; padding: 0.5rem 1.2rem; border: none; border-radius: 5px; font-weight: 500; transition: background-color 0.2s ease-in-out, transform 0.1s ease-in-out; cursor: pointer;}
div[data-testid="stButton"] > button:hover { background-color: #5c9bff; transform: translateY(-1px); }
div[data-testid="stButton"] > button:active { background-color: #2a5db0; transform: translateY(0px); }
/* --- ãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ› --- */
div[data-testid="stTextArea"] textarea, div[data-testid="stChatInput"] textarea { border: 1px solid #555; background-color: #262730; color: #fafafa; padding: 0.6rem; border-radius: 5px; font-size: 1rem; width: 100%; box-sizing: border-box; }
div[data-testid="stTextArea"] textarea:focus, div[data-testid="stChatInput"] textarea:focus { border-color: #3b7cff; box-shadow: 0 0 0 2px rgba(59, 124, 255, 0.3); outline: none; }
/* --- æƒ…å ±è¡¨ç¤ºãƒœãƒƒã‚¯ã‚¹ --- */
div[data-testid="stInfo"], div[data-testid="stSuccess"], div[data-testid="stWarning"], div[data-testid="stError"] { border-radius: 5px; padding: 1rem 1.2rem; border: none; background-color: #262730; box-shadow: 0 1px 3px rgba(0,0,0,0.2); margin-bottom: 1rem; color: #fafafa; }
div[data-testid="stInfo"] { border-left: 5px solid #3b7cff; }
div[data-testid="stSuccess"] { border-left: 5px solid #3dd56d; }
div[data-testid="stWarning"] { border-left: 5px solid #ffc107; }
div[data-testid="stError"] { border-left: 5px solid #dc3545; }
/* --- ãƒãƒ£ãƒƒãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ --- */
div[data-testid="stChatMessage"] { background-color: #262730; border: 1px solid #333; border-radius: 8px; margin-bottom: 1rem; padding: 1rem 1.2rem; }
/* --- Expander --- */
div[data-testid="stExpander"] { border: 1px solid #333; border-radius: 5px; background-color: #1c1e24; margin-top: 2rem; box-shadow: 0 1px 3px rgba(0,0,0,0.2); }
div[data-testid="stExpander"] summary { font-weight: 500; color: #eee; font-size: 1rem; padding: 0.8rem 1rem; cursor: pointer; }
div[data-testid="stExpander"] summary:hover { background-color: #262730; }
div[data-testid="stExpanderDetails"] { padding: 0.5rem 1.5rem 1.5rem 1.5rem; border-top: 1px solid #333; background-color: #262730; }
/* --- ã‚³ãƒ¼ãƒ‰è¡¨ç¤º --- */
div[data-testid="stCodeBlock"] > pre { background-color: #0e1117 !important; border: 1px solid #333 !important; border-radius: 4px !important; padding: 0.8rem !important; color: #eee !important; font-family: 'Courier New', Courier, monospace !important; }
/* --- ãƒ©ã‚¸ã‚ªãƒœã‚¿ãƒ³ --- */
div[data-testid="stRadio"] label { color: #ccc; margin-right: 0.8rem; }
/* --- ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ  --- */
div[data-testid="stDataFrame"] { border: 1px solid #333; border-radius: 4px; }
</style>
"""

# --- Streamlit ã‚¢ãƒ—ãƒªã®ãƒ¡ã‚¤ãƒ³å‡¦ç† ---
def main():
    st.set_page_config(page_title="RAG Evaluation (Hybrid)", layout="centered", initial_sidebar_state="collapsed")
    st.markdown(DARK_THEME_CSS, unsafe_allow_html=True)
    st.title("ä½•ã¨ãªãã§ç­”ãˆã‚‹å›")

    # --- LoRA ãƒ‘ã‚¹ã‚’ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã§ç®¡ç† ---
    # ã‚¢ãƒ—ãƒªèµ·å‹•æ™‚ã« config.py ã‹ã‚‰åˆæœŸå€¤ã‚’èª­ã¿è¾¼ã‚€
    if "lora_adapter_path" not in st.session_state:
        st.session_state.lora_adapter_path = config.lora_adapter_path

    # --- ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åˆæœŸåŒ–ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’åˆ©ç”¨ï¼‰---
    # `load_pipeline_cached` ã‚’å‘¼ã³å‡ºã—ã¦ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’å–å¾—
    # LoRAãƒ‘ã‚¹ãŒå¤‰æ›´ã•ã‚Œã‚‹ã¨ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒã‚¯ãƒªã‚¢ã•ã‚Œå†å®Ÿè¡Œã•ã‚Œã‚‹
    with st.spinner("ã‚·ã‚¹ãƒ†ãƒ ã‚’åˆæœŸåŒ–ä¸­... (åˆå›ã¾ãŸã¯LoRAå¤‰æ›´æ™‚)"):
        pipeline_components = load_pipeline_cached(lora_adapter_path=st.session_state.lora_adapter_path)

    # çµæœã‚’å¤‰æ•°ã«æ ¼ç´ï¼ˆå¤±æ•—æ™‚ã¯ã“ã“ã§åœæ­¢ï¼‰
    if pipeline_components[0] is None: # vectordb ãŒ None ã‹ãƒã‚§ãƒƒã‚¯
        st.error("ã‚·ã‚¹ãƒ†ãƒ ã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ãƒ­ã‚°ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        # ã“ã“ã§ st.stop() ã‚’å‘¼ã¶ã¨ã€ã‚µã‚¤ãƒ‰ãƒãƒ¼ãªã©ã‚‚è¡¨ç¤ºã•ã‚Œãªããªã‚‹ãŸã‚æ³¨æ„
        # å¿…è¦ãªã‚‰ã‚¨ãƒ©ãƒ¼ãƒšãƒ¼ã‚¸ã«ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆã™ã‚‹ãªã©ã®å‡¦ç†ã‚‚æ¤œè¨
        st.stop()
    vectordb, intermediate_llm, tokenizer, embedding_function = pipeline_components
    logger.info("Pipeline components ready.")

    # --- ãã®ä»–ã®UIç”¨ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹åˆæœŸåŒ– ---
    ui_state_defaults = {
        "query_history": [], "current_query": "", "current_answer_stream": None,
        "current_source_docs": [], "evaluation_recorded_for_last_answer": False,
        "last_full_answer": "", "metadata_filter_str": '{}', "variant_answers": [],
        "variant_settings": [{"k": config.rag_variant_k[i] if i < len(config.rag_variant_k) else 3,
                             "temperature": config.temperature, # ä¸­é–“LLMã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚’ä½¿ç”¨
                             "top_p": config.top_p,
                             "repetition_penalty": config.repetition_penalty}
                             for i in range(3)] # 3ã¤ã®Variantã‚’ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ç”¨æ„
    }
    for key, default_value in ui_state_defaults.items():
        if key not in st.session_state: st.session_state[key] = default_value

    # --- ã‚µã‚¤ãƒ‰ãƒãƒ¼ ---
    with st.sidebar:
        st.header("âš™ï¸ è¨­å®š")
        st.markdown("##### LoRA è¨­å®š")
        lora_path_input = st.text_input("LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ‘ã‚¹", value=st.session_state.lora_adapter_path or "", help="ç©ºæ¬„ã§LoRAç„¡åŠ¹ã€‚å¤‰æ›´å¾Œã¯[å†åˆæœŸåŒ–]å®Ÿè¡Œã€‚")
        # å†åˆæœŸåŒ–ãƒœã‚¿ãƒ³ãŒæŠ¼ã•ã‚ŒãŸã‹ã€å…¥åŠ›ãƒ‘ã‚¹ãŒå¤‰ã‚ã£ãŸå ´åˆã«å†åˆæœŸåŒ–ã‚’ãƒˆãƒªã‚¬ãƒ¼
        if st.button("å†åˆæœŸåŒ– (LoRAé©ç”¨/è§£é™¤)"):
            new_path = lora_path_input.strip() if lora_path_input else None
            # ç¾åœ¨ã®ãƒ‘ã‚¹ã¨ç•°ãªã‚‹å ´åˆã®ã¿ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢ã¨ãƒªãƒ©ãƒ³
            if new_path != st.session_state.lora_adapter_path:
                st.session_state.lora_adapter_path = new_path
                load_pipeline_cached.clear()
                st.toast("ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’å†åˆæœŸåŒ–ã—ã¾ã™...")
                st.rerun() # å†å®Ÿè¡Œã—ã¦ load_pipeline_cached ã‚’å†å®Ÿè¡Œã•ã›ã‚‹
            else:
                st.toast("LoRAãƒ‘ã‚¹ã«å¤‰æ›´ãŒãªã„ãŸã‚ã€å†åˆæœŸåŒ–ã¯ã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã¾ã—ãŸã€‚")

        st.divider()
        st.markdown("##### ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«è¨­å®š")
        st.caption("å„Variantã®ä¸­é–“ç”Ÿæˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’èª¿æ•´")
        # Variantã”ã¨ã®è¨­å®š
        for i in range(len(st.session_state.variant_settings)):
            with st.expander(f"Variant {i+1} è¨­å®š", expanded=(i==0)):
                settings = st.session_state.variant_settings[i]
                settings["k"] = st.number_input(f"æ¤œç´¢æ•° k (V{i+1})", 1, 20, int(settings.get("k", 3)), key=f"k_{i}")
                # ä¸­é–“ç”Ÿæˆç”¨ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨ã—ã¦è¨­å®š (APIç”¨ã¨ã¯åˆ¥)
                settings["temperature"] = st.slider(f"Temperature (V{i+1})", 0.0, 1.0, float(settings.get("temperature", config.temperature)), 0.05, key=f"temp_{i}")
                settings["top_p"] = st.slider(f"Top_p (V{i+1})", 0.1, 1.0, float(settings.get("top_p", config.top_p)), 0.05, key=f"top_p_{i}")
                settings["repetition_penalty"] = st.slider(f"Rep Penalty (V{i+1})", 1.0, 2.0, float(settings.get("repetition_penalty", config.repetition_penalty)), 0.05, key=f"rep_pen_{i}")

        st.divider()
        st.markdown("##### æ¤œç´¢ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼")
        filter_input = st.text_area("ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ (JSON)", st.session_state.metadata_filter_str, height=100, help='ä¾‹: {"type": "ä»•æ§˜"}')
        st.session_state.metadata_filter_str = filter_input # å¤‰æ›´ã‚’å³æ™‚åæ˜  (rerunã¯ä¸è¦)

        st.divider()
        if st.button("è¡¨ç¤ºã‚¯ãƒªã‚¢"):
            keys_to_clear = ["query_history", "current_query", "current_answer_stream", "current_source_docs", "evaluation_recorded_for_last_answer", "last_full_answer", "variant_answers"]
            for key in keys_to_clear:
                if key in st.session_state: del st.session_state[key]
            st.toast("è¡¨ç¤ºã‚’ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸã€‚"); st.rerun()

        st.divider(); st.markdown("##### ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±")
        try:
            if torch.cuda.is_available():
                 idx = torch.cuda.current_device(); name = torch.cuda.get_device_name(idx); alloc = torch.cuda.memory_allocated(idx)/1e9; reserved = torch.cuda.memory_reserved(idx)/1e9
                 st.caption(f"GPU: {name}\nMem: A {alloc:.2f} / R {reserved:.2f} GB")
            else: st.caption("Mode: CPU")
        except Exception as gpu_e: st.warning(f"GPUæƒ…å ±å–å¾—å¤±æ•—: {gpu_e}")

    # --- ä¼šè©±å±¥æ­´ã®è¡¨ç¤º ---
    st.markdown("### ä¼šè©±å±¥æ­´")
    if not st.session_state.query_history: st.info("ã¾ã ä¼šè©±ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")
    else:
        for message in st.session_state.query_history:
            with st.chat_message(message["role"]): st.markdown(message["content"])

    # --- é€²è¡Œä¸­ã®å›ç­”è¡¨ç¤ºã‚¨ãƒªã‚¢ ---
    streaming_placeholder = st.empty()

    # --- è³ªå•å…¥åŠ› (`st.chat_input`) ---
    user_query_input = st.chat_input("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„...")

    # --- æ–°ã—ã„è³ªå•ãŒå…¥åŠ›ã•ã‚ŒãŸå ´åˆã®å‡¦ç† ---
    if user_query_input:
        st.session_state.current_query = preprocess_query(user_query_input)
        st.session_state.evaluation_recorded_for_last_answer = False
        st.session_state.last_full_answer = ""
        st.session_state.variant_answers = []
        st.session_state.current_source_docs = []
        st.session_state.current_answer_stream = None # ã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚’ã‚¯ãƒªã‚¢

        # å±¥æ­´ã«è¿½åŠ ã—ã¦è¡¨ç¤º (é‡è¤‡è¡¨ç¤ºã‚’é˜²ããŸã‚ã€ä¸€æ—¦ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆã€‚ã‚¹ãƒˆãƒªãƒ¼ãƒ å®Œäº†å¾Œã«è¿½åŠ )
        # st.session_state.query_history.append({"role": "user", "content": st.session_state.current_query})
        # with st.chat_message("user"): st.markdown(st.session_state.current_query)

        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã®ãƒ‘ãƒ¼ã‚¹
        parsed_metadata_filter = None
        try:
            filter_str = st.session_state.metadata_filter_str.strip()
            if filter_str and filter_str != '{}': parsed_metadata_filter = json.loads(filter_str)
            if parsed_metadata_filter and not isinstance(parsed_metadata_filter, dict): st.warning("ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã¯JSONè¾æ›¸å½¢å¼ã§ã€‚", icon="âš ï¸"); parsed_metadata_filter = None
            elif parsed_metadata_filter: logger.info(f"Applying metadata filter: {parsed_metadata_filter}")
            else: logger.info("No metadata filter applied.")
        except json.JSONDecodeError: st.warning("ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼JSONå½¢å¼ä¸æ­£ã€‚", icon="âš ï¸"); parsed_metadata_filter = None

        # å›ç­”ç”Ÿæˆãƒ—ãƒ­ã‚»ã‚¹é–‹å§‹
        try:
            logger.info(f"Calling ask_question_ensemble_stream with query: {st.session_state.current_query}")
            # <<< CORRECTED: ãƒ­ãƒ¼ã‚«ãƒ«å¤‰æ•°ã‹ã‚‰ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’æ¸¡ã™ >>>
            response = ask_question_ensemble_stream(
                vectordb=vectordb,                 # ãƒ­ãƒ¼ã‚«ãƒ«å¤‰æ•°ã‹ã‚‰
                intermediate_llm=intermediate_llm, # ãƒ­ãƒ¼ã‚«ãƒ«å¤‰æ•°ã‹ã‚‰
                tokenizer=tokenizer,               # ãƒ­ãƒ¼ã‚«ãƒ«å¤‰æ•°ã‹ã‚‰
                embedding_function=embedding_function, # ãƒ­ãƒ¼ã‚«ãƒ«å¤‰æ•°ã‹ã‚‰
                config=config,
                query=st.session_state.current_query,
                logger=logger,
                metadata_filter=parsed_metadata_filter,
                variant_params=st.session_state.variant_settings # ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§è¨­å®šã•ã‚ŒãŸå€¤
            )
            # <<< END CORRECTED >>>
            st.session_state.current_answer_stream = response.get("result_stream")
            st.session_state.current_source_docs = response.get("source_documents", [])
            st.session_state.variant_answers = response.get("variant_answers", [])
            logger.info("Response object received from ask_question_ensemble_stream.")
            # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã‚’å±¥æ­´ã«è¿½åŠ ã™ã‚‹ã®ã¯ã“ã“ã§è¡Œã†
            st.session_state.query_history.append({"role": "user", "content": st.session_state.current_query})

        except Exception as e:
            logger.error(f"Error during ask_question_ensemble_stream call: {e}", exc_info=True)
            st.error(f"è³ªå•å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            st.session_state.current_answer_stream = iter([f"ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {e}"])
            st.session_state.current_source_docs = []
            st.session_state.variant_answers = []
            # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•å±¥æ­´è¿½åŠ ã¯ã“ã“ã§ã‚‚è¡Œã†
            if not st.session_state.query_history or st.session_state.query_history[-1]['content'] != st.session_state.current_query:
                 st.session_state.query_history.append({"role": "user", "content": st.session_state.current_query})

        # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°è¡¨ç¤ºå‡¦ç†ã®ãŸã‚ã«å†å®Ÿè¡Œ
        st.rerun()

    # --- å›ç­”ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°è¡¨ç¤º ---
    if st.session_state.current_answer_stream:
        with streaming_placeholder.container():
             with st.chat_message("assistant"):
                 answer_placeholder = st.empty() # ã“ã“ã«ãƒãƒ£ãƒ³ã‚¯ã‚’è¿½è¨˜ã—ã¦ã„ã
                 full_response = ""
                 logger.info("Starting answer streaming...")
                 try:
                     for chunk in st.session_state.current_answer_stream:
                          full_response += chunk
                          answer_placeholder.markdown(full_response + "â–Œ") # ã‚«ãƒ¼ã‚½ãƒ«é¢¨ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³
                     answer_placeholder.markdown(full_response) # ã‚«ãƒ¼ã‚½ãƒ«ã‚’æ¶ˆã—ã¦æœ€çµ‚çµæœè¡¨ç¤º
                     st.session_state.last_full_answer = full_response # å®Œå…¨ãªå›ç­”ã‚’ä¿å­˜
                     # å±¥æ­´ã¸ã®è¿½åŠ ã¯ã‚¹ãƒˆãƒªãƒ¼ãƒ å®Œäº†å¾Œã«è¡Œã†
                     if not st.session_state.query_history or st.session_state.query_history[-1]['content'] != full_response:
                          # ç›´å‰ãŒãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ã‹ã€ã‚ã‚‹ã„ã¯ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã§ãªã‘ã‚Œã°è¿½åŠ 
                          if st.session_state.query_history[-1]['role'] == 'user' or "ã‚¨ãƒ©ãƒ¼" not in st.session_state.query_history[-1]['content']:
                               st.session_state.query_history.append({"role": "assistant", "content": full_response})
                          else: # ç›´å‰ãŒã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãªã‚‰ä¸Šæ›¸ã
                               st.session_state.query_history[-1]['content'] = full_response
                     logger.info("Streaming finished.")

                 except Exception as stream_e:
                     logger.error(f"Error during streaming display: {stream_e}", exc_info=True)
                     st.error(f"å›ç­”è¡¨ç¤ºä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {stream_e}")
                     st.session_state.last_full_answer = f"è¡¨ç¤ºã‚¨ãƒ©ãƒ¼: {stream_e}"
                     error_message = st.session_state.last_full_answer
                     # ã‚¨ãƒ©ãƒ¼å±¥æ­´è¿½åŠ 
                     if not st.session_state.query_history or st.session_state.query_history[-1].get("content") != error_message:
                         if st.session_state.query_history[-1]['role'] == 'user':
                              st.session_state.query_history.append({"role": "assistant", "content": error_message})
                         else: st.session_state.query_history[-1]['content'] = error_message

                 finally:
                      st.session_state.current_answer_stream = None # ã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚’ã‚¯ãƒªã‚¢
                      # ã“ã“ã§ rerun ã™ã‚‹ã‹ã¯æ¤œè¨ã€‚Expanderã®è¡¨ç¤ºãªã©ã‚’å³æ™‚åæ˜ ã•ã›ã‚‹ãªã‚‰å¿…è¦ã€‚
                      st.rerun()

    # --- è©³ç´°æƒ…å ±ã¨variantæ¯”è¼ƒãƒ»è©•ä¾¡ ---
    # last_full_answer ãŒç¢ºå®šã—ãŸã‚‰è¡¨ç¤º
    if st.session_state.last_full_answer:
        with st.expander("è©³ç´°æƒ…å ± (Variantæ¯”è¼ƒãƒ»å‚ç…§ãƒ‡ãƒ¼ã‚¿ãƒ»è©•ä¾¡)"):

            # variantã”ã¨ã®å›ç­”æ¯”è¼ƒ
            if st.session_state.variant_answers:
                st.markdown("#### Variant ã”ã¨ã®ä¸­é–“å›ç­”")
                for idx, v_ans in enumerate(st.session_state.variant_answers):
                    params = st.session_state.variant_settings[idx]
                    st.markdown(f"**Variant {idx+1}** (k={params['k']}, temp={params['temperature']:.2f}, top_p={params['top_p']:.2f}, rep_pen={params['repetition_penalty']:.2f})")
                    st.text_area(f"V{idx+1}_Answer", v_ans, height=100, disabled=True, label_visibility="collapsed", key=f"v_ans_{idx}")
                st.divider()

            # å–å¾—ã—ãŸã‚½ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã®è¡¨ç¤º
            st.markdown("#### å‚ç…§ã•ã‚ŒãŸå¯èƒ½æ€§ã®ã‚ã‚‹ãƒ‡ãƒ¼ã‚¿")
            if st.session_state.current_source_docs:
                st.caption(f"é–¢é€£ãƒ‡ãƒ¼ã‚¿ãƒãƒ£ãƒ³ã‚¯ (ãƒ¦ãƒ‹ãƒ¼ã‚¯): {len(st.session_state.current_source_docs)} ä»¶")
                for i, doc in enumerate(st.session_state.current_source_docs):
                    meta_display = []; cols=config.metadata_display_columns; score=doc.metadata.get('rerank_score')
                    for col_name in cols:
                        if v:=doc.metadata.get(col_name): meta_display.append(f"**{col_name[:4]}**: `{str(v)[:20]}`") # çŸ­ç¸®è¡¨ç¤º
                    if score: meta_display.append(f"**Score**: `{score:.3f}`")
                    st.markdown(f"**CHUNK {i+1}** ({'|'.join(meta_display)})")
                    st.text_area(f"Content_{i+1}", doc.page_content, height=100, disabled=True, label_visibility="collapsed", key=f"src_content_{i}")
            else: st.info("å‚ç…§ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")
            st.divider()

            # è©•ä¾¡å…¥åŠ›ã‚»ã‚¯ã‚·ãƒ§ãƒ³
            st.markdown("#### ã“ã®æœ€çµ‚å›ç­”ã‚’è©•ä¾¡")
            if not st.session_state.evaluation_recorded_for_last_answer:
                 # è©•ä¾¡ãƒ•ã‚©ãƒ¼ãƒ ã®ã‚­ãƒ¼ã‚’ä¸€æ„ã«ã™ã‚‹ï¼ˆã‚¯ã‚¨ãƒªã”ã¨ã«å¤‰ã‚ã‚‹ã‚ˆã†ã«ï¼‰
                 eval_key_suffix = f"_{len(st.session_state.query_history)}"
                 with st.form(f"evaluation_form_{eval_key_suffix}"):
                      eval_cols = st.columns(2)
                      with eval_cols[0]: ans_opts = ["æœªé¸æŠ", "âœ… OK", "âŒ NG", "ğŸ¤” éƒ¨åˆ†çš„"]; ans_eval = st.radio("å›ç­”?", ans_opts, key=f"eval_ans{eval_key_suffix}", horizontal=True, label_visibility="collapsed")
                      with eval_cols[1]: basis_opts = ["æœªé¸æŠ", "âœ… é©åˆ‡", "âŒ ä¸é©åˆ‡", "ğŸ¤· ä¸æ˜"]; bas_eval = st.radio("æ ¹æ‹ ?", basis_opts, key=f"eval_bas{eval_key_suffix}", horizontal=True, label_visibility="collapsed")
                      cmt = st.text_area("ã‚³ãƒ¡ãƒ³ãƒˆ", key=f"eval_com{eval_key_suffix}", height=80, placeholder="ã‚³ãƒ¡ãƒ³ãƒˆ...")
                      submitted = st.form_submit_button("è©•ä¾¡è¨˜éŒ²")
                      if submitted:
                           if ans_eval != "æœªé¸æŠ" and bas_eval != "æœªé¸æŠ":
                               ts=datetime.datetime.now().strftime("%Y%m%d_%H%M%S"); qry=st.session_state.query_history[-2]['content'] if len(st.session_state.query_history)>=2 else "N/A"
                               ok=record_evaluation(EVALUATION_FILE, ts, qry, st.session_state.last_full_answer, st.session_state.current_source_docs, ans_eval, bas_eval, cmt)
                               if ok: st.toast("âœ”ï¸è¨˜éŒ²å®Œäº†"); st.session_state.evaluation_recorded_for_last_answer = True; st.rerun()
                               else: st.error("è¨˜éŒ²å¤±æ•—")
                           else: st.warning("ä¸¡æ–¹ã®è©•ä¾¡ã‚’é¸æŠ")
            else: st.success("âœ”ï¸ã“ã®å›ç­”ã¯è©•ä¾¡æ¸ˆã¿ã§ã™ã€‚")
            st.divider()

            # è©•ä¾¡ãƒ­ã‚°ã®è¡¨ç¤º
            st.markdown("#### è©•ä¾¡ãƒ­ã‚°ï¼ˆæœ€æ–°10ä»¶ï¼‰")
            eval_file_path = os.path.join(os.path.dirname(__file__), EVALUATION_FILE)
            @st.cache_data
            def load_evaluation_data(fp):
                if os.path.exists(fp):
                    try: return pd.read_csv(fp)
                    except pd.errors.EmptyDataError: return pd.DataFrame()
                    except Exception as e: logger.error(f"Log display error: {e}"); return None
                else: return None
            df_eval = load_evaluation_data(eval_file_path)
            if df_eval is not None:
                 if not df_eval.empty:
                     cols = ['timestamp', 'query', 'answer_evaluation', 'basis_evaluation', 'comment']; st.dataframe(df_eval[cols].tail(10).reset_index(drop=True), use_container_width=True)
                     @st.cache_data
                     def get_csv_data(df): return df.to_csv(index=False, encoding='utf-8-sig').encode('utf-8-sig')
                     csv_d = get_csv_data(df_eval); st.download_button(label="å…¨ãƒ­ã‚°DL", data=csv_d, file_name=EVALUATION_FILE, mime='text/csv')
                 else: st.info("è©•ä¾¡ãƒ­ã‚°ãªã—")
            else: st.info(f"ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«({EVALUATION_FILE})ãªã—")


# --- ã‚¹ã‚¯ãƒªãƒ—ãƒˆå®Ÿè¡Œ ---
if __name__ == "__main__":
    main()